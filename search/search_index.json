{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The OpenVoiceOS Technical Manual Welcome to the OVOS developer documentation if you are not looking to develop with OpenVoiceOS the community-docs might be more suitable for your needs Perspective OVOS aims to be a full voice operating system, a operating system needs many applications and frameworks, OVOS provides all the required tooling to integrate voice in your projects The typical use case for OVOS is as a voice assistant, \"Hey Mycroft, what is ...\" , but as a voice operating system OVOS makes no such assumptions and can be customized for any other use case, we have seen our community integrate OVOS on robots, furniture, smart mirrors, cloud applications, TVs and more! The following companion projects can be seen as \"OVOS distros\", opinionated combinations of OVOS packages to provide a full voice assistant stack raspOVOS - optimized for raspberry pi, built on top of Raspberry Pi OS ovos-buildroot - production ready minimal linux for embedded devices, based on ovos-docker and buildroot ovos-installer - setup OVOS on top of an existing OS In these docs you will find info about the architecture of OVOS, the several kinds of plugins available, and OVOS application development guidelines. External Resources User oriented docs are automatically published at https://openvoiceos.github.io/community-docs Docker specific documentation can be found at https://openvoiceos.github.io/ovos-docker An index of all bus messages emitted or listened too by all the OVOS repositories can be found at https://openvoiceos.github.io/message_spec If you need to connect remote clients to OVOS, the HiveMind project has it's own dedicated documentations at https://jarbashivemind.github.io/HiveMind-community-docs/ Contributing to Documentation The OVOS developer documentation is written and maintained by users just like you! Think of these docs both as your starting point and also forever changing and incomplete Please open Issues and Pull Requests !","title":"Introduction"},{"location":"#the-openvoiceos-technical-manual","text":"Welcome to the OVOS developer documentation if you are not looking to develop with OpenVoiceOS the community-docs might be more suitable for your needs","title":"The OpenVoiceOS Technical Manual"},{"location":"#perspective","text":"OVOS aims to be a full voice operating system, a operating system needs many applications and frameworks, OVOS provides all the required tooling to integrate voice in your projects The typical use case for OVOS is as a voice assistant, \"Hey Mycroft, what is ...\" , but as a voice operating system OVOS makes no such assumptions and can be customized for any other use case, we have seen our community integrate OVOS on robots, furniture, smart mirrors, cloud applications, TVs and more! The following companion projects can be seen as \"OVOS distros\", opinionated combinations of OVOS packages to provide a full voice assistant stack raspOVOS - optimized for raspberry pi, built on top of Raspberry Pi OS ovos-buildroot - production ready minimal linux for embedded devices, based on ovos-docker and buildroot ovos-installer - setup OVOS on top of an existing OS In these docs you will find info about the architecture of OVOS, the several kinds of plugins available, and OVOS application development guidelines.","title":"Perspective"},{"location":"#external-resources","text":"User oriented docs are automatically published at https://openvoiceos.github.io/community-docs Docker specific documentation can be found at https://openvoiceos.github.io/ovos-docker An index of all bus messages emitted or listened too by all the OVOS repositories can be found at https://openvoiceos.github.io/message_spec If you need to connect remote clients to OVOS, the HiveMind project has it's own dedicated documentations at https://jarbashivemind.github.io/HiveMind-community-docs/","title":"External Resources"},{"location":"#contributing-to-documentation","text":"The OVOS developer documentation is written and maintained by users just like you! Think of these docs both as your starting point and also forever changing and incomplete Please open Issues and Pull Requests !","title":"Contributing to Documentation"},{"location":"OCA/","text":"OCA - OVOS Config Assistant OCA is a user facing interface to configure ovos devices Web UI OCA provides a local Web UI similar to ovos-backend-manager, in here you can configure your device, view metrics, handle OAuth and more CLI A command line interface is planned but not yet available to provide equivalent functionality to the Web UI Python utils from ovos_config_assistant.module_helpers import pprint_core_module_info pprint_core_module_info() \"\"\" ## Mycroft module info can import mycroft : True is ovos-core : True mycroft module location: /home/user/ovos-core/mycroft ## Downstream ovos.conf overrides Module: neon_core can import neon_core : False neon_core module location: None xdg compliance : True base xdg folder : neon mycroft config filename : neon.conf default mycroft.conf path : /home/user/NeonCore/neon_core/configuration/neon.conf Module: hivemind can import hivemind : False hivemind module location: None xdg compliance : True base xdg folder : hivemind mycroft config filename : hivemind.conf default mycroft.conf path : /home/user/PycharmProjects/ovos_workspace/ovos-core/.venv/lib/python3.9/site-packages/mycroft/configuration/mycroft.conf ## Downstream module overrides: Module: neon_speech uses config from : neon_core can import neon_speech : False neon_speech module location: None Module: neon_audio uses config from : neon_core can import neon_audio : False neon_audio module location: None Module: neon_enclosure uses config from : neon_core can import neon_enclosure : False neon_enclosure module location: None Module: hivemind_voice_satellite uses config from : hivemind can import hivemind_voice_satellite : True hivemind_voice_satellite module location: /home/user/HiveMind-voice-sat/hivemind_voice_satellite \"\"\" from ovos_config_assistant.config_helpers import pprint_ovos_conf pprint_ovos_conf() \"\"\" ## OVOS Configuration ovos.conf exists : True /home/user/.config/OpenVoiceOS/ovos.conf xdg compliance : True base xdg folder : mycroft mycroft config filename : mycroft.conf default mycroft.conf path : /home/user/ovos-core/.venv/lib/python3.9/site-packages/mycroft/configuration/mycroft.conf \"\"\"","title":"OCA - OVOS Config Assistant"},{"location":"OCA/#oca-ovos-config-assistant","text":"OCA is a user facing interface to configure ovos devices","title":"OCA - OVOS Config Assistant"},{"location":"OCA/#web-ui","text":"OCA provides a local Web UI similar to ovos-backend-manager, in here you can configure your device, view metrics, handle OAuth and more","title":"Web UI"},{"location":"OCA/#cli","text":"A command line interface is planned but not yet available to provide equivalent functionality to the Web UI","title":"CLI"},{"location":"OCA/#python-utils","text":"from ovos_config_assistant.module_helpers import pprint_core_module_info pprint_core_module_info() \"\"\" ## Mycroft module info can import mycroft : True is ovos-core : True mycroft module location: /home/user/ovos-core/mycroft ## Downstream ovos.conf overrides Module: neon_core can import neon_core : False neon_core module location: None xdg compliance : True base xdg folder : neon mycroft config filename : neon.conf default mycroft.conf path : /home/user/NeonCore/neon_core/configuration/neon.conf Module: hivemind can import hivemind : False hivemind module location: None xdg compliance : True base xdg folder : hivemind mycroft config filename : hivemind.conf default mycroft.conf path : /home/user/PycharmProjects/ovos_workspace/ovos-core/.venv/lib/python3.9/site-packages/mycroft/configuration/mycroft.conf ## Downstream module overrides: Module: neon_speech uses config from : neon_core can import neon_speech : False neon_speech module location: None Module: neon_audio uses config from : neon_core can import neon_audio : False neon_audio module location: None Module: neon_enclosure uses config from : neon_core can import neon_enclosure : False neon_enclosure module location: None Module: hivemind_voice_satellite uses config from : hivemind can import hivemind_voice_satellite : True hivemind_voice_satellite module location: /home/user/HiveMind-voice-sat/hivemind_voice_satellite \"\"\" from ovos_config_assistant.config_helpers import pprint_ovos_conf pprint_ovos_conf() \"\"\" ## OVOS Configuration ovos.conf exists : True /home/user/.config/OpenVoiceOS/ovos.conf xdg compliance : True base xdg folder : mycroft mycroft config filename : mycroft.conf default mycroft.conf path : /home/user/ovos-core/.venv/lib/python3.9/site-packages/mycroft/configuration/mycroft.conf \"\"\"","title":"Python utils"},{"location":"OCP/","text":"ovos-media EXPERIMENTAL - NEW ovos-core version 0.0.8 ovos-media is a work in progress, it does not yet ship with OVOS by default, but it can be manually enabled In order to use ovos-media you need to enable the OCP pipeline in ovos-core and to disable the old audio service disabling old audio service { \"enable_old_audioservice\": false } Enabling OCP pipeline { // Intent Pipeline / plugins config \"intents\" : { // the pipeline is a ordered set of frameworks to send an utterance too // if one of the frameworks fails the next one is used, until an answer is found \"pipeline\": [ \"converse\", \"ocp_high\", \"...\", \"common_qa\", \"ocp_medium\", \"...\", \"ocp_fallback\", \"fallback_low\" ] } } OCP OCP stands for OpenVoiceOS Common Play, it is a full-fledged media player service that can handle audio and video DEPRECATION WARNING OCP is in the process of migrating from a audio plugin to ovos-media service, this documentation is not valid for ovos-core version 0.0.7 OCP provides a pipeline component specialized in matching media queries. The pipeline classifies the media type (movie, music, podcast...) and queries OCP skills for results, you can read more about the OCP Pipeline docs Architecture Media Intents Before regular intent stage, taking into account current OCP state (media ready to play / playing) \"play {query}\" \"previous\" (media needs to be loaded) \"next\" (media needs to be loaded) \"pause\" (media needs to be loaded) \"play\" / \"resume\" (media needs to be loaded) \"stop\" (media needs to be loaded) \"I like that song\" (music needs to be playing) MPRIS integration OCP Integrates with MPRIS allows OCP to control external players Sync with external players Via MPRIS OCP can control and display data from external players, if using KDEConnect this includes playback in connected devices See a demo here This also includes voice intents, allowing you for example to voice control spotify Manage multiple players If OCP is set to manage external players it will ensure only one of them is playing media at once, if using KDEConnect this includes playback in connected devices See a demo here ( warning : contains black metal) Skills Menu Some skills provide featured_media, you can access these from the OCP menu Homescreen widget The homescreen skill that comes pre-installed with OpenVoiceOS also comes with a widget for the OCP framework. File Browser integration selected files will be played in OCP folders are considered playlists Favorite Songs You can like a song that is currently playing via GUI and intent \"I like that song\" Liked songs can be played via intent \"play my favorite songs\" or GUI Configuration under mycroft.conf { // Configure ovos-media service // similarly to wakewords, configure any number of playback handlers // playback handlers might be local applications or even remote devices \"media\": { // order of preference to try playback handlers // if unavailable or unable to handle a uri, the next in list is used // NB: users may request specific handlers in the utterance // keys are the strings defined in \"audio_players\" \"preferred_audio_services\": [\"gui\", \"vlc\", \"mplayer\", \"cli\"], // keys are the strings defined in \"web_players\" \"preferred_web_services\": [\"gui\", \"browser\"], // keys are the strings defined in \"video_players\" \"preferred_video_services\": [\"gui\", \"vlc\"], // PlaybackType.AUDIO handlers \"audio_players\": { // vlc player uses a headless vlc instance to handle uris \"vlc\": { // the plugin name \"module\": \"ovos-media-audio-plugin-vlc\", // friendly names a user may use to refer to this playback handler // those will be parsed by OCP and used to initiate // playback in the request playback handler \"aliases\": [\"VLC\"], // deactivate a plugin by setting to false \"active\": true }, // command line player uses configurable shell commands with file uris as arguments \"cli\": { // the plugin name \"module\": \"ovos-media-audio-plugin-cli\", // friendly names a user may use to refer to this playback handler // those will be parsed by OCP and used to initiate // playback in the request playback handler \"aliases\": [\"Command Line\"], // deactivate a plugin by setting to false \"active\": true }, // gui uses mycroft-gui natively to handle uris \"gui\": { // the plugin name \"module\": \"ovos-media-audio-plugin-gui\", // friendly names a user may use to refer to this playback handler // those will be parsed by OCP and used to initiate // playback in the request playback handler \"aliases\": [\"GUI\", \"Graphical User Interface\"], // deactivate a plugin by setting to false \"active\": true } }, // PlaybackType.VIDEO handlers \"video_players\": { // vlc player uses a headless vlc instance to handle uris \"vlc\": { // the plugin name \"module\": \"ovos-media-video-plugin-vlc\", // friendly names a user may use to refer to this playback handler // those will be parsed by OCP and used to initiate // playback in the request playback handler \"aliases\": [\"VLC\"], // deactivate a plugin by setting to false \"active\": true }, // gui uses mycroft-gui natively to handle uris \"gui\": { // the plugin name \"module\": \"ovos-media-video-plugin-gui\", // friendly names a user may use to refer to this playback handler // those will be parsed by OCP and used to initiate // playback in the request playback handler \"aliases\": [\"GUI\", \"Graphical User Interface\"], // deactivate a plugin by setting to false \"active\": true } }, // PlaybackType.WEBVIEW handlers \"web_players\": { // open url in the native browser \"browser\": { // the plugin name \"module\": \"ovos-media-web-plugin-browser\", // friendly names a user may use to refer to this playback handler // those will be parsed by OCP and used to initiate // playback in the request playback handler \"aliases\": [\"Browser\", \"Local Browser\", \"Default Browser\"], // deactivate a plugin by setting to false \"active\": true }, // gui uses mycroft-gui natively to handle uris \"gui\": { // the plugin name \"module\": \"ovos-media-web-plugin-gui\", // friendly names a user may use to refer to this playback handler // those will be parsed by OCP and used to initiate // playback in the request playback handler \"aliases\": [\"GUI\", \"Graphical User Interface\"], // deactivate a plugin by setting to false \"active\": true } } } } Troubleshooting Having trouble getting OCP to run properly and be exposed as an MPRIS media player? Check the following: The DBUS_SESSION_BUS_ADDRESS environment variable is what OCP uses to try to connect to dbus . On an OVOS system it will look something like unix:path=/run/user/1000/bus . To get the right user ID, run id -u . If DBUS_SESSION_BUS_ADDRESS is not set, the next place OCP checks is the DISPLAY environment variable. If this is set and looks similar to the value above, then you can probably exclude DBUS_SESSION_BUS_ADDRESS , but if neither are set then use DBUS_SESSION_BUS_ADDRESS . Make sure your OCP settings in your config file like something like the following, taking note of the dbus_type value: \"media\": { \"dbus_type\": \"session\" } If your dbus_type is set to system then OCP will still work, but since it requires root privileges to read from the system dbus, external systems or programs without root privileges cannot read the MPRIS data there. You can confirm if the OCP player is registered with dbus using the following command: dbus-send --session --dest=org.freedesktop.DBus --type=method_call --print-reply /org/freedesktop/DBus org.freedesktop.DBus.ListNames The output should look something like the following, if it is working: method return time=1691467760.293397 sender=org.freedesktop.DBus -> destination=:1.10 serial=3 reply_serial=2 array [ string \"org.freedesktop.DBus\" string \"org.freedesktop.systemd1\" string \":1.10\" string \"org.mpris.MediaPlayer2.OCP\" string \":1.9\" string \":1.1\" ] The important part is the org.mpris.MediaPlayer2.OCP value. If the above steps do not work, please reach out to the OVOS team on Matrix for assistance.","title":"Media"},{"location":"OCP/#ovos-media","text":"EXPERIMENTAL - NEW ovos-core version 0.0.8 ovos-media is a work in progress, it does not yet ship with OVOS by default, but it can be manually enabled In order to use ovos-media you need to enable the OCP pipeline in ovos-core and to disable the old audio service disabling old audio service { \"enable_old_audioservice\": false } Enabling OCP pipeline { // Intent Pipeline / plugins config \"intents\" : { // the pipeline is a ordered set of frameworks to send an utterance too // if one of the frameworks fails the next one is used, until an answer is found \"pipeline\": [ \"converse\", \"ocp_high\", \"...\", \"common_qa\", \"ocp_medium\", \"...\", \"ocp_fallback\", \"fallback_low\" ] } }","title":"ovos-media"},{"location":"OCP/#ocp","text":"OCP stands for OpenVoiceOS Common Play, it is a full-fledged media player service that can handle audio and video DEPRECATION WARNING OCP is in the process of migrating from a audio plugin to ovos-media service, this documentation is not valid for ovos-core version 0.0.7 OCP provides a pipeline component specialized in matching media queries. The pipeline classifies the media type (movie, music, podcast...) and queries OCP skills for results, you can read more about the OCP Pipeline docs","title":"OCP"},{"location":"OCP/#architecture","text":"","title":"Architecture"},{"location":"OCP/#media-intents","text":"Before regular intent stage, taking into account current OCP state (media ready to play / playing) \"play {query}\" \"previous\" (media needs to be loaded) \"next\" (media needs to be loaded) \"pause\" (media needs to be loaded) \"play\" / \"resume\" (media needs to be loaded) \"stop\" (media needs to be loaded) \"I like that song\" (music needs to be playing)","title":"Media Intents"},{"location":"OCP/#mpris-integration","text":"OCP Integrates with MPRIS allows OCP to control external players","title":"MPRIS integration"},{"location":"OCP/#sync-with-external-players","text":"Via MPRIS OCP can control and display data from external players, if using KDEConnect this includes playback in connected devices See a demo here This also includes voice intents, allowing you for example to voice control spotify","title":"Sync with external players"},{"location":"OCP/#manage-multiple-players","text":"If OCP is set to manage external players it will ensure only one of them is playing media at once, if using KDEConnect this includes playback in connected devices See a demo here ( warning : contains black metal)","title":"Manage multiple players"},{"location":"OCP/#skills-menu","text":"Some skills provide featured_media, you can access these from the OCP menu","title":"Skills Menu"},{"location":"OCP/#homescreen-widget","text":"The homescreen skill that comes pre-installed with OpenVoiceOS also comes with a widget for the OCP framework.","title":"Homescreen widget"},{"location":"OCP/#file-browser-integration","text":"selected files will be played in OCP folders are considered playlists","title":"File Browser integration"},{"location":"OCP/#favorite-songs","text":"You can like a song that is currently playing via GUI and intent \"I like that song\" Liked songs can be played via intent \"play my favorite songs\" or GUI","title":"Favorite Songs"},{"location":"OCP/#configuration","text":"under mycroft.conf { // Configure ovos-media service // similarly to wakewords, configure any number of playback handlers // playback handlers might be local applications or even remote devices \"media\": { // order of preference to try playback handlers // if unavailable or unable to handle a uri, the next in list is used // NB: users may request specific handlers in the utterance // keys are the strings defined in \"audio_players\" \"preferred_audio_services\": [\"gui\", \"vlc\", \"mplayer\", \"cli\"], // keys are the strings defined in \"web_players\" \"preferred_web_services\": [\"gui\", \"browser\"], // keys are the strings defined in \"video_players\" \"preferred_video_services\": [\"gui\", \"vlc\"], // PlaybackType.AUDIO handlers \"audio_players\": { // vlc player uses a headless vlc instance to handle uris \"vlc\": { // the plugin name \"module\": \"ovos-media-audio-plugin-vlc\", // friendly names a user may use to refer to this playback handler // those will be parsed by OCP and used to initiate // playback in the request playback handler \"aliases\": [\"VLC\"], // deactivate a plugin by setting to false \"active\": true }, // command line player uses configurable shell commands with file uris as arguments \"cli\": { // the plugin name \"module\": \"ovos-media-audio-plugin-cli\", // friendly names a user may use to refer to this playback handler // those will be parsed by OCP and used to initiate // playback in the request playback handler \"aliases\": [\"Command Line\"], // deactivate a plugin by setting to false \"active\": true }, // gui uses mycroft-gui natively to handle uris \"gui\": { // the plugin name \"module\": \"ovos-media-audio-plugin-gui\", // friendly names a user may use to refer to this playback handler // those will be parsed by OCP and used to initiate // playback in the request playback handler \"aliases\": [\"GUI\", \"Graphical User Interface\"], // deactivate a plugin by setting to false \"active\": true } }, // PlaybackType.VIDEO handlers \"video_players\": { // vlc player uses a headless vlc instance to handle uris \"vlc\": { // the plugin name \"module\": \"ovos-media-video-plugin-vlc\", // friendly names a user may use to refer to this playback handler // those will be parsed by OCP and used to initiate // playback in the request playback handler \"aliases\": [\"VLC\"], // deactivate a plugin by setting to false \"active\": true }, // gui uses mycroft-gui natively to handle uris \"gui\": { // the plugin name \"module\": \"ovos-media-video-plugin-gui\", // friendly names a user may use to refer to this playback handler // those will be parsed by OCP and used to initiate // playback in the request playback handler \"aliases\": [\"GUI\", \"Graphical User Interface\"], // deactivate a plugin by setting to false \"active\": true } }, // PlaybackType.WEBVIEW handlers \"web_players\": { // open url in the native browser \"browser\": { // the plugin name \"module\": \"ovos-media-web-plugin-browser\", // friendly names a user may use to refer to this playback handler // those will be parsed by OCP and used to initiate // playback in the request playback handler \"aliases\": [\"Browser\", \"Local Browser\", \"Default Browser\"], // deactivate a plugin by setting to false \"active\": true }, // gui uses mycroft-gui natively to handle uris \"gui\": { // the plugin name \"module\": \"ovos-media-web-plugin-gui\", // friendly names a user may use to refer to this playback handler // those will be parsed by OCP and used to initiate // playback in the request playback handler \"aliases\": [\"GUI\", \"Graphical User Interface\"], // deactivate a plugin by setting to false \"active\": true } } } }","title":"Configuration"},{"location":"OCP/#troubleshooting","text":"Having trouble getting OCP to run properly and be exposed as an MPRIS media player? Check the following: The DBUS_SESSION_BUS_ADDRESS environment variable is what OCP uses to try to connect to dbus . On an OVOS system it will look something like unix:path=/run/user/1000/bus . To get the right user ID, run id -u . If DBUS_SESSION_BUS_ADDRESS is not set, the next place OCP checks is the DISPLAY environment variable. If this is set and looks similar to the value above, then you can probably exclude DBUS_SESSION_BUS_ADDRESS , but if neither are set then use DBUS_SESSION_BUS_ADDRESS . Make sure your OCP settings in your config file like something like the following, taking note of the dbus_type value: \"media\": { \"dbus_type\": \"session\" } If your dbus_type is set to system then OCP will still work, but since it requires root privileges to read from the system dbus, external systems or programs without root privileges cannot read the MPRIS data there. You can confirm if the OCP player is registered with dbus using the following command: dbus-send --session --dest=org.freedesktop.DBus --type=method_call --print-reply /org/freedesktop/DBus org.freedesktop.DBus.ListNames The output should look something like the following, if it is working: method return time=1691467760.293397 sender=org.freedesktop.DBus -> destination=:1.10 serial=3 reply_serial=2 array [ string \"org.freedesktop.DBus\" string \"org.freedesktop.systemd1\" string \":1.10\" string \"org.mpris.MediaPlayer2.OCP\" string \":1.9\" string \":1.1\" ] The important part is the org.mpris.MediaPlayer2.OCP value. If the above steps do not work, please reach out to the OVOS team on Matrix for assistance.","title":"Troubleshooting"},{"location":"OCP_pipeline/","text":"OCP Pipeline Layer 1 - Unambiguous Before regular intent stage, taking into account current OCP state (media ready to play / playing) Only matches if user unambiguously wants to trigger OCP uses padacioso for exact matches play {query} previous (media needs to be loaded) next (media needs to be loaded) pause (media needs to be loaded) play / resume (media needs to be loaded) stop (media needs to be loaded) from ocp_nlp.intents import OCPPipelineMatcher ocp = OCPPipelineMatcher() print(ocp.match_high(\"play metallica\", \"en-us\")) # IntentMatch(intent_service='OCP_intents', # intent_type='ocp:play', # intent_data={'media_type': <MediaType.MUSIC: 2>, 'query': 'metallica', # 'entities': {'album_name': 'Metallica', 'artist_name': 'Metallica'}, # 'conf': 0.96, 'lang': 'en-us'}, # skill_id='ovos.common_play', utterance='play metallica') Layer 2 - Semi-Ambiguous uses a binary classifier to detect if a query is about media playback from ocp_nlp.intents import OCPPipelineMatcher ocp = OCPPipelineMatcher() print(ocp.match_high(\"put on some metallica\", \"en-us\")) # None print(ocp.match_medium(\"put on some metallica\", \"en-us\")) # IntentMatch(intent_service='OCP_media', # intent_type='ocp:play', # intent_data={'media_type': <MediaType.MUSIC: 2>, # 'entities': {'album_name': 'Metallica', 'artist_name': 'Metallica', 'movie_name': 'Some'}, # 'query': 'put on some metallica', # 'conf': 0.9578441098114333}, # skill_id='ovos.common_play', utterance='put on some metallica') Layer 3 - Ambiguous Uses keyword matching and requires at least 1 keyword OCP skills can provide these keywords at runtime, additional keywords for things such as media_genre were collected via SPARQL queries to wikidata from ocp_nlp.intents import OCPPipelineMatcher ocp = OCPPipelineMatcher() print(ocp.match_medium(\"i wanna hear metallica\", \"en-us\")) # None print(ocp.match_fallback(\"i wanna hear metallica\", \"en-us\")) # IntentMatch(intent_service='OCP_fallback', # intent_type='ocp:play', # intent_data={'media_type': <MediaType.MUSIC: 2>, # 'entities': {'album_name': 'Metallica', 'artist_name': 'Metallica'}, # 'query': 'i wanna hear metallica', # 'conf': 0.5027561091821287}, # skill_id='ovos.common_play', utterance='i wanna hear metallica') Classifiers Architecture Efficient entity matching is done via Aho\u2013Corasick algorithm , keyword features are essentially a keyword count. The way the OCP dataset was collected ensures these features were present during training and interpretable, therefore during runtime any number of entities.csv files can be loaded, OVOS skills can also register their own keywords Media Type Classifier internally used to tag utterances before OCP search process, this informs the result selection by giving priority to certain skills and helps performance by skipping some skills completely during search uses a scikit-learn classifier trained in a large synthetic dataset class MediaType: GENERIC = 0 # nothing else matches AUDIO = 1 # things like ambient noises MUSIC = 2 VIDEO = 3 # eg, youtube videos AUDIOBOOK = 4 GAME = 5 # because it shares the verb \"play\", mostly for disambguation PODCAST = 6 RADIO = 7 # live radio NEWS = 8 # news reports TV = 9 # live tv stream MOVIE = 10 TRAILER = 11 AUDIO_DESCRIPTION = 12 # narrated movie for the blind VISUAL_STORY = 13 # things like animated comic books BEHIND_THE_SCENES = 14 DOCUMENTARY = 15 RADIO_THEATRE = 16 SHORT_FILM = 17 # typically movies under 45 min SILENT_MOVIE = 18 VIDEO_EPISODES = 19 # tv series etc BLACK_WHITE_MOVIE = 20 CARTOON = 21 ANIME = 22 ADULT = 69 # for content filtering HENTAI = 70 # for content filtering ADULT_AUDIO = 71 # for content filtering The features of this classifier have been engineered to allow influencing classifications at runtime based on available skills Classifier options: - heuristic based on keyword features (baseline - lang agnostic) ~= 20% accuracy - trained on text only features (count vectorizer baseline - english) ~= 85% accuracy - trained on keyword features (lang agnostic - runtime keywords influence classification) ~= 88% accuracy - trained on probabilities of text only classifier + keyword features (english only - runtime keywords influence classification) ~= 95% accuracy NOTE: several classification algorithms have been tested, Perceptron and MLP are the most sensitive to the runtime bias properly Binary classifier using the dataset collected for media type + ovos-datasets Classifier options: - trained on text only features (count vectorizer baseline - english) ~= 95% accuracy - trained on keyword features (lang agnostic - runtime keywords influence classification) ~= 90% accuracy Usage check if an utterance is playback related from ocp_nlp.classify import BinaryPlaybackClassifier clf = BinaryPlaybackClassifier() clf.load() preds = clf.predict([\"play a song\", \"play my morning jams\", \"i want to watch the matrix\", \"tell me a joke\", \"who are you\", \"you suck\"]) print(preds) # ['OCP' 'OCP' 'OCP' 'other' 'other' 'other'] get media type of a playback utterance from ocp_nlp.classify import MediaTypeClassifier, BiasedMediaTypeClassifier # basic text only classifier clf1 = MediaTypeClassifier() clf1.load() label, confidence = clf1.predict_prob([\"play metallica\"])[0] print(label, confidence) # [('music', 0.3438956411030462)] # keyword biased classifier, uses the above internally for extra features clf = BiasedMediaTypeClassifier(lang=\"en\", preload=True) # load entities database clf.load() # klownevilus is an unknown entity label, confidence = clf.predict_prob([\"play klownevilus\"])[0] print(label, confidence) # music 0.3398020446925623 # probability increases for movie clf.register_entity(\"movie_name\", [\"klownevilus\"]) # movie correctly predicted now label, confidence = clf.predict_prob([\"play klownevilus\"])[0] print(label, confidence) # movie 0.540225616798516","title":"OCP Pipeline"},{"location":"OCP_pipeline/#ocp-pipeline","text":"","title":"OCP Pipeline"},{"location":"OCP_pipeline/#layer-1-unambiguous","text":"Before regular intent stage, taking into account current OCP state (media ready to play / playing) Only matches if user unambiguously wants to trigger OCP uses padacioso for exact matches play {query} previous (media needs to be loaded) next (media needs to be loaded) pause (media needs to be loaded) play / resume (media needs to be loaded) stop (media needs to be loaded) from ocp_nlp.intents import OCPPipelineMatcher ocp = OCPPipelineMatcher() print(ocp.match_high(\"play metallica\", \"en-us\")) # IntentMatch(intent_service='OCP_intents', # intent_type='ocp:play', # intent_data={'media_type': <MediaType.MUSIC: 2>, 'query': 'metallica', # 'entities': {'album_name': 'Metallica', 'artist_name': 'Metallica'}, # 'conf': 0.96, 'lang': 'en-us'}, # skill_id='ovos.common_play', utterance='play metallica')","title":"Layer 1 - Unambiguous"},{"location":"OCP_pipeline/#layer-2-semi-ambiguous","text":"uses a binary classifier to detect if a query is about media playback from ocp_nlp.intents import OCPPipelineMatcher ocp = OCPPipelineMatcher() print(ocp.match_high(\"put on some metallica\", \"en-us\")) # None print(ocp.match_medium(\"put on some metallica\", \"en-us\")) # IntentMatch(intent_service='OCP_media', # intent_type='ocp:play', # intent_data={'media_type': <MediaType.MUSIC: 2>, # 'entities': {'album_name': 'Metallica', 'artist_name': 'Metallica', 'movie_name': 'Some'}, # 'query': 'put on some metallica', # 'conf': 0.9578441098114333}, # skill_id='ovos.common_play', utterance='put on some metallica')","title":"Layer 2 - Semi-Ambiguous"},{"location":"OCP_pipeline/#layer-3-ambiguous","text":"Uses keyword matching and requires at least 1 keyword OCP skills can provide these keywords at runtime, additional keywords for things such as media_genre were collected via SPARQL queries to wikidata from ocp_nlp.intents import OCPPipelineMatcher ocp = OCPPipelineMatcher() print(ocp.match_medium(\"i wanna hear metallica\", \"en-us\")) # None print(ocp.match_fallback(\"i wanna hear metallica\", \"en-us\")) # IntentMatch(intent_service='OCP_fallback', # intent_type='ocp:play', # intent_data={'media_type': <MediaType.MUSIC: 2>, # 'entities': {'album_name': 'Metallica', 'artist_name': 'Metallica'}, # 'query': 'i wanna hear metallica', # 'conf': 0.5027561091821287}, # skill_id='ovos.common_play', utterance='i wanna hear metallica')","title":"Layer 3 - Ambiguous"},{"location":"OCP_pipeline/#classifiers","text":"","title":"Classifiers"},{"location":"OCP_pipeline/#architecture","text":"Efficient entity matching is done via Aho\u2013Corasick algorithm , keyword features are essentially a keyword count. The way the OCP dataset was collected ensures these features were present during training and interpretable, therefore during runtime any number of entities.csv files can be loaded, OVOS skills can also register their own keywords","title":"Architecture"},{"location":"OCP_pipeline/#media-type-classifier","text":"internally used to tag utterances before OCP search process, this informs the result selection by giving priority to certain skills and helps performance by skipping some skills completely during search uses a scikit-learn classifier trained in a large synthetic dataset class MediaType: GENERIC = 0 # nothing else matches AUDIO = 1 # things like ambient noises MUSIC = 2 VIDEO = 3 # eg, youtube videos AUDIOBOOK = 4 GAME = 5 # because it shares the verb \"play\", mostly for disambguation PODCAST = 6 RADIO = 7 # live radio NEWS = 8 # news reports TV = 9 # live tv stream MOVIE = 10 TRAILER = 11 AUDIO_DESCRIPTION = 12 # narrated movie for the blind VISUAL_STORY = 13 # things like animated comic books BEHIND_THE_SCENES = 14 DOCUMENTARY = 15 RADIO_THEATRE = 16 SHORT_FILM = 17 # typically movies under 45 min SILENT_MOVIE = 18 VIDEO_EPISODES = 19 # tv series etc BLACK_WHITE_MOVIE = 20 CARTOON = 21 ANIME = 22 ADULT = 69 # for content filtering HENTAI = 70 # for content filtering ADULT_AUDIO = 71 # for content filtering The features of this classifier have been engineered to allow influencing classifications at runtime based on available skills Classifier options: - heuristic based on keyword features (baseline - lang agnostic) ~= 20% accuracy - trained on text only features (count vectorizer baseline - english) ~= 85% accuracy - trained on keyword features (lang agnostic - runtime keywords influence classification) ~= 88% accuracy - trained on probabilities of text only classifier + keyword features (english only - runtime keywords influence classification) ~= 95% accuracy NOTE: several classification algorithms have been tested, Perceptron and MLP are the most sensitive to the runtime bias properly","title":"Media Type Classifier"},{"location":"OCP_pipeline/#binary-classifier","text":"using the dataset collected for media type + ovos-datasets Classifier options: - trained on text only features (count vectorizer baseline - english) ~= 95% accuracy - trained on keyword features (lang agnostic - runtime keywords influence classification) ~= 90% accuracy","title":"Binary classifier"},{"location":"OCP_pipeline/#usage","text":"check if an utterance is playback related from ocp_nlp.classify import BinaryPlaybackClassifier clf = BinaryPlaybackClassifier() clf.load() preds = clf.predict([\"play a song\", \"play my morning jams\", \"i want to watch the matrix\", \"tell me a joke\", \"who are you\", \"you suck\"]) print(preds) # ['OCP' 'OCP' 'OCP' 'other' 'other' 'other'] get media type of a playback utterance from ocp_nlp.classify import MediaTypeClassifier, BiasedMediaTypeClassifier # basic text only classifier clf1 = MediaTypeClassifier() clf1.load() label, confidence = clf1.predict_prob([\"play metallica\"])[0] print(label, confidence) # [('music', 0.3438956411030462)] # keyword biased classifier, uses the above internally for extra features clf = BiasedMediaTypeClassifier(lang=\"en\", preload=True) # load entities database clf.load() # klownevilus is an unknown entity label, confidence = clf.predict_prob([\"play klownevilus\"])[0] print(label, confidence) # music 0.3398020446925623 # probability increases for movie clf.register_entity(\"movie_name\", [\"klownevilus\"]) # movie correctly predicted now label, confidence = clf.predict_prob([\"play klownevilus\"])[0] print(label, confidence) # movie 0.540225616798516","title":"Usage"},{"location":"OCP_skills/","text":"OCP Skills OCP skills are built from the OVOSCommonPlaybackSkill class These skills work as media providers, they return results for OCP to playback The actual voice interaction is handled by OCP, skills only implement the returning of results Search Results Search results are returned as a list of dicts, skills can also use iterators to yield results 1 at a time as they become available Mandatory fields are uri: str # URL/URI of media, OCP will handle formatting and file handling title: str media_type: MediaType playback: PlaybackType match_confidence: int # 0-100 Other optional metadata includes artists, album, length and images for the GUI artist: str album: str image: str # uri/file path bg_image: str # uri/file path skill_icon: str # uri/file path length: int # seconds, -1 for live streams OCP Skill General Steps to create a skill subclass your skill from OVOSCommonPlaybackSkill In the __init__ method indicate the media types you want to handle self.voc_match(phrase, \"skill_name\") to handle specific requests for your skill self.remove_voc(phrase, \"skill_name\") to remove matched phrases from the search request Implement the ocp_search decorator, as many as you want (they run in parallel) The decorated method can return a list or be an iterator of result_dict (track or playlist) The search function can be entirely inline or call another Python library, like pandorinha or plexapi self.extend_timeout() to delay OCP from selecting a result, requesting more time to perform the search Implement a confidence score formula Values are between 0 and 100 High confidence scores cancel other OCP skill searches ocp_featured_media , return a playlist for the OCP menu if selected from GUI (optional) Create a requirements.txt file with third-party package requirements from os.path import join, dirname import radiosoma from ovos_utils import classproperty from ovos_utils.ocp import MediaType, PlaybackType from ovos_utils.parse import fuzzy_match from ovos_workshop.decorators.ocp import ocp_search, ocp_featured_media from ovos_workshop.skills.common_play import OVOSCommonPlaybackSkill class SomaFMSkill(OVOSCommonPlaybackSkill): def __init__(self, *args, **kwargs): # media type this skill can handle self.supported_media = [MediaType.MUSIC, MediaType.RADIO] self.skill_icon = join(dirname(__file__), \"ui\", \"somafm.png\") super().__init__(*args, **kwargs) @ocp_featured_media() def featured_media(self): # playlist when selected from OCP skills menu return [{ \"match_confidence\": 90, \"media_type\": MediaType.RADIO, \"uri\": ch.direct_stream, \"playback\": PlaybackType.AUDIO, \"image\": ch.image, \"bg_image\": ch.image, \"skill_icon\": self.skill_icon, \"title\": ch.title, \"author\": \"SomaFM\", \"length\": 0 } for ch in radiosoma.get_stations()] @ocp_search() def search_somafm(self, phrase, media_type): # check if user asked for a known radio station base_score = 0 if media_type == MediaType.RADIO: base_score += 20 else: base_score -= 30 if self.voc_match(phrase, \"radio\"): base_score += 10 phrase = self.remove_voc(phrase, \"radio\") if self.voc_match(phrase, \"somafm\"): base_score += 30 # explicit request phrase = self.remove_voc(phrase, \"somafm\") for ch in radiosoma.get_stations(): score = round(base_score + fuzzy_match(ch.title.lower(), phrase.lower()) * 100) if score < 50: continue yield { \"match_confidence\": min(100, score), \"media_type\": MediaType.RADIO, \"uri\": ch.direct_stream, \"playback\": PlaybackType.AUDIO, \"image\": ch.image, \"bg_image\": ch.image, \"skill_icon\": self.skill_icon, \"title\": ch.title, \"artistr\": \"SomaFM\", \"length\": 0 } OCP Keywords OCP skills often need to match hundreds or thousands of strings against the query string, self.voc_match can quickly become impractical to use in this scenario To help with this the OCP skill class provides efficient keyword matching def register_ocp_keyword(self, label: str, samples: List, langs: List[str] = None): \"\"\" register strings as native OCP keywords (eg, movie_name, artist_name ...) ocp keywords can be efficiently matched with self.ocp_match helper method that uses Aho\u2013Corasick algorithm \"\"\" def load_ocp_keyword_from_csv(self, csv_path: str, lang: str): \"\"\" load entities from a .csv file for usage with self.ocp_voc_match see the ocp_entities.csv datatsets for example files built from wikidata SPARQL queries examples contents of csv file label,entity film_genre,swashbuckler film film_genre,neo-noir film_genre,actual play film film_genre,alternate history film film_genre,spy film ... \"\"\" OCP Voc match uses Aho\u2013Corasick algorithm to match OCP keywords this efficiently matches many keywords against an utterance OCP keywords are registered via self.register_ocp_keyword wordlists can also be loaded from a .csv file, see the OCP dataset for a list of keywords gathered from wikidata with SPARQL queries OCP Database Skill import json from ovos_utils.messagebus import FakeBus from ovos_utils.ocp import MediaType from ovos_workshop.skills.common_play import OVOSCommonPlaybackSkill class HorrorBabbleSkill(OVOSCommonPlaybackSkill): def initialize(self): # get file from # https://github.com/JarbasSkills/skill-horrorbabble/blob/dev/bootstrap.json with open(\"hb.json\") as f: db = json.load(f) book_names = [] book_authors = [] for url, data in db.items(): t = data[\"title\"].split(\"/\")[0].strip() if \" by \" in t: title, author = t.split(\" by \") title = title.replace('\"', \"\").strip() author = author.split(\"(\")[0].strip() book_names.append(title) book_authors.append(author) if \" \" in author: book_authors += author.split(\" \") elif t.startswith('\"') and t.endswith('\"'): book_names.append(t[1:-1]) else: book_names.append(t) self.register_ocp_keyword(MediaType.AUDIOBOOK, \"book_author\", list(set(book_authors))) self.register_ocp_keyword(MediaType.AUDIOBOOK, \"book_name\", list(set(book_names))) self.register_ocp_keyword(MediaType.AUDIOBOOK, \"audiobook_streaming_provider\", [\"HorrorBabble\", \"Horror Babble\"]) s = HorrorBabbleSkill(bus=FakeBus(), skill_id=\"demo.fake\") entities = s.ocp_voc_match(\"read The Call of Cthulhu by Lovecraft\") # {'book_author': 'Lovecraft', 'book_name': 'The Call of Cthulhu'} print(entities) entities = s.ocp_voc_match(\"play HorrorBabble\") # {'audiobook_streaming_provider': 'HorrorBabble'} print(entities) Playlist Results Results can also be playlists, not only single tracks, for instance full albums or a full season for a series When a playlist is selected from Search Results, it will replace the Now Playing list Playlist results look exactly the same as regular results, but instead of a uri they provide a playlist playlist: list # list of dicts, each dict is a regular search result title: str media_type: MediaType playback: PlaybackType match_confidence: int # 0-100 NOTE: nested playlists are a work in progress and not guaranteed to be functional, ie, the \"playlist\" dict key should not include other playlists Playlist Skill class MyJamsSkill(OVOSCommonPlaybackSkill): def __init__(self, *args, **kwargs): self.supported_media = [MediaType.MUSIC] self.skill_icon = join(dirname(__file__), \"ui\", \"myjams.png\") super().__init__(*args, **kwargs) @ocp_search() def search_my_jams(self, phrase, media_type): if self.voc_match(...): results = [...] # regular result dicts, as in examples above score = 70 # TODO yield { \"match_confidence\": min(100, score), \"media_type\": MediaType.MUSIC, \"playlist\": results, # replaces \"uri\" \"playback\": PlaybackType.AUDIO, \"image\": self.image, \"bg_image\": self.image, \"skill_icon\": self.skill_icon, \"title\": \"MyJams\", \"length\": sum([r[\"length\"] for r in results]) # total playlist duration }","title":"Media Skills"},{"location":"OCP_skills/#ocp-skills","text":"OCP skills are built from the OVOSCommonPlaybackSkill class These skills work as media providers, they return results for OCP to playback The actual voice interaction is handled by OCP, skills only implement the returning of results","title":"OCP Skills"},{"location":"OCP_skills/#search-results","text":"Search results are returned as a list of dicts, skills can also use iterators to yield results 1 at a time as they become available Mandatory fields are uri: str # URL/URI of media, OCP will handle formatting and file handling title: str media_type: MediaType playback: PlaybackType match_confidence: int # 0-100 Other optional metadata includes artists, album, length and images for the GUI artist: str album: str image: str # uri/file path bg_image: str # uri/file path skill_icon: str # uri/file path length: int # seconds, -1 for live streams","title":"Search Results"},{"location":"OCP_skills/#ocp-skill","text":"General Steps to create a skill subclass your skill from OVOSCommonPlaybackSkill In the __init__ method indicate the media types you want to handle self.voc_match(phrase, \"skill_name\") to handle specific requests for your skill self.remove_voc(phrase, \"skill_name\") to remove matched phrases from the search request Implement the ocp_search decorator, as many as you want (they run in parallel) The decorated method can return a list or be an iterator of result_dict (track or playlist) The search function can be entirely inline or call another Python library, like pandorinha or plexapi self.extend_timeout() to delay OCP from selecting a result, requesting more time to perform the search Implement a confidence score formula Values are between 0 and 100 High confidence scores cancel other OCP skill searches ocp_featured_media , return a playlist for the OCP menu if selected from GUI (optional) Create a requirements.txt file with third-party package requirements from os.path import join, dirname import radiosoma from ovos_utils import classproperty from ovos_utils.ocp import MediaType, PlaybackType from ovos_utils.parse import fuzzy_match from ovos_workshop.decorators.ocp import ocp_search, ocp_featured_media from ovos_workshop.skills.common_play import OVOSCommonPlaybackSkill class SomaFMSkill(OVOSCommonPlaybackSkill): def __init__(self, *args, **kwargs): # media type this skill can handle self.supported_media = [MediaType.MUSIC, MediaType.RADIO] self.skill_icon = join(dirname(__file__), \"ui\", \"somafm.png\") super().__init__(*args, **kwargs) @ocp_featured_media() def featured_media(self): # playlist when selected from OCP skills menu return [{ \"match_confidence\": 90, \"media_type\": MediaType.RADIO, \"uri\": ch.direct_stream, \"playback\": PlaybackType.AUDIO, \"image\": ch.image, \"bg_image\": ch.image, \"skill_icon\": self.skill_icon, \"title\": ch.title, \"author\": \"SomaFM\", \"length\": 0 } for ch in radiosoma.get_stations()] @ocp_search() def search_somafm(self, phrase, media_type): # check if user asked for a known radio station base_score = 0 if media_type == MediaType.RADIO: base_score += 20 else: base_score -= 30 if self.voc_match(phrase, \"radio\"): base_score += 10 phrase = self.remove_voc(phrase, \"radio\") if self.voc_match(phrase, \"somafm\"): base_score += 30 # explicit request phrase = self.remove_voc(phrase, \"somafm\") for ch in radiosoma.get_stations(): score = round(base_score + fuzzy_match(ch.title.lower(), phrase.lower()) * 100) if score < 50: continue yield { \"match_confidence\": min(100, score), \"media_type\": MediaType.RADIO, \"uri\": ch.direct_stream, \"playback\": PlaybackType.AUDIO, \"image\": ch.image, \"bg_image\": ch.image, \"skill_icon\": self.skill_icon, \"title\": ch.title, \"artistr\": \"SomaFM\", \"length\": 0 }","title":"OCP Skill"},{"location":"OCP_skills/#ocp-keywords","text":"OCP skills often need to match hundreds or thousands of strings against the query string, self.voc_match can quickly become impractical to use in this scenario To help with this the OCP skill class provides efficient keyword matching def register_ocp_keyword(self, label: str, samples: List, langs: List[str] = None): \"\"\" register strings as native OCP keywords (eg, movie_name, artist_name ...) ocp keywords can be efficiently matched with self.ocp_match helper method that uses Aho\u2013Corasick algorithm \"\"\" def load_ocp_keyword_from_csv(self, csv_path: str, lang: str): \"\"\" load entities from a .csv file for usage with self.ocp_voc_match see the ocp_entities.csv datatsets for example files built from wikidata SPARQL queries examples contents of csv file label,entity film_genre,swashbuckler film film_genre,neo-noir film_genre,actual play film film_genre,alternate history film film_genre,spy film ... \"\"\"","title":"OCP Keywords"},{"location":"OCP_skills/#ocp-voc-match","text":"uses Aho\u2013Corasick algorithm to match OCP keywords this efficiently matches many keywords against an utterance OCP keywords are registered via self.register_ocp_keyword wordlists can also be loaded from a .csv file, see the OCP dataset for a list of keywords gathered from wikidata with SPARQL queries","title":"OCP Voc match"},{"location":"OCP_skills/#ocp-database-skill","text":"import json from ovos_utils.messagebus import FakeBus from ovos_utils.ocp import MediaType from ovos_workshop.skills.common_play import OVOSCommonPlaybackSkill class HorrorBabbleSkill(OVOSCommonPlaybackSkill): def initialize(self): # get file from # https://github.com/JarbasSkills/skill-horrorbabble/blob/dev/bootstrap.json with open(\"hb.json\") as f: db = json.load(f) book_names = [] book_authors = [] for url, data in db.items(): t = data[\"title\"].split(\"/\")[0].strip() if \" by \" in t: title, author = t.split(\" by \") title = title.replace('\"', \"\").strip() author = author.split(\"(\")[0].strip() book_names.append(title) book_authors.append(author) if \" \" in author: book_authors += author.split(\" \") elif t.startswith('\"') and t.endswith('\"'): book_names.append(t[1:-1]) else: book_names.append(t) self.register_ocp_keyword(MediaType.AUDIOBOOK, \"book_author\", list(set(book_authors))) self.register_ocp_keyword(MediaType.AUDIOBOOK, \"book_name\", list(set(book_names))) self.register_ocp_keyword(MediaType.AUDIOBOOK, \"audiobook_streaming_provider\", [\"HorrorBabble\", \"Horror Babble\"]) s = HorrorBabbleSkill(bus=FakeBus(), skill_id=\"demo.fake\") entities = s.ocp_voc_match(\"read The Call of Cthulhu by Lovecraft\") # {'book_author': 'Lovecraft', 'book_name': 'The Call of Cthulhu'} print(entities) entities = s.ocp_voc_match(\"play HorrorBabble\") # {'audiobook_streaming_provider': 'HorrorBabble'} print(entities)","title":"OCP Database Skill"},{"location":"OCP_skills/#playlist-results","text":"Results can also be playlists, not only single tracks, for instance full albums or a full season for a series When a playlist is selected from Search Results, it will replace the Now Playing list Playlist results look exactly the same as regular results, but instead of a uri they provide a playlist playlist: list # list of dicts, each dict is a regular search result title: str media_type: MediaType playback: PlaybackType match_confidence: int # 0-100 NOTE: nested playlists are a work in progress and not guaranteed to be functional, ie, the \"playlist\" dict key should not include other playlists","title":"Playlist Results"},{"location":"OCP_skills/#playlist-skill","text":"class MyJamsSkill(OVOSCommonPlaybackSkill): def __init__(self, *args, **kwargs): self.supported_media = [MediaType.MUSIC] self.skill_icon = join(dirname(__file__), \"ui\", \"myjams.png\") super().__init__(*args, **kwargs) @ocp_search() def search_my_jams(self, phrase, media_type): if self.voc_match(...): results = [...] # regular result dicts, as in examples above score = 70 # TODO yield { \"match_confidence\": min(100, score), \"media_type\": MediaType.MUSIC, \"playlist\": results, # replaces \"uri\" \"playback\": PlaybackType.AUDIO, \"image\": self.image, \"bg_image\": self.image, \"skill_icon\": self.skill_icon, \"title\": \"MyJams\", \"length\": sum([r[\"length\"] for r in results]) # total playlist duration }","title":"Playlist Skill"},{"location":"OPM/","text":"OPM OPM is the OVOS Plugin Manager , this base package provides arbitrary plugins to the ovos ecosystem OPM plugins import their base classes from OPM making them portable and independent of core, plugins can be used in your standalone projects By using OPM you can ensure a standard interface to plugins and easily make them configurable in your project, plugin code and example configurations are mapped to a string via python entrypoints in setup.py Plugin Packaging Plugins need to define one entrypoint with their plugin type and plugin class # OPM recognized plugin types class PluginTypes(str, Enum): PHAL = \"ovos.plugin.phal\" ADMIN = \"ovos.plugin.phal.admin\" SKILL = \"ovos.plugin.skill\" VAD = \"ovos.plugin.VAD\" PHONEME = \"ovos.plugin.g2p\" AUDIO = 'mycroft.plugin.audioservice' STT = 'mycroft.plugin.stt' TTS = 'mycroft.plugin.tts' WAKEWORD = 'mycroft.plugin.wake_word' TRANSLATE = \"neon.plugin.lang.translate\" LANG_DETECT = \"neon.plugin.lang.detect\" UTTERANCE_TRANSFORMER = \"neon.plugin.text\" METADATA_TRANSFORMER = \"neon.plugin.metadata\" AUDIO_TRANSFORMER = \"neon.plugin.audio\" QUESTION_SOLVER = \"neon.plugin.solver\" COREFERENCE_SOLVER = \"intentbox.coreference\" KEYWORD_EXTRACTION = \"intentbox.keywords\" UTTERANCE_SEGMENTATION = \"intentbox.segmentation\" TOKENIZATION = \"intentbox.tokenization\" POSTAG = \"intentbox.postag\" plugins can also optionally provide metadata about language support and sample configs via the {plugin_type}.config entrypoint A typical setup.py for a plugin looks like this from setuptools import setup ### replace this data with your plugin specific info PLUGIN_TYPE = \"mycroft.plugin.stt\" # see Enum above PLUGIN_NAME = \"ovos-stt-plugin-name\" PLUGIN_PKG = PLUGIN_NAME.replace(\"-\", \"_\") PLUGIN_CLAZZ = \"MyPlugin\" PLUGIN_CONFIGS = \"MyPluginConfig\" ### PLUGIN_ENTRY_POINT = f'{PLUGIN_NAME} = {PLUGIN_PKG}:{PLUGIN_CLAZZ}' CONFIG_ENTRY_POINT = f'{PLUGIN_NAME}.config = {PLUGIN_PKG}:{PLUGIN_CONFIGS}' # add version, author, license, description.... setup( name=PLUGIN_NAME, version='0.1.0', packages=[PLUGIN_PKG], install_requires=[\"speechrecognition>=3.8.1\", \"ovos-plugin-manager>=0.0.1\"], keywords='mycroft ovos plugin', entry_points={PLUGIN_TYPE: PLUGIN_ENTRY_POINT, f'{PLUGIN_TYPE}.config': CONFIG_ENTRY_POINT} ) Projects using OPM OPM plugins are know to be natively supported by the following projects (non-exhaustive list) ovos-core ovos-local-backend ovos-tts-server ovos-stt-http-server ovos-translate-server neon-core HiveMind voice satellite Additionally, some plugins (AudioService, WakeWord, TTS and STT) are also backwards compatible with mycroft-core","title":"Plugin Manager"},{"location":"OPM/#opm","text":"OPM is the OVOS Plugin Manager , this base package provides arbitrary plugins to the ovos ecosystem OPM plugins import their base classes from OPM making them portable and independent of core, plugins can be used in your standalone projects By using OPM you can ensure a standard interface to plugins and easily make them configurable in your project, plugin code and example configurations are mapped to a string via python entrypoints in setup.py","title":"OPM"},{"location":"OPM/#plugin-packaging","text":"Plugins need to define one entrypoint with their plugin type and plugin class # OPM recognized plugin types class PluginTypes(str, Enum): PHAL = \"ovos.plugin.phal\" ADMIN = \"ovos.plugin.phal.admin\" SKILL = \"ovos.plugin.skill\" VAD = \"ovos.plugin.VAD\" PHONEME = \"ovos.plugin.g2p\" AUDIO = 'mycroft.plugin.audioservice' STT = 'mycroft.plugin.stt' TTS = 'mycroft.plugin.tts' WAKEWORD = 'mycroft.plugin.wake_word' TRANSLATE = \"neon.plugin.lang.translate\" LANG_DETECT = \"neon.plugin.lang.detect\" UTTERANCE_TRANSFORMER = \"neon.plugin.text\" METADATA_TRANSFORMER = \"neon.plugin.metadata\" AUDIO_TRANSFORMER = \"neon.plugin.audio\" QUESTION_SOLVER = \"neon.plugin.solver\" COREFERENCE_SOLVER = \"intentbox.coreference\" KEYWORD_EXTRACTION = \"intentbox.keywords\" UTTERANCE_SEGMENTATION = \"intentbox.segmentation\" TOKENIZATION = \"intentbox.tokenization\" POSTAG = \"intentbox.postag\" plugins can also optionally provide metadata about language support and sample configs via the {plugin_type}.config entrypoint A typical setup.py for a plugin looks like this from setuptools import setup ### replace this data with your plugin specific info PLUGIN_TYPE = \"mycroft.plugin.stt\" # see Enum above PLUGIN_NAME = \"ovos-stt-plugin-name\" PLUGIN_PKG = PLUGIN_NAME.replace(\"-\", \"_\") PLUGIN_CLAZZ = \"MyPlugin\" PLUGIN_CONFIGS = \"MyPluginConfig\" ### PLUGIN_ENTRY_POINT = f'{PLUGIN_NAME} = {PLUGIN_PKG}:{PLUGIN_CLAZZ}' CONFIG_ENTRY_POINT = f'{PLUGIN_NAME}.config = {PLUGIN_PKG}:{PLUGIN_CONFIGS}' # add version, author, license, description.... setup( name=PLUGIN_NAME, version='0.1.0', packages=[PLUGIN_PKG], install_requires=[\"speechrecognition>=3.8.1\", \"ovos-plugin-manager>=0.0.1\"], keywords='mycroft ovos plugin', entry_points={PLUGIN_TYPE: PLUGIN_ENTRY_POINT, f'{PLUGIN_TYPE}.config': CONFIG_ENTRY_POINT} )","title":"Plugin Packaging"},{"location":"OPM/#projects-using-opm","text":"OPM plugins are know to be natively supported by the following projects (non-exhaustive list) ovos-core ovos-local-backend ovos-tts-server ovos-stt-http-server ovos-translate-server neon-core HiveMind voice satellite Additionally, some plugins (AudioService, WakeWord, TTS and STT) are also backwards compatible with mycroft-core","title":"Projects using OPM"},{"location":"PHAL/","text":"PHAL PHAL is our Platform/Hardware Abstraction Layer, it completely replaces the concept of hardcoded \"enclosure\" from mycroft-core Any number of plugins providing functionality can be loaded and validated at runtime, plugins can be system integrations to handle things like reboot and shutdown, or hardware drivers such as mycroft mark2 plugin PHAL plugins can perform actions such as hardware detection before loading, eg, the mark2 plugin will not load if it does not detect the sj201 hat. This makes plugins safe to install and bundle by default in our base images to PHAL or not to PHAL In mycroft-core the equivalent to PHAL plugins would usually be shipped as skills or hardcoded in OVOS sometimes it may be unclear if we should develop a skill or plugin, there isn't a one size fits all answer, in some circumstances it may make sense to create both a plugin and a companion skill Plugins Platform/Hardware specific integrations are loaded by PHAL, these plugins can handle all sorts of system activities Plugin Description ovos-PHAL-plugin-alsa volume control ovos-PHAL-plugin-system reboot / shutdown / factory reset ovos-PHAL-plugin-mk1 mycroft mark1 integration ovos-PHAL-plugin-respeaker-2mic respeaker 2mic hat integration ovos-PHAL-plugin-respeaker-4mic respeaker 4mic hat integration ovos-PHAL-plugin-wifi-setup wifi setup (central plugin) ovos-PHAL-plugin-gui-network-client wifi setup (GUI interface) ovos-PHAL-plugin-balena-wifi wifi setup (hotspot) ovos-PHAL-plugin-network-manager wifi setup (network manager) ovos-PHAL-plugin-ipgeo automatic geolocation (IP address) ovos-PHAL-plugin-gpsd automatic geolocation (GPS) neon-phal-plugin-linear_led LED control for the mycroft mark2 Template PHAL plugins do not follow a strict template, they are usually event listeners that perform certain actions and integrate with other components from ovos_bus_client import Message from ovos_plugin_manager.phal import PHALPlugin class MyPHALPluginValidator: @staticmethod def validate(config=None): \"\"\" this method is called before loading the plugin. If it returns False the plugin is not loaded. This allows a plugin to run platform checks\"\"\" return True class MyPHALPlugin(PHALPlugin): validator = MyPHALPluginValidator def __init__(self, bus=None, config=None): super().__init__(bus=bus, name=\"ovos-PHAL-plugin-NAME\", config=config) # register events for plugin self.bus.on(\"my.event\", self.handle_event) def handle_event(self, message): # TODO plugin stuff self.bus.emit(Message(\"my.event.response\")) def shutdown(self): # cleanly remove any event listeners and perform shutdown actions self.bus.remove(\"my.event\", self.handle_event) super().shutdown() You can find plugin packaging documentation here Admin PHAL AdminPHAL performs the exact same function as PHAL, but plugins it loads will have root privileges. This service is intended for handling any OS-level interactions requiring escalation of privileges. Be very careful when installing Admin plugins and scrutinize them closely NOTE: Because this service runs as root, plugins it loads are responsible for not writing configuration changes which would result in breaking config file permissions. Service to use AdminPHAL create a launcher /usr/libexec/mycroft-systemd-admin-phal import sdnotify from ovos_PHAL.admin import main n = sdnotify.SystemdNotifier() def notify_ready(): n.notify('READY=1') print('Startup of Admin service complete') def notify_stopping(): n.notify('STOPPING=1') print('Stopping Admin service') main(ready_hook=notify_ready, stopping_hook=notify_stopping) and system service /usr/lib/systemd/user/mycroft-admin-phal.service [Unit] Description=Admin PHAL PartOf=mycroft.service After=mycroft-messagebus.service [Service] Type=notify TimeoutStopSec=30 Restart=always User=root ExecStart=/usr/libexec/mycroft-systemd-admin-phal [Install] WantedBy=mycroft.service Admin Plugins AdminPlugins are just like regular PHAL plugins that run with root privileges A plugin needs to identify itself as an admin plugin via it's entry point, PHAL will not load Admin plugins and AdminPHAL will not load regular plugins Admin plugins will only load if their configuration contains \"enabled\": true . All admin plugins need to be explicitly enabled You can find plugin packaging documentation here","title":"Platform/Hardware Abstraction Layer"},{"location":"PHAL/#phal","text":"PHAL is our Platform/Hardware Abstraction Layer, it completely replaces the concept of hardcoded \"enclosure\" from mycroft-core Any number of plugins providing functionality can be loaded and validated at runtime, plugins can be system integrations to handle things like reboot and shutdown, or hardware drivers such as mycroft mark2 plugin PHAL plugins can perform actions such as hardware detection before loading, eg, the mark2 plugin will not load if it does not detect the sj201 hat. This makes plugins safe to install and bundle by default in our base images","title":"PHAL"},{"location":"PHAL/#to-phal-or-not-to-phal","text":"In mycroft-core the equivalent to PHAL plugins would usually be shipped as skills or hardcoded in OVOS sometimes it may be unclear if we should develop a skill or plugin, there isn't a one size fits all answer, in some circumstances it may make sense to create both a plugin and a companion skill","title":"to PHAL or not to PHAL"},{"location":"PHAL/#plugins","text":"Platform/Hardware specific integrations are loaded by PHAL, these plugins can handle all sorts of system activities Plugin Description ovos-PHAL-plugin-alsa volume control ovos-PHAL-plugin-system reboot / shutdown / factory reset ovos-PHAL-plugin-mk1 mycroft mark1 integration ovos-PHAL-plugin-respeaker-2mic respeaker 2mic hat integration ovos-PHAL-plugin-respeaker-4mic respeaker 4mic hat integration ovos-PHAL-plugin-wifi-setup wifi setup (central plugin) ovos-PHAL-plugin-gui-network-client wifi setup (GUI interface) ovos-PHAL-plugin-balena-wifi wifi setup (hotspot) ovos-PHAL-plugin-network-manager wifi setup (network manager) ovos-PHAL-plugin-ipgeo automatic geolocation (IP address) ovos-PHAL-plugin-gpsd automatic geolocation (GPS) neon-phal-plugin-linear_led LED control for the mycroft mark2","title":"Plugins"},{"location":"PHAL/#template","text":"PHAL plugins do not follow a strict template, they are usually event listeners that perform certain actions and integrate with other components from ovos_bus_client import Message from ovos_plugin_manager.phal import PHALPlugin class MyPHALPluginValidator: @staticmethod def validate(config=None): \"\"\" this method is called before loading the plugin. If it returns False the plugin is not loaded. This allows a plugin to run platform checks\"\"\" return True class MyPHALPlugin(PHALPlugin): validator = MyPHALPluginValidator def __init__(self, bus=None, config=None): super().__init__(bus=bus, name=\"ovos-PHAL-plugin-NAME\", config=config) # register events for plugin self.bus.on(\"my.event\", self.handle_event) def handle_event(self, message): # TODO plugin stuff self.bus.emit(Message(\"my.event.response\")) def shutdown(self): # cleanly remove any event listeners and perform shutdown actions self.bus.remove(\"my.event\", self.handle_event) super().shutdown() You can find plugin packaging documentation here","title":"Template"},{"location":"PHAL/#admin-phal","text":"AdminPHAL performs the exact same function as PHAL, but plugins it loads will have root privileges. This service is intended for handling any OS-level interactions requiring escalation of privileges. Be very careful when installing Admin plugins and scrutinize them closely NOTE: Because this service runs as root, plugins it loads are responsible for not writing configuration changes which would result in breaking config file permissions.","title":"Admin PHAL"},{"location":"PHAL/#service","text":"to use AdminPHAL create a launcher /usr/libexec/mycroft-systemd-admin-phal import sdnotify from ovos_PHAL.admin import main n = sdnotify.SystemdNotifier() def notify_ready(): n.notify('READY=1') print('Startup of Admin service complete') def notify_stopping(): n.notify('STOPPING=1') print('Stopping Admin service') main(ready_hook=notify_ready, stopping_hook=notify_stopping) and system service /usr/lib/systemd/user/mycroft-admin-phal.service [Unit] Description=Admin PHAL PartOf=mycroft.service After=mycroft-messagebus.service [Service] Type=notify TimeoutStopSec=30 Restart=always User=root ExecStart=/usr/libexec/mycroft-systemd-admin-phal [Install] WantedBy=mycroft.service","title":"Service"},{"location":"PHAL/#admin-plugins","text":"AdminPlugins are just like regular PHAL plugins that run with root privileges A plugin needs to identify itself as an admin plugin via it's entry point, PHAL will not load Admin plugins and AdminPHAL will not load regular plugins Admin plugins will only load if their configuration contains \"enabled\": true . All admin plugins need to be explicitly enabled You can find plugin packaging documentation here","title":"Admin Plugins"},{"location":"audio_service/","text":"Audio Service The audio service is responsible for handling TTS and simple sounds playback TTS Two TTS plugins may be loaded at once, if the primary plugin fails for some reason the second plugin will be used. This allows you to have a lower quality offline voice as fallback to account for internet outages, this ensures your device can always give you feedback \"tts\": { \"pulse_duck\": false, // plugins to load \"module\": \"ovos-tts-plugin-server\", \"fallback_module\": \"ovos-tts-plugin-mimic\", // individual plugin configs \"ovos-tts-plugin-server\": { \"host\": \"https://tts.smartgic.io/piper\", \"v2\": true, \"verify_ssl\": true, \"tts_timeout\": 5, } } Skill Methods skills can use self.play_audio , self.acknowledge , self.speak and self.speak_dialog methods to interact with ovos-audio def play_audio(self, filename: str, instant: bool = False): \"\"\" Queue and audio file for playback @param filename: File to play @param instant: if True audio will be played instantly instead of queued with TTS \"\"\" def acknowledge(self): \"\"\" Acknowledge a successful request. This method plays a sound to acknowledge a request that does not require a verbal response. This is intended to provide simple feedback to the user that their request was handled successfully. \"\"\" def speak(self, utterance: str, expect_response: bool = False, wait: Union[bool, int] = False): \"\"\"Speak a sentence. Args: utterance (str): sentence mycroft should speak expect_response (bool): set to True if Mycroft should listen for a response immediately after speaking the utterance. wait (Union[bool, int]): set to True to block while the text is being spoken for 15 seconds. Alternatively, set to an integer to specify a timeout in seconds. \"\"\" def speak_dialog(self, key: str, data: Optional[dict] = None, expect_response: bool = False, wait: Union[bool, int] = False): \"\"\" Speak a random sentence from a dialog file. Args: key (str): dialog file key (e.g. \"hello\" to speak from the file \"locale/en-us/hello.dialog\") data (dict): information used to populate sentence expect_response (bool): set to True if Mycroft should listen for a response immediately after speaking the utterance. wait (Union[bool, int]): set to True to block while the text is being spoken for 15 seconds. Alternatively, set to an integer to specify a timeout in seconds. \"\"\" to play sounds via bus messages emit \"mycroft.audio.play_sound\" or \"mycroft.audio.queue\" with data {\"uri\": \"path/sound.mp3\"} PlaybackThread ovos-audio implements a queue for sounds, any OVOS component can queue a sound for playback. Usually only TTS speech is queue for playback, but sounds effects may also be queued for richer experiences, for example in a story telling skill The PlaybackThread ensures sounds don't play over each other but instead sequentially, listening might be triggered after TTS finishes playing if requested in the \"speak\" message shorts sounds can be played outside the PlaybackThread, usually when instant feedback is required, such as in the listening sound or on error sounds You can configure default sounds and the playback commands under mycroft.conf // File locations of sounds to play for default events \"sounds\": { \"start_listening\": \"snd/start_listening.wav\", \"end_listening\": \"snd/end_listening.wav\", \"acknowledge\": \"snd/acknowledge.mp3\", \"error\": \"snd/error.mp3\" }, // Mechanism used to play WAV audio files // by default ovos-utils will try to detect best player \"play_wav_cmdline\": \"paplay %1 --stream-name=mycroft-voice\", // Mechanism used to play MP3 audio files // by default ovos-utils will try to detect best player \"play_mp3_cmdline\": \"mpg123 %1\", // Mechanism used to play OGG audio files // by default ovos-utils will try to detect best player \"play_ogg_cmdline\": \"ogg123 -q %1\", NOTE: by default the playback commands are not set and OVOS will try to determine the best way to play a sound automatically Native playback Usually playback is triggered in response to originating bus message, eg \"recognizer_loop:utterance\" , this message contains metadata that is used to determine if playback should happen. message.context may contain a source and destination, playback is only triggered if a message destination is a native_source or if missing (considered a broadcast). This separation of native sources allows remote clients such as an android app to interact with OVOS without the actual device where ovos-core is running repeating all TTS and music playback out loud You can learn more about message targeting here By default, only utterances originating from the speech client are considered native \"Audio\": { \"native_sources\": [\"debug_cli\", \"audio\"] } Transformer Plugins NEW in ovos-core version 0.0.8 Similarly to audio transformers in ovos-dinkum-listener , the utterance and audio data generated by TTS are exposed to a set of plugins that can transform them before playback Dialog Transformers Similarly to utterance transformers in core, ovos-audio exposes utterance and message. context to a set of plugins that can transform it before TTS stage The utterance to be spoken is sent sequentially to all transformer plugins, ordered by priority (developer defined), until finally it is sent to the TTS stage To enable a transformer add it to mycroft.conf // To enable a dialog transformer plugin just add it's name with any relevant config // these plugins can mutate utterances before TTS \"dialog_transformers\": { \"ovos-dialog-translation-plugin\": {}, \"ovos-dialog-transformer-openai-plugin\": { \"rewrite_prompt\": \"rewrite the text as if you were explaining it to a 5 year old\" } } TTS Transformers The audio to be spoken is sent sequentially to all transformer plugins, ordered by priority (developer defined), until finally it played back to the user NOTE : Does not work with StreamingTTS To enable a transformer add it to mycroft.conf // To enable a tts transformer plugin just add it's name with any relevant config // these plugins can mutate audio after TTS \"tts_transformers\": { \"ovos-tts-transformer-sox-plugin\": { \"default_effects\": { \"speed\": {\"factor\": 1.1} } } } Classic Audio Service DEPRECATION WARNING - ovos-core version 0.0.8 is moving long-form media playback to ovos-media service, this is a work in progress and will be a default in version 0.1.0 when ovos-media is introduced ovos-media is a work in progress, it does not yet ship with OVOS by default, but it can be manually enabled In order to use ovos-media you need to enable the OCP pipeline in ovos-core and to disable the old audio service disabling old audio service { \"enable_old_audioservice\": false } DEPRECATION WARNING - \"enable_old_audioservice\": true will use the old OCP that shipped as an audio plugin in order to work in classic mycroft You can enable additional Audio plugins under the \"Audio\" section of mycroft.conf OCP will decide when to call the Audio service and what plugin to use \"Audio\": { \"backends\": { \"OCP\": { \"type\": \"ovos_common_play\", \"active\": true }, \"simple\": { \"type\": \"ovos_audio_simple\", \"active\": true }, \"vlc\": { \"type\": \"ovos_vlc\", \"active\": true } } },","title":"Audio"},{"location":"audio_service/#audio-service","text":"The audio service is responsible for handling TTS and simple sounds playback","title":"Audio Service"},{"location":"audio_service/#tts","text":"Two TTS plugins may be loaded at once, if the primary plugin fails for some reason the second plugin will be used. This allows you to have a lower quality offline voice as fallback to account for internet outages, this ensures your device can always give you feedback \"tts\": { \"pulse_duck\": false, // plugins to load \"module\": \"ovos-tts-plugin-server\", \"fallback_module\": \"ovos-tts-plugin-mimic\", // individual plugin configs \"ovos-tts-plugin-server\": { \"host\": \"https://tts.smartgic.io/piper\", \"v2\": true, \"verify_ssl\": true, \"tts_timeout\": 5, } }","title":"TTS"},{"location":"audio_service/#skill-methods","text":"skills can use self.play_audio , self.acknowledge , self.speak and self.speak_dialog methods to interact with ovos-audio def play_audio(self, filename: str, instant: bool = False): \"\"\" Queue and audio file for playback @param filename: File to play @param instant: if True audio will be played instantly instead of queued with TTS \"\"\" def acknowledge(self): \"\"\" Acknowledge a successful request. This method plays a sound to acknowledge a request that does not require a verbal response. This is intended to provide simple feedback to the user that their request was handled successfully. \"\"\" def speak(self, utterance: str, expect_response: bool = False, wait: Union[bool, int] = False): \"\"\"Speak a sentence. Args: utterance (str): sentence mycroft should speak expect_response (bool): set to True if Mycroft should listen for a response immediately after speaking the utterance. wait (Union[bool, int]): set to True to block while the text is being spoken for 15 seconds. Alternatively, set to an integer to specify a timeout in seconds. \"\"\" def speak_dialog(self, key: str, data: Optional[dict] = None, expect_response: bool = False, wait: Union[bool, int] = False): \"\"\" Speak a random sentence from a dialog file. Args: key (str): dialog file key (e.g. \"hello\" to speak from the file \"locale/en-us/hello.dialog\") data (dict): information used to populate sentence expect_response (bool): set to True if Mycroft should listen for a response immediately after speaking the utterance. wait (Union[bool, int]): set to True to block while the text is being spoken for 15 seconds. Alternatively, set to an integer to specify a timeout in seconds. \"\"\" to play sounds via bus messages emit \"mycroft.audio.play_sound\" or \"mycroft.audio.queue\" with data {\"uri\": \"path/sound.mp3\"}","title":"Skill Methods"},{"location":"audio_service/#playbackthread","text":"ovos-audio implements a queue for sounds, any OVOS component can queue a sound for playback. Usually only TTS speech is queue for playback, but sounds effects may also be queued for richer experiences, for example in a story telling skill The PlaybackThread ensures sounds don't play over each other but instead sequentially, listening might be triggered after TTS finishes playing if requested in the \"speak\" message shorts sounds can be played outside the PlaybackThread, usually when instant feedback is required, such as in the listening sound or on error sounds You can configure default sounds and the playback commands under mycroft.conf // File locations of sounds to play for default events \"sounds\": { \"start_listening\": \"snd/start_listening.wav\", \"end_listening\": \"snd/end_listening.wav\", \"acknowledge\": \"snd/acknowledge.mp3\", \"error\": \"snd/error.mp3\" }, // Mechanism used to play WAV audio files // by default ovos-utils will try to detect best player \"play_wav_cmdline\": \"paplay %1 --stream-name=mycroft-voice\", // Mechanism used to play MP3 audio files // by default ovos-utils will try to detect best player \"play_mp3_cmdline\": \"mpg123 %1\", // Mechanism used to play OGG audio files // by default ovos-utils will try to detect best player \"play_ogg_cmdline\": \"ogg123 -q %1\", NOTE: by default the playback commands are not set and OVOS will try to determine the best way to play a sound automatically","title":"PlaybackThread"},{"location":"audio_service/#native-playback","text":"Usually playback is triggered in response to originating bus message, eg \"recognizer_loop:utterance\" , this message contains metadata that is used to determine if playback should happen. message.context may contain a source and destination, playback is only triggered if a message destination is a native_source or if missing (considered a broadcast). This separation of native sources allows remote clients such as an android app to interact with OVOS without the actual device where ovos-core is running repeating all TTS and music playback out loud You can learn more about message targeting here By default, only utterances originating from the speech client are considered native \"Audio\": { \"native_sources\": [\"debug_cli\", \"audio\"] }","title":"Native playback"},{"location":"audio_service/#transformer-plugins","text":"NEW in ovos-core version 0.0.8 Similarly to audio transformers in ovos-dinkum-listener , the utterance and audio data generated by TTS are exposed to a set of plugins that can transform them before playback","title":"Transformer Plugins"},{"location":"audio_service/#dialog-transformers","text":"Similarly to utterance transformers in core, ovos-audio exposes utterance and message. context to a set of plugins that can transform it before TTS stage The utterance to be spoken is sent sequentially to all transformer plugins, ordered by priority (developer defined), until finally it is sent to the TTS stage To enable a transformer add it to mycroft.conf // To enable a dialog transformer plugin just add it's name with any relevant config // these plugins can mutate utterances before TTS \"dialog_transformers\": { \"ovos-dialog-translation-plugin\": {}, \"ovos-dialog-transformer-openai-plugin\": { \"rewrite_prompt\": \"rewrite the text as if you were explaining it to a 5 year old\" } }","title":"Dialog Transformers"},{"location":"audio_service/#tts-transformers","text":"The audio to be spoken is sent sequentially to all transformer plugins, ordered by priority (developer defined), until finally it played back to the user NOTE : Does not work with StreamingTTS To enable a transformer add it to mycroft.conf // To enable a tts transformer plugin just add it's name with any relevant config // these plugins can mutate audio after TTS \"tts_transformers\": { \"ovos-tts-transformer-sox-plugin\": { \"default_effects\": { \"speed\": {\"factor\": 1.1} } } }","title":"TTS Transformers"},{"location":"audio_service/#classic-audio-service","text":"DEPRECATION WARNING - ovos-core version 0.0.8 is moving long-form media playback to ovos-media service, this is a work in progress and will be a default in version 0.1.0 when ovos-media is introduced ovos-media is a work in progress, it does not yet ship with OVOS by default, but it can be manually enabled In order to use ovos-media you need to enable the OCP pipeline in ovos-core and to disable the old audio service disabling old audio service { \"enable_old_audioservice\": false } DEPRECATION WARNING - \"enable_old_audioservice\": true will use the old OCP that shipped as an audio plugin in order to work in classic mycroft You can enable additional Audio plugins under the \"Audio\" section of mycroft.conf OCP will decide when to call the Audio service and what plugin to use \"Audio\": { \"backends\": { \"OCP\": { \"type\": \"ovos_common_play\", \"active\": true }, \"simple\": { \"type\": \"ovos_audio_simple\", \"active\": true }, \"vlc\": { \"type\": \"ovos_vlc\", \"active\": true } } },","title":"Classic Audio Service"},{"location":"backend/","text":"Backend Services OVOS supports connecting devices to a backend, allowing you to manage multiple OVOS devices from a single location Developers do not need to worry about backend details in their applications and skills You can interact with the backends in your applications with ovos-backend-client Identity Information A unique uuid and pairing information generated by registering with Home is stored in: ~/.config/mycroft/identity/identity2.json <-- DO NOT SHARE THIS WITH OTHERS! This file uniquely identifies your device and should be kept safe Backend Options Personal backend - self hosted Selene - https://api.mycroft.ai ( Deprecated - Selene is dead since February 2024) Offline - support for setting your own api keys and query services directly API Offline Personal Selene Admin yes [1] yes no Device yes yes yes Metrics yes yes yes Dataset yes yes yes OAuth yes yes yes Wolfram yes [2] yes yes Geolocate yes yes yes STT yes [2] yes yes Weather yes [2] yes yes Email yes [2] yes yes [1] will update user level mycroft.conf [2] needs additional configuration (eg. credentials) Offline Backend OVOS by default runs without a backend, in this case you will need to configure api keys manually by default the configuration uses the offline backend { \"server\": { \"backend_type\": \"offline\" } } Personal Backend Personal backend is a reverse engineered alternative to selene that predates it It provides the same functionality for devices and packs some extra options It is not intended to serve different users or thousands of devices, there are no user accounts! This is currently the only way to run a vanilla mycroft-core device offline edit your configuration to use your own personal backend instance { \"server\": { \"backend_type\": \"personal\", \"url\": \"http://0.0.0.0:6712\", \"version\": \"v1\", \"update\": true, \"metrics\": true, \"sync_skill_settings\": true } } source code Selene The official mycroft home backend is called selene, users need to create an account and pair devices with the mycroft servers. With Mycroft's demise selene finally went down on February 2024 Support for selene was fully removed during ovos-core version 0.0.8 Selene is AGPL licensed: backend source code frontend source code","title":"Backend"},{"location":"backend/#backend-services","text":"OVOS supports connecting devices to a backend, allowing you to manage multiple OVOS devices from a single location Developers do not need to worry about backend details in their applications and skills You can interact with the backends in your applications with ovos-backend-client","title":"Backend Services"},{"location":"backend/#identity-information","text":"A unique uuid and pairing information generated by registering with Home is stored in: ~/.config/mycroft/identity/identity2.json <-- DO NOT SHARE THIS WITH OTHERS! This file uniquely identifies your device and should be kept safe","title":"Identity Information"},{"location":"backend/#backend-options","text":"Personal backend - self hosted Selene - https://api.mycroft.ai ( Deprecated - Selene is dead since February 2024) Offline - support for setting your own api keys and query services directly API Offline Personal Selene Admin yes [1] yes no Device yes yes yes Metrics yes yes yes Dataset yes yes yes OAuth yes yes yes Wolfram yes [2] yes yes Geolocate yes yes yes STT yes [2] yes yes Weather yes [2] yes yes Email yes [2] yes yes [1] will update user level mycroft.conf [2] needs additional configuration (eg. credentials)","title":"Backend Options"},{"location":"backend/#offline-backend","text":"OVOS by default runs without a backend, in this case you will need to configure api keys manually by default the configuration uses the offline backend { \"server\": { \"backend_type\": \"offline\" } }","title":"Offline Backend"},{"location":"backend/#personal-backend","text":"Personal backend is a reverse engineered alternative to selene that predates it It provides the same functionality for devices and packs some extra options It is not intended to serve different users or thousands of devices, there are no user accounts! This is currently the only way to run a vanilla mycroft-core device offline edit your configuration to use your own personal backend instance { \"server\": { \"backend_type\": \"personal\", \"url\": \"http://0.0.0.0:6712\", \"version\": \"v1\", \"update\": true, \"metrics\": true, \"sync_skill_settings\": true } } source code","title":"Personal Backend"},{"location":"backend/#selene","text":"The official mycroft home backend is called selene, users need to create an account and pair devices with the mycroft servers. With Mycroft's demise selene finally went down on February 2024 Support for selene was fully removed during ovos-core version 0.0.8 Selene is AGPL licensed: backend source code frontend source code","title":"Selene"},{"location":"backend_client/","text":"OVOS Backend Client Python client library for interaction with several supported backends under a single unified interface Geolocation from ovos_backend_client.api import GeolocationApi geo = GeolocationApi() data = geo.get_geolocation(\"Lisbon Portugal\") # {'city': 'Lisboa', # 'country': 'Portugal', # 'latitude': 38.7077507, # 'longitude': -9.1365919, # 'timezone': 'Europe/Lisbon'} Remote Settings To interact with skill settings on selene from ovos_backend_client.settings import RemoteSkillSettings # in ovos-core skill_id is deterministic and safe s = RemoteSkillSettings(\"skill.author\") s.download() s.settings[\"existing_value\"] = True s.settings[\"new_value\"] = \"will NOT show up in UI\" s.upload() # auto generate new settings meta for all new values before uploading s.settings[\"new_value\"] = \"will show up in UI\" s.generate_meta() # now \"new_value\" is in meta s.upload() Selene Cloud by hijacking skill settings we allows storing arbitrary data in selene and use it across devices and skills from ovos_backend_client.cloud import SeleneCloud cloud = SeleneCloud() cloud.add_entry(\"test\", {\"secret\": \"NOT ENCRYPTED MAN\"}) data = cloud.get_entry(\"test\") an encrypted version is also supported if you dont trust selene! from ovos_backend_client.cloud import SecretSeleneCloud k = \"D8fmXEP5VqzVw2HE\" # you need this to read back the data cloud = SecretSeleneCloud(k) cloud.add_entry(\"test\", {\"secret\": \"secret data, selene cant read this\"}) data = cloud.get_entry(\"test\") OAuth Retrieving the tokens in a skill does not depend on the selected backend, the mechanism to register a token is backend specific First you need to authorize the application, this can be done with ovos-config-assistant if running offline or ovos-backend-manager if using personal backend If using selene there is no automated process to add a token, you need to contact support@mycroft.ai from ovos_backend_client.api import OAuthApi, BackendType # api = OAuthApi() # determine oauth backend from mycroft.conf api = OAuthApi(backend_type=BackendType.OFFLINE) # explicitly use ovos-backend-manager oauth token_json = api.get_oauth_token(\"spotify\") OpenWeatherMap Proxy from ovos_backend_client.api import OpenWeatherMapApi owm = OpenWeatherMapApi() data = owm.get_weather() # dict - see api docs from owm onecall api Wolfram Alpha proxy from ovos_backend_client.api import WolframAlphaApi wolf = WolframAlphaApi() answer = wolf.spoken(\"what is the speed of light\") # The speed of light has a value of about 300 million meters per second data = wolf.full_results(\"2+2\") # dict - see api docs from wolfram STT a companion stt plugin is available - ovos-stt-plugin-selene DEPRECATED - use ovos-stt-server instead Admin Api (personal backend only!) since local backend does not provide a web ui a admin api can be used to manage your devices from ovos_backend_client.api import AdminApi admin = AdminApi(\"secret_admin_key\") uuid = \"...\" # check identity2.json in the device you want to manage # manually pair a device identity_json = admin.pair(uuid) # set device info info = {\"opt_in\": True, \"name\": \"my_device\", \"device_location\": \"kitchen\", \"email\": \"notifications@me.com\", \"isolated_skills\": False, \"lang\": \"en-us\"} admin.set_device_info(uuid, info) # set device preferences prefs = {\"time_format\": \"full\", \"date_format\": \"DMY\", \"system_unit\": \"metric\", \"lang\": \"en-us\", \"wake_word\": \"hey_mycroft\", \"ww_config\": {\"phonemes\": \"HH EY . M AY K R AO F T\", \"module\": \"ovos-ww-plugin-pocketsphinx\", \"threshold\": 1e-90}, \"tts_module\": \"ovos-tts-plugin-mimic\", \"tts_config\": {\"voice\": \"ap\"}} admin.set_device_prefs(uuid, prefs) # set location data loc = { \"city\": { \"code\": \"Lawrence\", \"name\": \"Lawrence\", \"state\": { \"code\": \"KS\", \"name\": \"Kansas\", \"country\": { \"code\": \"US\", \"name\": \"United States\" } } }, \"coordinate\": { \"latitude\": 38.971669, \"longitude\": -95.23525 }, \"timezone\": { \"code\": \"America/Chicago\", \"name\": \"Central Standard Time\", \"dstOffset\": 3600000, \"offset\": -21600000 } } admin.set_device_location(uuid, loc)","title":"Backend Client"},{"location":"backend_client/#ovos-backend-client","text":"Python client library for interaction with several supported backends under a single unified interface","title":"OVOS Backend Client"},{"location":"backend_client/#geolocation","text":"from ovos_backend_client.api import GeolocationApi geo = GeolocationApi() data = geo.get_geolocation(\"Lisbon Portugal\") # {'city': 'Lisboa', # 'country': 'Portugal', # 'latitude': 38.7077507, # 'longitude': -9.1365919, # 'timezone': 'Europe/Lisbon'}","title":"Geolocation"},{"location":"backend_client/#remote-settings","text":"To interact with skill settings on selene from ovos_backend_client.settings import RemoteSkillSettings # in ovos-core skill_id is deterministic and safe s = RemoteSkillSettings(\"skill.author\") s.download() s.settings[\"existing_value\"] = True s.settings[\"new_value\"] = \"will NOT show up in UI\" s.upload() # auto generate new settings meta for all new values before uploading s.settings[\"new_value\"] = \"will show up in UI\" s.generate_meta() # now \"new_value\" is in meta s.upload()","title":"Remote Settings"},{"location":"backend_client/#selene-cloud","text":"by hijacking skill settings we allows storing arbitrary data in selene and use it across devices and skills from ovos_backend_client.cloud import SeleneCloud cloud = SeleneCloud() cloud.add_entry(\"test\", {\"secret\": \"NOT ENCRYPTED MAN\"}) data = cloud.get_entry(\"test\") an encrypted version is also supported if you dont trust selene! from ovos_backend_client.cloud import SecretSeleneCloud k = \"D8fmXEP5VqzVw2HE\" # you need this to read back the data cloud = SecretSeleneCloud(k) cloud.add_entry(\"test\", {\"secret\": \"secret data, selene cant read this\"}) data = cloud.get_entry(\"test\")","title":"Selene Cloud"},{"location":"backend_client/#oauth","text":"Retrieving the tokens in a skill does not depend on the selected backend, the mechanism to register a token is backend specific First you need to authorize the application, this can be done with ovos-config-assistant if running offline or ovos-backend-manager if using personal backend If using selene there is no automated process to add a token, you need to contact support@mycroft.ai from ovos_backend_client.api import OAuthApi, BackendType # api = OAuthApi() # determine oauth backend from mycroft.conf api = OAuthApi(backend_type=BackendType.OFFLINE) # explicitly use ovos-backend-manager oauth token_json = api.get_oauth_token(\"spotify\")","title":"OAuth"},{"location":"backend_client/#openweathermap-proxy","text":"from ovos_backend_client.api import OpenWeatherMapApi owm = OpenWeatherMapApi() data = owm.get_weather() # dict - see api docs from owm onecall api","title":"OpenWeatherMap Proxy"},{"location":"backend_client/#wolfram-alpha-proxy","text":"from ovos_backend_client.api import WolframAlphaApi wolf = WolframAlphaApi() answer = wolf.spoken(\"what is the speed of light\") # The speed of light has a value of about 300 million meters per second data = wolf.full_results(\"2+2\") # dict - see api docs from wolfram","title":"Wolfram Alpha proxy"},{"location":"backend_client/#stt","text":"a companion stt plugin is available - ovos-stt-plugin-selene DEPRECATED - use ovos-stt-server instead","title":"STT"},{"location":"backend_client/#admin-api-personal-backend-only","text":"since local backend does not provide a web ui a admin api can be used to manage your devices from ovos_backend_client.api import AdminApi admin = AdminApi(\"secret_admin_key\") uuid = \"...\" # check identity2.json in the device you want to manage # manually pair a device identity_json = admin.pair(uuid) # set device info info = {\"opt_in\": True, \"name\": \"my_device\", \"device_location\": \"kitchen\", \"email\": \"notifications@me.com\", \"isolated_skills\": False, \"lang\": \"en-us\"} admin.set_device_info(uuid, info) # set device preferences prefs = {\"time_format\": \"full\", \"date_format\": \"DMY\", \"system_unit\": \"metric\", \"lang\": \"en-us\", \"wake_word\": \"hey_mycroft\", \"ww_config\": {\"phonemes\": \"HH EY . M AY K R AO F T\", \"module\": \"ovos-ww-plugin-pocketsphinx\", \"threshold\": 1e-90}, \"tts_module\": \"ovos-tts-plugin-mimic\", \"tts_config\": {\"voice\": \"ap\"}} admin.set_device_prefs(uuid, prefs) # set location data loc = { \"city\": { \"code\": \"Lawrence\", \"name\": \"Lawrence\", \"state\": { \"code\": \"KS\", \"name\": \"Kansas\", \"country\": { \"code\": \"US\", \"name\": \"United States\" } } }, \"coordinate\": { \"latitude\": 38.971669, \"longitude\": -95.23525 }, \"timezone\": { \"code\": \"America/Chicago\", \"name\": \"Central Standard Time\", \"dstOffset\": 3600000, \"offset\": -21600000 } } admin.set_device_location(uuid, loc)","title":"Admin Api (personal backend only!)"},{"location":"bus_client/","text":"OVOS MessageBus Client The OVOS MessageBus Client is a Python module providing a simple interface for the OVOS MessageBus. It can be used to connect to OVOS, send messages, and react to messages sent by the OVOS system. The module is available through PyPI.org or directly on Github . OVOSBusClient() The OVOSBusClient() object can be setup to connect to any host and port as well as any endpoint on that host. this makes it quite versatile and will work on the main bus as well as on a gui bus. If no arguments are provided it will try to connect to a local instance of OVOS on the default endpoint and port. Message() The Message object is a representation of the messagebus message, this will always contain a message type but can also contain data and context. Data is usually real information while the context typically contain information on where the message originated or who the intended recipient is. Message('MESSAGE_TYPE', data={'meaning': 42}, context={'origin': 'A.Dent'}) Sending a Message In the following example we setup an instance of the MessageBusClient then emit a speak Message with a data payload. OVOS would consume this Message and speak \"Hello World\". from ovos_bus_client import MessageBusClient, Message print('Setting up client to connect to a local mycroft instance') client = MessageBusClient() client.run_in_thread() print('Sending speak message...') client.emit(Message('speak', data={'utterance': 'Hello World'})) Listening for a Message In the following example we setup an instance of the MessageBusClient. We then define a function print_utterance that prints the utterance from a Message. This is registered as a handler for the speak Message. Finally we call the run_forever() method to keep the client running. If this code had run before the example above, it would catch the speak Message we emitted and print: OVOS said \"Hello World\" from ovos_bus_client import MessageBusClient, Message print('Setting up client to connect to a local ovos instance') client = MessageBusClient() def print_utterance(message): print('OVOS said \"{}\"'.format(message.data.get('utterance'))) print('Registering handler for speak message...') client.on('speak', print_utterance) client.run_forever() Manually connecting to the MessageBus Here is an example Python script to connect to the messagebus : #! /usr/bin/env python3 import sys from websocket import create_connection uri = 'ws://' + sys.argv[1] + ':8181/core' ws = create_connection(uri) print(\"Sending \" + sys.argv[2] + \" to \" + uri + \"...\") if len(sys.argv) >= 4: data = sys.argv[3] else: data = \"{}\" message = '{\"type\": \"' + sys.argv[2] + '\", \"data\": ' + data +'}' result = ws.send(message) print(\"Receiving...\" ) result = ws.recv() print(\"Received '%s'\" % result) ws.close()","title":"Messagebus Client"},{"location":"bus_client/#ovos-messagebus-client","text":"The OVOS MessageBus Client is a Python module providing a simple interface for the OVOS MessageBus. It can be used to connect to OVOS, send messages, and react to messages sent by the OVOS system. The module is available through PyPI.org or directly on Github .","title":"OVOS MessageBus Client"},{"location":"bus_client/#ovosbusclient","text":"The OVOSBusClient() object can be setup to connect to any host and port as well as any endpoint on that host. this makes it quite versatile and will work on the main bus as well as on a gui bus. If no arguments are provided it will try to connect to a local instance of OVOS on the default endpoint and port.","title":"OVOSBusClient()"},{"location":"bus_client/#message","text":"The Message object is a representation of the messagebus message, this will always contain a message type but can also contain data and context. Data is usually real information while the context typically contain information on where the message originated or who the intended recipient is. Message('MESSAGE_TYPE', data={'meaning': 42}, context={'origin': 'A.Dent'})","title":"Message()"},{"location":"bus_client/#sending-a-message","text":"In the following example we setup an instance of the MessageBusClient then emit a speak Message with a data payload. OVOS would consume this Message and speak \"Hello World\". from ovos_bus_client import MessageBusClient, Message print('Setting up client to connect to a local mycroft instance') client = MessageBusClient() client.run_in_thread() print('Sending speak message...') client.emit(Message('speak', data={'utterance': 'Hello World'}))","title":"Sending a Message"},{"location":"bus_client/#listening-for-a-message","text":"In the following example we setup an instance of the MessageBusClient. We then define a function print_utterance that prints the utterance from a Message. This is registered as a handler for the speak Message. Finally we call the run_forever() method to keep the client running. If this code had run before the example above, it would catch the speak Message we emitted and print: OVOS said \"Hello World\" from ovos_bus_client import MessageBusClient, Message print('Setting up client to connect to a local ovos instance') client = MessageBusClient() def print_utterance(message): print('OVOS said \"{}\"'.format(message.data.get('utterance'))) print('Registering handler for speak message...') client.on('speak', print_utterance) client.run_forever()","title":"Listening for a Message"},{"location":"bus_client/#manually-connecting-to-the-messagebus","text":"Here is an example Python script to connect to the messagebus : #! /usr/bin/env python3 import sys from websocket import create_connection uri = 'ws://' + sys.argv[1] + ':8181/core' ws = create_connection(uri) print(\"Sending \" + sys.argv[2] + \" to \" + uri + \"...\") if len(sys.argv) >= 4: data = sys.argv[3] else: data = \"{}\" message = '{\"type\": \"' + sys.argv[2] + '\", \"data\": ' + data +'}' result = ws.send(message) print(\"Receiving...\" ) result = ws.recv() print(\"Received '%s'\" % result) ws.close()","title":"Manually connecting to the MessageBus"},{"location":"bus_service/","text":"Bus Service A Message Bus is mechanism for independent systems to communicate with each other using a set of messages for common commands or notifiers. In the OVOS ecosystem, the Messagebus is a websocket and the messages contain a message type with an optional JSON data packet. Some messages trigger actions and have side effects; some are simple notifiers of actions that either have occurred or are about to occur. The Messagebus connects the ovos-core processes and the Skills , and can also be joined by outside systems such as HiveMind . Messages can be sent from the producers and acted upon by Skills or other consumers within OVOS. You can think of the bus service as OVOS's nervous system The messagebus is considered an internal and private websocket, external clients should not connect directly to it. Configuration The messagebus has a dedicated section in mycroft.conf \"websocket\": { \"host\": \"127.0.0.1\", \"port\": 8181, \"route\": \"/core\", \"shared_connection\": true } Security in mycroft-core all skills share a bus connection, this allows malicious skills to manipulate it and affect other skills you can see a demonstration of this problem with BusBrickerSkill \"shared_connection\": false ensures each skill gets its own websocket connection and avoids this problem It is recommended you DO NOT change \"host\": \"127.0.0.1\" , this will ensure no outside world connections are allowed Exposing the bus Please do not expose the messagebus to the outside world! Anything connected to the bus can fully control OVOS, and OVOS usually has full control over the whole system! You can read more about the security issues over at Nhoya/MycroftAI-RCE If you need to connect to the bus from the outside world please check the companion project HiveMind Lots of guides for mycroft tell you to expose the websocket, please (re)read the info and links above, think 10 times before following steps blindly Messages A Message consists of a json payload, it contains a type , some data and a context . The context is considered to be metadata and might be changed at any time in transit, the context can contain anything depending on where the message came from, and often is completely empty. You can think of the message context as a sort of session data for a individual interaction, in general messages down the chain keep the context from the original message, most listeners (eg, skills) will only care about type and data . An index of all bus messages emitted or listened too by all the OVOS repositories can be found at https://openvoiceos.github.io/message_spec Session NEW in ovos-core version 0.0.8 The message.context may contain anything, but a special key is the \"session\" A Session contains per-request preferences and state data, external clients such as voice satellites have their own session the Session can be used to select user preferences or resume previous conversations by restoring skill state ovos-core always uses the reserved \"session_id\": \"default\" , and will fully manage the state of the default Session, individual clients are supposed to manage their own session and might ignore updates coming from ovos-core Things included in a Session: list of active skills (able to converse) list of intent contexts (follow up questions) utterance state (get_response vs regular utterance) language location tts / stt preferences (eg, voice) intent pipeline to be used site_id By default these values are populated from mycroft.conf , skills have some \"magic variables\" that also reflect the session, such as self.lang and self.location . NOTE Voice satellites will send a subset of their own mycroft.conf via their Session Transformers may modify the Session, for example to inject intent context It is up to individual skills to manage session, see the Session Aware Skills docs for details WARNING skills that are not session aware may keep a shared state may behave weirdly with voice satellites Message Targeting ovos-core uses the message context to add metadata about the messages themselves, where do they come from and what are they intended for. the Message object provides the following methods: message.forward method, keeps previous context. message continues going to same destination message.reply method swaps \"source\" with \"destination\" message goes back to source The context destination parameter in the original message can be set to a list with any number of intended targets: bus.emit(Message('recognizer_loop:utterance', data, context={'destination': ['audio', 'kde'], 'source': \"remote_service\")) Internal routing intent service will .reply to the original utterance message all skill/intent service messages are .forward (from previous intent service .reply ) WARNING skills sending their own messages might not respect this NOTE : HiveMind depends on the \"source\" and \"destination\" keys to route responses back to their respective satellites Sources OVOS injects the context when it emits an utterance, usually when spoken via STT service STT will identify itself as audio mycroft.conf contains a native_sources section you can configure to change how the audio service processes external requests \"Audio\": { \"native_sources\": [\"debug_cli\", \"audio\"] } Destinations TTS checks the message context if it's the intended target for the message and will only speak in the following conditions: Explicitly targeted i.e. the destination is \"audio\" destination is set to None destination is missing completely The idea is that for example when the android app is used to access OpenVoiceOS the device at home shouldn't start to speak. TTS will be executed when \"audio\" is the destination (configurable in \"native_sources\" ) A missing destination or if the destination is set to None is interpreted as a multicast and should trigger all output capable processes (be it the audio service, a web-interface, the KDE plasmoid or maybe the android app)","title":"Bus"},{"location":"bus_service/#bus-service","text":"A Message Bus is mechanism for independent systems to communicate with each other using a set of messages for common commands or notifiers. In the OVOS ecosystem, the Messagebus is a websocket and the messages contain a message type with an optional JSON data packet. Some messages trigger actions and have side effects; some are simple notifiers of actions that either have occurred or are about to occur. The Messagebus connects the ovos-core processes and the Skills , and can also be joined by outside systems such as HiveMind . Messages can be sent from the producers and acted upon by Skills or other consumers within OVOS. You can think of the bus service as OVOS's nervous system The messagebus is considered an internal and private websocket, external clients should not connect directly to it.","title":"Bus Service"},{"location":"bus_service/#configuration","text":"The messagebus has a dedicated section in mycroft.conf \"websocket\": { \"host\": \"127.0.0.1\", \"port\": 8181, \"route\": \"/core\", \"shared_connection\": true }","title":"Configuration"},{"location":"bus_service/#security","text":"in mycroft-core all skills share a bus connection, this allows malicious skills to manipulate it and affect other skills you can see a demonstration of this problem with BusBrickerSkill \"shared_connection\": false ensures each skill gets its own websocket connection and avoids this problem It is recommended you DO NOT change \"host\": \"127.0.0.1\" , this will ensure no outside world connections are allowed","title":"Security"},{"location":"bus_service/#exposing-the-bus","text":"Please do not expose the messagebus to the outside world! Anything connected to the bus can fully control OVOS, and OVOS usually has full control over the whole system! You can read more about the security issues over at Nhoya/MycroftAI-RCE If you need to connect to the bus from the outside world please check the companion project HiveMind Lots of guides for mycroft tell you to expose the websocket, please (re)read the info and links above, think 10 times before following steps blindly","title":"Exposing the bus"},{"location":"bus_service/#messages","text":"A Message consists of a json payload, it contains a type , some data and a context . The context is considered to be metadata and might be changed at any time in transit, the context can contain anything depending on where the message came from, and often is completely empty. You can think of the message context as a sort of session data for a individual interaction, in general messages down the chain keep the context from the original message, most listeners (eg, skills) will only care about type and data . An index of all bus messages emitted or listened too by all the OVOS repositories can be found at https://openvoiceos.github.io/message_spec","title":"Messages"},{"location":"bus_service/#session","text":"NEW in ovos-core version 0.0.8 The message.context may contain anything, but a special key is the \"session\" A Session contains per-request preferences and state data, external clients such as voice satellites have their own session the Session can be used to select user preferences or resume previous conversations by restoring skill state ovos-core always uses the reserved \"session_id\": \"default\" , and will fully manage the state of the default Session, individual clients are supposed to manage their own session and might ignore updates coming from ovos-core Things included in a Session: list of active skills (able to converse) list of intent contexts (follow up questions) utterance state (get_response vs regular utterance) language location tts / stt preferences (eg, voice) intent pipeline to be used site_id By default these values are populated from mycroft.conf , skills have some \"magic variables\" that also reflect the session, such as self.lang and self.location . NOTE Voice satellites will send a subset of their own mycroft.conf via their Session Transformers may modify the Session, for example to inject intent context It is up to individual skills to manage session, see the Session Aware Skills docs for details WARNING skills that are not session aware may keep a shared state may behave weirdly with voice satellites","title":"Session"},{"location":"bus_service/#message-targeting","text":"ovos-core uses the message context to add metadata about the messages themselves, where do they come from and what are they intended for. the Message object provides the following methods: message.forward method, keeps previous context. message continues going to same destination message.reply method swaps \"source\" with \"destination\" message goes back to source The context destination parameter in the original message can be set to a list with any number of intended targets: bus.emit(Message('recognizer_loop:utterance', data, context={'destination': ['audio', 'kde'], 'source': \"remote_service\"))","title":"Message Targeting"},{"location":"bus_service/#internal-routing","text":"intent service will .reply to the original utterance message all skill/intent service messages are .forward (from previous intent service .reply ) WARNING skills sending their own messages might not respect this NOTE : HiveMind depends on the \"source\" and \"destination\" keys to route responses back to their respective satellites","title":"Internal routing"},{"location":"bus_service/#sources","text":"OVOS injects the context when it emits an utterance, usually when spoken via STT service STT will identify itself as audio mycroft.conf contains a native_sources section you can configure to change how the audio service processes external requests \"Audio\": { \"native_sources\": [\"debug_cli\", \"audio\"] }","title":"Sources"},{"location":"bus_service/#destinations","text":"TTS checks the message context if it's the intended target for the message and will only speak in the following conditions: Explicitly targeted i.e. the destination is \"audio\" destination is set to None destination is missing completely The idea is that for example when the android app is used to access OpenVoiceOS the device at home shouldn't start to speak. TTS will be executed when \"audio\" is the destination (configurable in \"native_sources\" ) A missing destination or if the destination is set to None is interpreted as a multicast and should trigger all output capable processes (be it the audio service, a web-interface, the KDE plasmoid or maybe the android app)","title":"Destinations"},{"location":"common_query/","text":"Common Query Framework The Common Query Framework handles the common use case of \"general information\" or question answering. Many Skills may implement handlers for \"what is X\" or \"when did Y\", the Common Query Framework allows all these Skills be queried and a single \"best\" answer to be selected. This is similar to the Common Play Framework that handles the common use of \"playing\" music or other media. The Common Query Skill System is led by the Common Query Pipeline. The pipeline handles queries matching a question pattern such as \"What is the height of the Eiffle Tower\" and \"When is lunch\". A matched question will be sent to all Skills based upon the CommonQuerySkill base class. The Skills will return wether they can answer the query along with an answer when applicable. The \"best\" match will be selected and spoken to the user. CommonQuerySkill A Skill interfacing with the Common Query Framework inherits from the the CommonQuerySkill and needs to define a method CQS_match_query_phrase() taking an utterance as argument. The general structure is: from ovos_workshop.skills.common_query_skill import CommonQuerySkill, CQSMatchLevel class MyCommonQuerySkill(CommonQuerySkill): def CQS_match_query_phrase(self, utt): # Parsing implementation # [...] return (utt, CQSMatchLevel.LEVEL, answer_string) The CQS_match_query_phrase() method will parse the utterance and determine if it can handle the query. if it can't answer it will return None and if it can answer it will return a data tuple with the format ((str)Input Query, CQSMatchLevel, (str)Answer Text) The input query is returned to map the query to the answer. CQSMatchLevel is an Enum with the possible values CQSMatchLevel.EXACT : The Skill is very confident that it has the precise answer the user is looking for. There was a category match and a known entity is referenced. CQSMatchLevel.CATEGORY : The Skill could determine that the type of question matches a category that the Skill is good at finding. CQSMatchLevel.GENERAL : This Skill tries to answer all questions and found an answer. To show visuals or take some other action in response to being selected, see the CQS_action() method below. An Example Let's make a simple Skill that tells us the age of the various Monty Python members. A quick draft looks like this. (You can find the complete code here ) from ovos_workshop.skills.common_query_skill import CommonQuerySkill, CQSMatchLevel # Dict mapping python members to their age and whether they're alive or dead PYTHONS = { 'eric idle': (77,'alive'), 'michael palin': (77, 'alive'), 'john cleese': (80, 'alive'), 'graham chapman': (48, 'dead'), 'terry gilliam': (79, 'alive'), 'terry jones': (77, 'dead') } def python_in_utt(utterance): \"\"\"Find a monty python member in the utterance. Arguments: utterance (str): Sentence to check for Monty Python members Returns: (str) name of Monty Python member or None \"\"\" for key in PYTHONS: if key in utterance.lower(): # Return the found python return key # No python found return None class PythonAgeSkill(CommonQuerySkill): \"\"\"A Skill for checking the age of the python crew.\"\"\" def format_answer(self, python): \"\"\"Create string with answer for the specified \"python\" person.\"\"\" age, status = PYTHONS[python] if status == 'alive': return self.dialog_renderer.render('age_alive', {'person': python, 'age': age}) else: return self.dialog_renderer.render('age_dead', {'person': python, 'age': age}) def CQS_match_query_phrase(self, utt): \"\"\"Check the utterance if it is a question we can answer. Arguments: utt: The question Returns: tuple (input utterance, match level, response sentence, extra) \"\"\" # Check if this is an age query age_query = self.voc_match(utt, 'age') # Check if a monty python member is mentioned python = full_python_in_utt(utt) # If this is an age query and a monty python member is mentioned the # skill can answer this if age_query and python: # return high confidence return (utt, CQSMatchLevel.CATEGORY, self.format_answer(python)) else: return None As seen above the CQS_match_query_phrase() checks if this is an age related utterance and if the utterance contains the name of a Monty Python member. If both criteria are met it returns a match with a CQSMatchLevel.CATEGORY confidence together with a rendered dialog containing the answer. If both criteria are not fulfilled the method will return None indicating that it can't answer the query. This will be able to provide answers to queries such as \"how old is Graham Chapman\" \"what's Eric Idle's age\" To make this more exact we can add support for checking for the words \"monty python\", and if present return the highest confidence. The method for parsing the example is quite simplistic but there are many different toolkits out there for doing the question parsing. Adapt , little questions , padaos and many more! Match Confidence If we want to make sure this Skill is used when the user explicitly states it's the age of a Monty Python member, a slight modification to the Skill can be made: We'll change the end of the CQS_match_query_phrase() method to def CQS_match_query_phrase(self, utt): # (...) if 'monty python' in utt.lower(): confidence = CQSMatchLevel.EXACT else: confidence = CQSMatchLevel.CATEGORY # return high confidence return (utt, confidence, self.format_answer(python)) So if the utterance contains the phrase \"monty python\" the confidence will be set to CQSMatchLevel.EXACT making the Skill very very likely to be chosen to answer the query. CQS_action() In some cases the Skill should do additional operations when selected as the best match. It could be prepared for follow-up questions or show an image on the screen. The CQS_action() method allows for this, when a Skill is selected this method will be called. Let's make our Python Age Skill gloat that it was selected by adding a CQS_action() method like this: where phrase is the same phrase that were sent to CQS_match_query_phrase() and data is optional additional data from the query matching method. def CQS_action(self, utt, data): self.log.info('I got selected! What you say about that Wolfram Alpha Skill!?!?') Now each time the Skill is called the above message will be added to the log! Not very useful you say? Hmm, yes... let's add something useful, like show the age on the Mark-1 display. To accomplish this we need to get the age into the CQS_action() method in some way. we could store last age in as an internal variable but the more elegant way is to send data as part of the match tuple. To do this we must extend the returned match tuple from CQS_match_query_phrase() with a data entry. So the return statement becomes def CQS_match_query_phrase(self, utt): # (...) data = {'age': PYTHONS[python], 'python': python} return (utt, confidence, self.format_answer(python), data) The data structure declared here will be sent to CQS_Action() and we can update the method to def CQS_action(self, utt, data): self.log.info('I got selected! What you say about that Wolfram Alpha Skill!?!?') age = data.get('age') if age: self.log.info(f'Showing the age {age}') self.enclosure.mouth_text(str(age))","title":"Common Query Skills"},{"location":"common_query/#common-query-framework","text":"The Common Query Framework handles the common use case of \"general information\" or question answering. Many Skills may implement handlers for \"what is X\" or \"when did Y\", the Common Query Framework allows all these Skills be queried and a single \"best\" answer to be selected. This is similar to the Common Play Framework that handles the common use of \"playing\" music or other media. The Common Query Skill System is led by the Common Query Pipeline. The pipeline handles queries matching a question pattern such as \"What is the height of the Eiffle Tower\" and \"When is lunch\". A matched question will be sent to all Skills based upon the CommonQuerySkill base class. The Skills will return wether they can answer the query along with an answer when applicable. The \"best\" match will be selected and spoken to the user.","title":"Common Query Framework"},{"location":"common_query/#commonqueryskill","text":"A Skill interfacing with the Common Query Framework inherits from the the CommonQuerySkill and needs to define a method CQS_match_query_phrase() taking an utterance as argument. The general structure is: from ovos_workshop.skills.common_query_skill import CommonQuerySkill, CQSMatchLevel class MyCommonQuerySkill(CommonQuerySkill): def CQS_match_query_phrase(self, utt): # Parsing implementation # [...] return (utt, CQSMatchLevel.LEVEL, answer_string) The CQS_match_query_phrase() method will parse the utterance and determine if it can handle the query. if it can't answer it will return None and if it can answer it will return a data tuple with the format ((str)Input Query, CQSMatchLevel, (str)Answer Text) The input query is returned to map the query to the answer. CQSMatchLevel is an Enum with the possible values CQSMatchLevel.EXACT : The Skill is very confident that it has the precise answer the user is looking for. There was a category match and a known entity is referenced. CQSMatchLevel.CATEGORY : The Skill could determine that the type of question matches a category that the Skill is good at finding. CQSMatchLevel.GENERAL : This Skill tries to answer all questions and found an answer. To show visuals or take some other action in response to being selected, see the CQS_action() method below.","title":"CommonQuerySkill"},{"location":"common_query/#an-example","text":"Let's make a simple Skill that tells us the age of the various Monty Python members. A quick draft looks like this. (You can find the complete code here ) from ovos_workshop.skills.common_query_skill import CommonQuerySkill, CQSMatchLevel # Dict mapping python members to their age and whether they're alive or dead PYTHONS = { 'eric idle': (77,'alive'), 'michael palin': (77, 'alive'), 'john cleese': (80, 'alive'), 'graham chapman': (48, 'dead'), 'terry gilliam': (79, 'alive'), 'terry jones': (77, 'dead') } def python_in_utt(utterance): \"\"\"Find a monty python member in the utterance. Arguments: utterance (str): Sentence to check for Monty Python members Returns: (str) name of Monty Python member or None \"\"\" for key in PYTHONS: if key in utterance.lower(): # Return the found python return key # No python found return None class PythonAgeSkill(CommonQuerySkill): \"\"\"A Skill for checking the age of the python crew.\"\"\" def format_answer(self, python): \"\"\"Create string with answer for the specified \"python\" person.\"\"\" age, status = PYTHONS[python] if status == 'alive': return self.dialog_renderer.render('age_alive', {'person': python, 'age': age}) else: return self.dialog_renderer.render('age_dead', {'person': python, 'age': age}) def CQS_match_query_phrase(self, utt): \"\"\"Check the utterance if it is a question we can answer. Arguments: utt: The question Returns: tuple (input utterance, match level, response sentence, extra) \"\"\" # Check if this is an age query age_query = self.voc_match(utt, 'age') # Check if a monty python member is mentioned python = full_python_in_utt(utt) # If this is an age query and a monty python member is mentioned the # skill can answer this if age_query and python: # return high confidence return (utt, CQSMatchLevel.CATEGORY, self.format_answer(python)) else: return None As seen above the CQS_match_query_phrase() checks if this is an age related utterance and if the utterance contains the name of a Monty Python member. If both criteria are met it returns a match with a CQSMatchLevel.CATEGORY confidence together with a rendered dialog containing the answer. If both criteria are not fulfilled the method will return None indicating that it can't answer the query. This will be able to provide answers to queries such as \"how old is Graham Chapman\" \"what's Eric Idle's age\" To make this more exact we can add support for checking for the words \"monty python\", and if present return the highest confidence. The method for parsing the example is quite simplistic but there are many different toolkits out there for doing the question parsing. Adapt , little questions , padaos and many more!","title":"An Example"},{"location":"common_query/#match-confidence","text":"If we want to make sure this Skill is used when the user explicitly states it's the age of a Monty Python member, a slight modification to the Skill can be made: We'll change the end of the CQS_match_query_phrase() method to def CQS_match_query_phrase(self, utt): # (...) if 'monty python' in utt.lower(): confidence = CQSMatchLevel.EXACT else: confidence = CQSMatchLevel.CATEGORY # return high confidence return (utt, confidence, self.format_answer(python)) So if the utterance contains the phrase \"monty python\" the confidence will be set to CQSMatchLevel.EXACT making the Skill very very likely to be chosen to answer the query.","title":"Match Confidence"},{"location":"common_query/#cqs_action","text":"In some cases the Skill should do additional operations when selected as the best match. It could be prepared for follow-up questions or show an image on the screen. The CQS_action() method allows for this, when a Skill is selected this method will be called. Let's make our Python Age Skill gloat that it was selected by adding a CQS_action() method like this: where phrase is the same phrase that were sent to CQS_match_query_phrase() and data is optional additional data from the query matching method. def CQS_action(self, utt, data): self.log.info('I got selected! What you say about that Wolfram Alpha Skill!?!?') Now each time the Skill is called the above message will be added to the log! Not very useful you say? Hmm, yes... let's add something useful, like show the age on the Mark-1 display. To accomplish this we need to get the age into the CQS_action() method in some way. we could store last age in as an internal variable but the more elegant way is to send data as part of the match tuple. To do this we must extend the returned match tuple from CQS_match_query_phrase() with a data entry. So the return statement becomes def CQS_match_query_phrase(self, utt): # (...) data = {'age': PYTHONS[python], 'python': python} return (utt, confidence, self.format_answer(python), data) The data structure declared here will be sent to CQS_Action() and we can update the method to def CQS_action(self, utt, data): self.log.info('I got selected! What you say about that Wolfram Alpha Skill!?!?') age = data.get('age') if age: self.log.info(f'Showing the age {age}') self.enclosure.mouth_text(str(age))","title":"CQS_action()"},{"location":"community/","text":"Projects incubated under OpenVoiceOS The following projects are fully maintained by our community under the OpenVoiceOS github organization Thank you guys! Projects OVOS uptime monitor ovos-docker ovos-installer raspbian-ovos projen skill generator/retrofitter Skills ovos-skill-easter-eggs ovos-skill-randomness Plugins ovos-ww-plugin-openWakeWord ovos-microphone-plugin-sounddevice ovos-PHAL-plugin-homeassistant ovos-translate-plugin-deepl","title":"Community"},{"location":"community/#projects-incubated-under-openvoiceos","text":"The following projects are fully maintained by our community under the OpenVoiceOS github organization Thank you guys!","title":"Projects incubated under OpenVoiceOS"},{"location":"community/#projects","text":"OVOS uptime monitor ovos-docker ovos-installer raspbian-ovos projen skill generator/retrofitter","title":"Projects"},{"location":"community/#skills","text":"ovos-skill-easter-eggs ovos-skill-randomness","title":"Skills"},{"location":"community/#plugins","text":"ovos-ww-plugin-openWakeWord ovos-microphone-plugin-sounddevice ovos-PHAL-plugin-homeassistant ovos-translate-plugin-deepl","title":"Plugins"},{"location":"config/","text":"Configuration Config Files When the configuration loader starts, it looks in several locations byorder, and loads ALL configurations. Keys that exist in multiple configuration files will be overridden by the last file to contain the value. This process results in a minimal amount being written for a specific device and user, without modifying default distribution files. The main configuration files are commonly referred too as: default - the file shipped by the ovos-config package, containing default values system - located at /etc/mycroft/mycroft.conf , intended to be shipped by specific platforms such as pre built raspberry pi images for dedicated hardware or by your OS package manager remote - locate at ~/.config/mycroft/web_cache.json , if using a backend to manage multiple devices this is where any downloaded config would be stored user - located at ~/.config/mycroft/mycroft.conf , intended for users to modify, usually does not exist until created by a user Configuring Configuration There are a couple of special configuration keys that change the way the configuration stack loads. protected_keys A \"protected_keys\" configuration section may be added to a Default or System Config file (default /etc/mycroft/mycroft.conf ). This configuration section specifies other configuration keys that may not be specified in remote or user configurations. Keys may specify nested parameters with . to exclude specific keys within nested dictionaries. An example config could be: { \"protected_keys\": { \"remote\": [ \"gui_websocket.host\", \"websocket.host\" ], \"user\": [ \"gui_websocket.host\" ] } } This example specifies that config['gui_websocket']['host'] may be specified in user configuration, but not remote. config['websocket']['host'] may not be specified in user or remote config, so it will only consider default and system configurations. disable_user_config If this config parameter is set to True in Default or System configuration, no user configurations will be loaded (no XDG configuration paths). disable_remote_config If this config parameter is set to True in Default or System configuration, the remote configuration ( web_cache.json ) will not be loaded. Meta Configuration OS ENV vars The configuration files loaded can be modified by environment variables, or alternatively by ovos.conf (described in the next section) if Configuration() is called the following configs would be loaded in this order: {ovos-config-package}/mycroft.conf os.environ.get('MYCROFT_SYSTEM_CONFIG') or /etc/mycroft/mycroft.conf os.environ.get('MYCROFT_WEB_CACHE') or $XDG_CONFIG_PATH/mycroft/web_cache.json $XDG_CONFIG_DIRS/mycroft/mycroft.conf /etc/xdg/mycroft/mycroft.conf $XDG_CONFIG_HOME/mycroft/mycroft.conf (default ~/.config/mycroft/mycroft.conf ) Configuration files can be in either json or yaml format. json files must have .json or .conf file extensions, yaml files must have .yml or .yaml file extensions ovos.conf The ovos_config package determines which config files to load based on ovos.conf . This file is optional and does NOT need to exist. NOTE : You should not need to use or worry about this file except in extreme situations while mycroft.conf configures the voice assistant, ovos.conf configures the library all XDG paths across OpenVoiceOS packages build their paths taking \"base_folder\" from ovos.conf into consideration ovos.conf decides what files are loaded by the Configuration class described above, as an end user or skill developer you should never have to worry about this get_ovos_config will return default values that load mycroft.conf unless otherwise configured. ovos.conf files are loaded in the following order, with later files taking priority over earlier ones in the list: /etc/OpenVoiceOS/ovos.conf $XDG_CONFIG_DIRS/OpenVoiceOS/ovos.conf /etc/xdg/OpenVoiceOS/ovos.conf $XDG_CONFIG_HOME/OpenVoiceOS/ovos.conf (default ~/.config/OpenVoiceOS/ovos.conf ) A simple ovos_config should have a structure like: { \"base_folder\": \"mycroft\", \"config_filename\": \"mycroft.conf\", \"default_config_path\": \"<Absolute Path to ovos-config>/mycroft.conf\", \"module_overrides\": {}, \"submodule_mappings\": {} } Config in downstream packages ovos.conf allows downstream voice assistants such as neon-core to change their config files to neon.yaml { \"base_folder\": \"mycroft\", \"config_filename\": \"mycroft.conf\", \"default_config_path\": \"<Absolute Path to ovos-config>/mycroft.conf\", \"module_overrides\": { \"neon_core\": { \"base_folder\": \"neon\", \"config_filename\": \"neon.yaml\", \"default_config_path\": \"/etc/example/config/neon.yaml\" } }, \"submodule_mappings\": { \"neon_messagebus\": \"neon_core\", \"neon_speech\": \"neon_core\", \"neon_audio\": \"neon_core\", \"neon_gui\": \"neon_core\" } } Note : default_config_path should always be an absolute path. any manual override must specify an absolute path to a json or yaml config file. Using the above example, if Configuration() is called from neon-core , the following configs would be loaded in this order: /etc/example/config/neon.yaml os.environ.get('MYCROFT_SYSTEM_CONFIG') or /etc/neon/neon.yaml os.environ.get('MYCROFT_WEB_CACHE') or $XDG_CONFIG_PATH/neon/web_cache.json $XDG_CONFIG_DIRS/neon/neon.yaml /etc/xdg/neon/neon.yaml $XDG_CONFIG_HOME/neon/neon.yaml (default ~/.config/neon/neon.yaml ) A call to get_ovos_config from neon_core or neon_messagebus will return a configuration like: { \"base_folder\": \"neon\", \"config_filename\": \"neon.yaml\", \"default_config_path\": \"/etc/example/config/neon.yaml\", \"module_overrides\": { \"neon_core\": { \"base_folder\": \"neon\", \"config_filename\": \"neon.yaml\", \"default_config_path\": \"/etc/example/config/neon.yaml\" } }, \"submodule_mappings\": { \"neon_messagebus\": \"neon_core\", \"neon_speech\": \"neon_core\", \"neon_audio\": \"neon_core\", \"neon_gui\": \"neon_core\" } } If get_ovos_config was called from ovos_core with the same configuration file as the last example, the returned configuration would be: { \"base_folder\": \"mycroft\", \"config_filename\": \"mycroft.conf\", \"default_config_path\": \"<Path to ovos-config>/mycroft.conf\", \"module_overrides\": { \"neon_core\": { \"base_folder\": \"neon\", \"config_filename\": \"neon.yaml\", \"default_config_path\": \"/etc/example/config/neon.yaml\" } }, \"submodule_mappings\": { \"neon_messagebus\": \"neon_core\", \"neon_speech\": \"neon_core\", \"neon_audio\": \"neon_core\", \"neon_gui\": \"neon_core\" } } Both projects could be installed side by side and each would load their corresponding config files","title":"Configuration"},{"location":"config/#configuration","text":"","title":"Configuration"},{"location":"config/#config-files","text":"When the configuration loader starts, it looks in several locations byorder, and loads ALL configurations. Keys that exist in multiple configuration files will be overridden by the last file to contain the value. This process results in a minimal amount being written for a specific device and user, without modifying default distribution files. The main configuration files are commonly referred too as: default - the file shipped by the ovos-config package, containing default values system - located at /etc/mycroft/mycroft.conf , intended to be shipped by specific platforms such as pre built raspberry pi images for dedicated hardware or by your OS package manager remote - locate at ~/.config/mycroft/web_cache.json , if using a backend to manage multiple devices this is where any downloaded config would be stored user - located at ~/.config/mycroft/mycroft.conf , intended for users to modify, usually does not exist until created by a user","title":"Config Files"},{"location":"config/#configuring-configuration","text":"There are a couple of special configuration keys that change the way the configuration stack loads.","title":"Configuring Configuration"},{"location":"config/#protected_keys","text":"A \"protected_keys\" configuration section may be added to a Default or System Config file (default /etc/mycroft/mycroft.conf ). This configuration section specifies other configuration keys that may not be specified in remote or user configurations. Keys may specify nested parameters with . to exclude specific keys within nested dictionaries. An example config could be: { \"protected_keys\": { \"remote\": [ \"gui_websocket.host\", \"websocket.host\" ], \"user\": [ \"gui_websocket.host\" ] } } This example specifies that config['gui_websocket']['host'] may be specified in user configuration, but not remote. config['websocket']['host'] may not be specified in user or remote config, so it will only consider default and system configurations.","title":"protected_keys"},{"location":"config/#disable_user_config","text":"If this config parameter is set to True in Default or System configuration, no user configurations will be loaded (no XDG configuration paths).","title":"disable_user_config"},{"location":"config/#disable_remote_config","text":"If this config parameter is set to True in Default or System configuration, the remote configuration ( web_cache.json ) will not be loaded.","title":"disable_remote_config"},{"location":"config/#meta-configuration","text":"","title":"Meta Configuration"},{"location":"config/#os-env-vars","text":"The configuration files loaded can be modified by environment variables, or alternatively by ovos.conf (described in the next section) if Configuration() is called the following configs would be loaded in this order: {ovos-config-package}/mycroft.conf os.environ.get('MYCROFT_SYSTEM_CONFIG') or /etc/mycroft/mycroft.conf os.environ.get('MYCROFT_WEB_CACHE') or $XDG_CONFIG_PATH/mycroft/web_cache.json $XDG_CONFIG_DIRS/mycroft/mycroft.conf /etc/xdg/mycroft/mycroft.conf $XDG_CONFIG_HOME/mycroft/mycroft.conf (default ~/.config/mycroft/mycroft.conf ) Configuration files can be in either json or yaml format. json files must have .json or .conf file extensions, yaml files must have .yml or .yaml file extensions","title":"OS ENV vars"},{"location":"config/#ovosconf","text":"The ovos_config package determines which config files to load based on ovos.conf . This file is optional and does NOT need to exist. NOTE : You should not need to use or worry about this file except in extreme situations while mycroft.conf configures the voice assistant, ovos.conf configures the library all XDG paths across OpenVoiceOS packages build their paths taking \"base_folder\" from ovos.conf into consideration ovos.conf decides what files are loaded by the Configuration class described above, as an end user or skill developer you should never have to worry about this get_ovos_config will return default values that load mycroft.conf unless otherwise configured. ovos.conf files are loaded in the following order, with later files taking priority over earlier ones in the list: /etc/OpenVoiceOS/ovos.conf $XDG_CONFIG_DIRS/OpenVoiceOS/ovos.conf /etc/xdg/OpenVoiceOS/ovos.conf $XDG_CONFIG_HOME/OpenVoiceOS/ovos.conf (default ~/.config/OpenVoiceOS/ovos.conf ) A simple ovos_config should have a structure like: { \"base_folder\": \"mycroft\", \"config_filename\": \"mycroft.conf\", \"default_config_path\": \"<Absolute Path to ovos-config>/mycroft.conf\", \"module_overrides\": {}, \"submodule_mappings\": {} }","title":"ovos.conf"},{"location":"config/#config-in-downstream-packages","text":"ovos.conf allows downstream voice assistants such as neon-core to change their config files to neon.yaml { \"base_folder\": \"mycroft\", \"config_filename\": \"mycroft.conf\", \"default_config_path\": \"<Absolute Path to ovos-config>/mycroft.conf\", \"module_overrides\": { \"neon_core\": { \"base_folder\": \"neon\", \"config_filename\": \"neon.yaml\", \"default_config_path\": \"/etc/example/config/neon.yaml\" } }, \"submodule_mappings\": { \"neon_messagebus\": \"neon_core\", \"neon_speech\": \"neon_core\", \"neon_audio\": \"neon_core\", \"neon_gui\": \"neon_core\" } } Note : default_config_path should always be an absolute path. any manual override must specify an absolute path to a json or yaml config file. Using the above example, if Configuration() is called from neon-core , the following configs would be loaded in this order: /etc/example/config/neon.yaml os.environ.get('MYCROFT_SYSTEM_CONFIG') or /etc/neon/neon.yaml os.environ.get('MYCROFT_WEB_CACHE') or $XDG_CONFIG_PATH/neon/web_cache.json $XDG_CONFIG_DIRS/neon/neon.yaml /etc/xdg/neon/neon.yaml $XDG_CONFIG_HOME/neon/neon.yaml (default ~/.config/neon/neon.yaml ) A call to get_ovos_config from neon_core or neon_messagebus will return a configuration like: { \"base_folder\": \"neon\", \"config_filename\": \"neon.yaml\", \"default_config_path\": \"/etc/example/config/neon.yaml\", \"module_overrides\": { \"neon_core\": { \"base_folder\": \"neon\", \"config_filename\": \"neon.yaml\", \"default_config_path\": \"/etc/example/config/neon.yaml\" } }, \"submodule_mappings\": { \"neon_messagebus\": \"neon_core\", \"neon_speech\": \"neon_core\", \"neon_audio\": \"neon_core\", \"neon_gui\": \"neon_core\" } } If get_ovos_config was called from ovos_core with the same configuration file as the last example, the returned configuration would be: { \"base_folder\": \"mycroft\", \"config_filename\": \"mycroft.conf\", \"default_config_path\": \"<Path to ovos-config>/mycroft.conf\", \"module_overrides\": { \"neon_core\": { \"base_folder\": \"neon\", \"config_filename\": \"neon.yaml\", \"default_config_path\": \"/etc/example/config/neon.yaml\" } }, \"submodule_mappings\": { \"neon_messagebus\": \"neon_core\", \"neon_speech\": \"neon_core\", \"neon_audio\": \"neon_core\", \"neon_gui\": \"neon_core\" } } Both projects could be installed side by side and each would load their corresponding config files","title":"Config in downstream packages"},{"location":"context/","text":"Follow up questions NOTE: Conversational context is currently only available with the Adapt Intent Parser, and is not yet available for Padatious Keyword Contexts How tall is John Cleese? \"John Cleese is 196 centimeters\" Where's he from? \"He's from England\" Context is added manually by the Skill creator using either the self.set_context() method or the @adds_context() decorator. Consider the following intent handlers: @intent_handler(IntentBuilder().require('PythonPerson').require('Length')) def handle_length(self, message): python = message.data.get('PythonPerson') self.speak(f'{python} is {length_dict[python]} cm tall') @intent_handler(IntentBuilder().require('PythonPerson').require('WhereFrom')) def handle_from(self, message): python = message.data.get('PythonPerson') self.speak(f'{python} is from {from_dict[python]}') To interact with the above handlers the user would need to say User: How tall is John Cleese? Mycroft: John Cleese is 196 centimeters User: Where is John Cleese from? Mycroft: He's from England To get a more natural response the functions can be changed to let OVOS know which PythonPerson we're talking about by using the self.set_context() method to give context: @intent_handler(IntentBuilder().require('PythonPerson').require('Length')) def handle_length(self, message): # PythonPerson can be any of the Monty Python members python = message.data.get('PythonPerson') self.speak(f'{python} is {length_dict[python]} cm tall') self.set_context('PythonPerson', python) @intent_handler(IntentBuilder().require('PythonPerson').require('WhereFrom')) def handle_from(self, message): # PythonPerson can be any of the Monty Python members python = message.data.get('PythonPerson') self.speak(f'He is from {from_dict[python]}') self.set_context('PythonPerson', python) When either of the methods are called the PythonPerson keyword is added to OVOS's context, which means that if there is a match with Length but PythonPerson is missing OVOS will assume the last mention of that keyword. The interaction can now become the one described at the top of the page. User: How tall is John Cleese? OVOS detects the Length keyword and the PythonPerson keyword OVOS: 196 centimeters John Cleese is added to the current context User: Where's he from? OVOS detects the WhereFrom keyword but not any PythonPerson keyword. The Context Manager is activated and returns the latest entry of PythonPerson which is John Cleese OVOS: He's from England Cross Skill Context The context is limited by the keywords provided by the current Skill. But we can use context across skills via self.set_cross_skill_context to enable conversations with other Skills as well. @intent_handler(IntentBuilder().require(PythonPerson).require(WhereFrom)) def handle_from(self, message): # PythonPerson can be any of the Monty Python members python = message.data.get('PythonPerson') self.speak(f'He is from {from_dict[python]}') self.set_context('PythonPerson', python) # context for this skill only self.set_cross_skill_context('Location', from_dict[python]) # context for ALL skills In this example Location keyword is shared with the WeatherSkill User: Where is John Cleese from? Mycroft: He's from England User: What's the weather like over there? Mycroft: Raining and 14 degrees... Hint Keyword contexts Context do not need to have a value, their presence can be used to simply indicate a previous interaction happened In this case Context can also be implemented by using decorators instead of calling self.set_context from ovos_workshop.decorators import adds_context, removes_context class TeaSkill(OVOSSkill): @intent_handler(IntentBuilder('TeaIntent').require(\"TeaKeyword\")) @adds_context('MilkContext') def handle_tea_intent(self, message): self.milk = False self.speak('Of course, would you like Milk with that?', expect_response=True) @intent_handler(IntentBuilder('NoMilkIntent').require(\"NoKeyword\"). require('MilkContext').build()) @removes_context('MilkContext') @adds_context('HoneyContext') def handle_no_milk_intent(self, message): self.speak('all right, any Honey?', expect_response=True) NOTE : cross skill context is not yet exposed via decorators Using context to enable Intents To make sure certain Intents can't be triggered unless some previous stage in a conversation has occurred. Context can be used to create \"bubbles\" of available intent handlers. User: Hey Mycroft, bring me some Tea Mycroft: Of course, would you like Milk with that? User: No Mycroft: How about some Honey? User: All right then Mycroft: Here you go, here's your Tea with Honey from ovos_workshop.decorators import adds_context, removes_context class TeaSkill(OVOSSkill): @intent_handler(IntentBuilder('TeaIntent').require(\"TeaKeyword\")) @adds_context('MilkContext') def handle_tea_intent(self, message): self.milk = False self.speak('Of course, would you like Milk with that?', expect_response=True) @intent_handler(IntentBuilder('NoMilkIntent').require(\"NoKeyword\"). require('MilkContext').build()) @removes_context('MilkContext') @adds_context('HoneyContext') def handle_no_milk_intent(self, message): self.speak('all right, any Honey?', expect_response=True) @intent_handler(IntentBuilder('YesMilkIntent').require(\"YesKeyword\"). require('MilkContext').build()) @removes_context('MilkContext') @adds_context('HoneyContext') def handle_yes_milk_intent(self, message): self.milk = True self.speak('What about Honey?', expect_response=True) @intent_handler(IntentBuilder('NoHoneyIntent').require(\"NoKeyword\"). require('HoneyContext').build()) @removes_context('HoneyContext') def handle_no_honey_intent(self, message): if self.milk: self.speak('Heres your Tea with a dash of Milk') else: self.speak('Heres your Tea, straight up') @intent_handler(IntentBuilder('YesHoneyIntent').require(\"YesKeyword\"). require('HoneyContext').build()) @removes_context('HoneyContext') def handle_yes_honey_intent(self, message): if self.milk: self.speak('Heres your Tea with Milk and Honey') else: self.speak('Heres your Tea with Honey') When starting up only the TeaIntent will be available. When that has been triggered and MilkContext is added the MilkYesIntent and MilkNoIntent are available since the MilkContext is set. when a yes or no is received the MilkContext is removed and can't be accessed. In it's place the HoneyContext is added making the YesHoneyIntent and NoHoneyIntent available. You can find an example Tea Skill using conversational context on Github . As you can see, Conversational Context lends itself well to implementing a dialog tree or conversation tree .","title":"Follow up Questions"},{"location":"context/#follow-up-questions","text":"NOTE: Conversational context is currently only available with the Adapt Intent Parser, and is not yet available for Padatious","title":"Follow up questions"},{"location":"context/#keyword-contexts","text":"How tall is John Cleese? \"John Cleese is 196 centimeters\" Where's he from? \"He's from England\" Context is added manually by the Skill creator using either the self.set_context() method or the @adds_context() decorator. Consider the following intent handlers: @intent_handler(IntentBuilder().require('PythonPerson').require('Length')) def handle_length(self, message): python = message.data.get('PythonPerson') self.speak(f'{python} is {length_dict[python]} cm tall') @intent_handler(IntentBuilder().require('PythonPerson').require('WhereFrom')) def handle_from(self, message): python = message.data.get('PythonPerson') self.speak(f'{python} is from {from_dict[python]}') To interact with the above handlers the user would need to say User: How tall is John Cleese? Mycroft: John Cleese is 196 centimeters User: Where is John Cleese from? Mycroft: He's from England To get a more natural response the functions can be changed to let OVOS know which PythonPerson we're talking about by using the self.set_context() method to give context: @intent_handler(IntentBuilder().require('PythonPerson').require('Length')) def handle_length(self, message): # PythonPerson can be any of the Monty Python members python = message.data.get('PythonPerson') self.speak(f'{python} is {length_dict[python]} cm tall') self.set_context('PythonPerson', python) @intent_handler(IntentBuilder().require('PythonPerson').require('WhereFrom')) def handle_from(self, message): # PythonPerson can be any of the Monty Python members python = message.data.get('PythonPerson') self.speak(f'He is from {from_dict[python]}') self.set_context('PythonPerson', python) When either of the methods are called the PythonPerson keyword is added to OVOS's context, which means that if there is a match with Length but PythonPerson is missing OVOS will assume the last mention of that keyword. The interaction can now become the one described at the top of the page. User: How tall is John Cleese? OVOS detects the Length keyword and the PythonPerson keyword OVOS: 196 centimeters John Cleese is added to the current context User: Where's he from? OVOS detects the WhereFrom keyword but not any PythonPerson keyword. The Context Manager is activated and returns the latest entry of PythonPerson which is John Cleese OVOS: He's from England","title":"Keyword Contexts"},{"location":"context/#cross-skill-context","text":"The context is limited by the keywords provided by the current Skill. But we can use context across skills via self.set_cross_skill_context to enable conversations with other Skills as well. @intent_handler(IntentBuilder().require(PythonPerson).require(WhereFrom)) def handle_from(self, message): # PythonPerson can be any of the Monty Python members python = message.data.get('PythonPerson') self.speak(f'He is from {from_dict[python]}') self.set_context('PythonPerson', python) # context for this skill only self.set_cross_skill_context('Location', from_dict[python]) # context for ALL skills In this example Location keyword is shared with the WeatherSkill User: Where is John Cleese from? Mycroft: He's from England User: What's the weather like over there? Mycroft: Raining and 14 degrees...","title":"Cross Skill Context"},{"location":"context/#hint-keyword-contexts","text":"Context do not need to have a value, their presence can be used to simply indicate a previous interaction happened In this case Context can also be implemented by using decorators instead of calling self.set_context from ovos_workshop.decorators import adds_context, removes_context class TeaSkill(OVOSSkill): @intent_handler(IntentBuilder('TeaIntent').require(\"TeaKeyword\")) @adds_context('MilkContext') def handle_tea_intent(self, message): self.milk = False self.speak('Of course, would you like Milk with that?', expect_response=True) @intent_handler(IntentBuilder('NoMilkIntent').require(\"NoKeyword\"). require('MilkContext').build()) @removes_context('MilkContext') @adds_context('HoneyContext') def handle_no_milk_intent(self, message): self.speak('all right, any Honey?', expect_response=True) NOTE : cross skill context is not yet exposed via decorators","title":"Hint Keyword contexts"},{"location":"context/#using-context-to-enable-intents","text":"To make sure certain Intents can't be triggered unless some previous stage in a conversation has occurred. Context can be used to create \"bubbles\" of available intent handlers. User: Hey Mycroft, bring me some Tea Mycroft: Of course, would you like Milk with that? User: No Mycroft: How about some Honey? User: All right then Mycroft: Here you go, here's your Tea with Honey from ovos_workshop.decorators import adds_context, removes_context class TeaSkill(OVOSSkill): @intent_handler(IntentBuilder('TeaIntent').require(\"TeaKeyword\")) @adds_context('MilkContext') def handle_tea_intent(self, message): self.milk = False self.speak('Of course, would you like Milk with that?', expect_response=True) @intent_handler(IntentBuilder('NoMilkIntent').require(\"NoKeyword\"). require('MilkContext').build()) @removes_context('MilkContext') @adds_context('HoneyContext') def handle_no_milk_intent(self, message): self.speak('all right, any Honey?', expect_response=True) @intent_handler(IntentBuilder('YesMilkIntent').require(\"YesKeyword\"). require('MilkContext').build()) @removes_context('MilkContext') @adds_context('HoneyContext') def handle_yes_milk_intent(self, message): self.milk = True self.speak('What about Honey?', expect_response=True) @intent_handler(IntentBuilder('NoHoneyIntent').require(\"NoKeyword\"). require('HoneyContext').build()) @removes_context('HoneyContext') def handle_no_honey_intent(self, message): if self.milk: self.speak('Heres your Tea with a dash of Milk') else: self.speak('Heres your Tea, straight up') @intent_handler(IntentBuilder('YesHoneyIntent').require(\"YesKeyword\"). require('HoneyContext').build()) @removes_context('HoneyContext') def handle_yes_honey_intent(self, message): if self.milk: self.speak('Heres your Tea with Milk and Honey') else: self.speak('Heres your Tea with Honey') When starting up only the TeaIntent will be available. When that has been triggered and MilkContext is added the MilkYesIntent and MilkNoIntent are available since the MilkContext is set. when a yes or no is received the MilkContext is removed and can't be accessed. In it's place the HoneyContext is added making the YesHoneyIntent and NoHoneyIntent available. You can find an example Tea Skill using conversational context on Github . As you can see, Conversational Context lends itself well to implementing a dialog tree or conversation tree .","title":"Using context to enable Intents"},{"location":"converse/","text":"Converse Each Skill may define a converse() method. This method will be called anytime the Skill has been recently active and a new utterance is processed. The converse method expects a single argument which is a standard Mycroft Message object. This is the same object an intent handler receives. Converse methods must return a Boolean value. True if an utterance was handled, otherwise False. Basic usage Let's use a version of the Ice Cream Skill we've been building up and add a converse method to catch any brief statements of thanks that might directly follow an order. from ovos_workshop.skills import OVOSSkill from ovos_workshop.decorators import intent_handler class IceCreamSkill(OVOSSkill): def initialize(self): self.flavors = ['vanilla', 'chocolate', 'mint'] @intent_handler('request.icecream.intent') def handle_request_icecream(self): self.speak_dialog('welcome') selection = self.ask_selection(self.flavors, 'what.flavor') self.speak_dialog('coming-right-up', {'flavor': selection}) def converse(self, message): if self.voc_match(message.data['utterances'][0], 'Thankyou'): self.speak_dialog(\"you-are-welcome\") return True In this example: A User might request an ice cream which is handled by handle_request_icecream() The Skill would be added to the system Active Skill list for up to 5 minutes. Any utterance received by OVOS would trigger this Skills converse system whilst it is considered active. If the User followed up with a pleasantry such as \"Hey Mycroft, thanks\" - the converse method would match this vocab against the Thankyou.voc file in the Skill and speak the contents of the you-are-welcome.dialog file. The method would return True and the utterance would be consumed meaning the intent parsing service would never be triggered. Any utterance that did not match would be silently ignored and allowed to continue on to other converse methods and finally to the intent parsing service. WARNING skills that are not Session aware may behave weirdly with voice satellites, see the parrot skill for an example. Active Skill List A Skill is considered active if it has been called in the last 5 minutes. Skills are called in order of when they were last active. For example, if a user spoke the following commands: Hey Mycroft, set a timer for 10 minutes Hey Mycroft, what's the weather Then the utterance \"what's the weather\" would first be sent to the Timer Skill's converse() method, then to the intent service for normal handling where the Weather Skill would be called. As the Weather Skill was called it has now been added to the front of the Active Skills List. Hence, the next utterance received will be directed to: WeatherSkill.converse() TimerSkill.converse() Normal intent parsing service Making a Skill Active There are occasions where a Skill has not been triggered by the User, but it should still be considered \"Active\". In the case of our Ice Cream Skill - we might have a function that will execute when the customers order is ready. At this point, we also want to be responsive to the customers thanks, so we call self.activate() to manually add our Skill to the front of the Active Skills List. from ovos_workshop.skills import OVOSSkill from ovos_workshop.decorators import intent_handler class IceCreamSkill(OVOSSkill): def on_order_ready(self, message): self.activate() def handle_activate(self, message: Message): \"\"\" Called when this skill is considered active by the intent service; converse method will be called with every utterance. Override this method to do any optional preparation. @param message: `{self.skill_id}.activate` Message \"\"\" LOG.info(\"Skill has been activated\") Deactivating a Skill The active skill list will be pruned by ovos-core , any skills that have not been interacted with for longer than 5 minutes will be deactivated Individual Skills may react to this event, to clean up state or, in some rare cases, to reactivate themselves from ovos_workshop.skills import OVOSSkill class AlwaysActiveSkill(OVOSSkill): def handle_deactivate(self, message: Message): \"\"\" Called when this skill is no longer considered active by the intent service; converse method will not be called until skill is active again. Override this method to do any optional cleanup. @param message: `{self.skill_id}.deactivate` Message \"\"\" self.activate() A skill can also deactivate itself at any time from ovos_workshop.skills import OVOSSkill class LazySkill(OVOSSkill): def handle_intent(self, message: Message): self.speak(\"leave me alone\") self.deactivate() Security A malicious or badly designed skill using the converse method can potentially hijack the whole conversation loop and render the skills service unusable Some settings are exposed to add some limitations to which skills can be activated and under what circumstances The concept of \"converse priority\" is under active development \"skills\": { // converse stage configuration \"converse\": { // the default number of seconds a skill remains active, // if the user does not interact with the skill in this timespan it // will be deactivated, default 5 minutes (same as mycroft) \"timeout\": 300, // override of \"skill_timeouts\" per skill_id // you can configure specific skills to remain active longer \"skill_timeouts\": {}, // conversational mode has 3 modes of operations: // - \"accept_all\" # default mycroft-core behavior // - \"whitelist\" # only call converse for skills in \"converse_whitelist\" // - \"blacklist\" # only call converse for skills NOT in \"converse_blacklist\" \"converse_mode\": \"accept_all\", \"converse_whitelist\": [], \"converse_blacklist\": [], // converse activation has 4 modes of operations: // - \"accept_all\" # default mycroft-core behavior, any skill can // # activate itself unconditionally // - \"priority\" # skills can only activate themselves if no skill with // # higher priority is active // - \"whitelist\" # only skills in \"converse_whitelist\" can activate themselves // - \"blacklist\" # only skills NOT in converse \"converse_blacklist\" can activate themselves // NOTE: this does not apply for regular skill activation, only to skill // initiated activation requests, eg, self.make_active() \"converse_activation\": \"accept_all\", // number of consecutive times a skill is allowed to activate itself // per minute, -1 for no limit (default), 0 to disable self-activation \"max_activations\": -1, // override of \"max_activations\" per skill_id // you can configure specific skills to activate more/less often \"skill_activations\": {}, // if false only skills can activate themselves // if true any skill can activate any other skill \"cross_activation\": true, // if false only skills can deactivate themselves // if true any skill can deactivate any other skill // NOTE: skill deactivation is not yet implemented \"cross_deactivation\": true, // you can add skill_id: priority to override the developer defined // priority of those skills, // converse priority is work in progress and not yet exposed to skills // priority is assumed to be 50 // the only current source for converse priorities is this setting \"converse_priorities\": { // \"skill_id\": 10 } } },","title":"Converse"},{"location":"converse/#converse","text":"Each Skill may define a converse() method. This method will be called anytime the Skill has been recently active and a new utterance is processed. The converse method expects a single argument which is a standard Mycroft Message object. This is the same object an intent handler receives. Converse methods must return a Boolean value. True if an utterance was handled, otherwise False.","title":"Converse"},{"location":"converse/#basic-usage","text":"Let's use a version of the Ice Cream Skill we've been building up and add a converse method to catch any brief statements of thanks that might directly follow an order. from ovos_workshop.skills import OVOSSkill from ovos_workshop.decorators import intent_handler class IceCreamSkill(OVOSSkill): def initialize(self): self.flavors = ['vanilla', 'chocolate', 'mint'] @intent_handler('request.icecream.intent') def handle_request_icecream(self): self.speak_dialog('welcome') selection = self.ask_selection(self.flavors, 'what.flavor') self.speak_dialog('coming-right-up', {'flavor': selection}) def converse(self, message): if self.voc_match(message.data['utterances'][0], 'Thankyou'): self.speak_dialog(\"you-are-welcome\") return True In this example: A User might request an ice cream which is handled by handle_request_icecream() The Skill would be added to the system Active Skill list for up to 5 minutes. Any utterance received by OVOS would trigger this Skills converse system whilst it is considered active. If the User followed up with a pleasantry such as \"Hey Mycroft, thanks\" - the converse method would match this vocab against the Thankyou.voc file in the Skill and speak the contents of the you-are-welcome.dialog file. The method would return True and the utterance would be consumed meaning the intent parsing service would never be triggered. Any utterance that did not match would be silently ignored and allowed to continue on to other converse methods and finally to the intent parsing service. WARNING skills that are not Session aware may behave weirdly with voice satellites, see the parrot skill for an example.","title":"Basic usage"},{"location":"converse/#active-skill-list","text":"A Skill is considered active if it has been called in the last 5 minutes. Skills are called in order of when they were last active. For example, if a user spoke the following commands: Hey Mycroft, set a timer for 10 minutes Hey Mycroft, what's the weather Then the utterance \"what's the weather\" would first be sent to the Timer Skill's converse() method, then to the intent service for normal handling where the Weather Skill would be called. As the Weather Skill was called it has now been added to the front of the Active Skills List. Hence, the next utterance received will be directed to: WeatherSkill.converse() TimerSkill.converse() Normal intent parsing service","title":"Active Skill List"},{"location":"converse/#making-a-skill-active","text":"There are occasions where a Skill has not been triggered by the User, but it should still be considered \"Active\". In the case of our Ice Cream Skill - we might have a function that will execute when the customers order is ready. At this point, we also want to be responsive to the customers thanks, so we call self.activate() to manually add our Skill to the front of the Active Skills List. from ovos_workshop.skills import OVOSSkill from ovos_workshop.decorators import intent_handler class IceCreamSkill(OVOSSkill): def on_order_ready(self, message): self.activate() def handle_activate(self, message: Message): \"\"\" Called when this skill is considered active by the intent service; converse method will be called with every utterance. Override this method to do any optional preparation. @param message: `{self.skill_id}.activate` Message \"\"\" LOG.info(\"Skill has been activated\")","title":"Making a Skill Active"},{"location":"converse/#deactivating-a-skill","text":"The active skill list will be pruned by ovos-core , any skills that have not been interacted with for longer than 5 minutes will be deactivated Individual Skills may react to this event, to clean up state or, in some rare cases, to reactivate themselves from ovos_workshop.skills import OVOSSkill class AlwaysActiveSkill(OVOSSkill): def handle_deactivate(self, message: Message): \"\"\" Called when this skill is no longer considered active by the intent service; converse method will not be called until skill is active again. Override this method to do any optional cleanup. @param message: `{self.skill_id}.deactivate` Message \"\"\" self.activate() A skill can also deactivate itself at any time from ovos_workshop.skills import OVOSSkill class LazySkill(OVOSSkill): def handle_intent(self, message: Message): self.speak(\"leave me alone\") self.deactivate()","title":"Deactivating a Skill"},{"location":"converse/#security","text":"A malicious or badly designed skill using the converse method can potentially hijack the whole conversation loop and render the skills service unusable Some settings are exposed to add some limitations to which skills can be activated and under what circumstances The concept of \"converse priority\" is under active development \"skills\": { // converse stage configuration \"converse\": { // the default number of seconds a skill remains active, // if the user does not interact with the skill in this timespan it // will be deactivated, default 5 minutes (same as mycroft) \"timeout\": 300, // override of \"skill_timeouts\" per skill_id // you can configure specific skills to remain active longer \"skill_timeouts\": {}, // conversational mode has 3 modes of operations: // - \"accept_all\" # default mycroft-core behavior // - \"whitelist\" # only call converse for skills in \"converse_whitelist\" // - \"blacklist\" # only call converse for skills NOT in \"converse_blacklist\" \"converse_mode\": \"accept_all\", \"converse_whitelist\": [], \"converse_blacklist\": [], // converse activation has 4 modes of operations: // - \"accept_all\" # default mycroft-core behavior, any skill can // # activate itself unconditionally // - \"priority\" # skills can only activate themselves if no skill with // # higher priority is active // - \"whitelist\" # only skills in \"converse_whitelist\" can activate themselves // - \"blacklist\" # only skills NOT in converse \"converse_blacklist\" can activate themselves // NOTE: this does not apply for regular skill activation, only to skill // initiated activation requests, eg, self.make_active() \"converse_activation\": \"accept_all\", // number of consecutive times a skill is allowed to activate itself // per minute, -1 for no limit (default), 0 to disable self-activation \"max_activations\": -1, // override of \"max_activations\" per skill_id // you can configure specific skills to activate more/less often \"skill_activations\": {}, // if false only skills can activate themselves // if true any skill can activate any other skill \"cross_activation\": true, // if false only skills can deactivate themselves // if true any skill can deactivate any other skill // NOTE: skill deactivation is not yet implemented \"cross_deactivation\": true, // you can add skill_id: priority to override the developer defined // priority of those skills, // converse priority is work in progress and not yet exposed to skills // priority is assumed to be 50 // the only current source for converse priorities is this setting \"converse_priorities\": { // \"skill_id\": 10 } } },","title":"Security"},{"location":"core/","text":"ovos-core OpenVoiceOS is an open source platform for smart speakers and other voice-centric devices. OpenVoiceOS is fully modular. Furthermore, common components have been repackaged as plugins. That means it isn't just a great assistant on its own, but also a pretty small library! ovos-core contains \"the brains\" of OpenVoiceOS, all the NLP components and skills are managed here Skills Service The skills service is responsible for loading skills and intent parsers All user queries are handled by the skills service, you can think of it as OVOS's brain All Mycroft Skills should work normally with ovos-core until version 0.1.0 , after that modernization is required! Under OpenVoiceOS skills are regular python packages, any installed skills will be loaded automatically by ovos-core Since ovos-core 0.0.8 it is also possible to launch a skill standalone via ovos-workshop , this enables individual skill containers in ovos-docker This can be also be helpful during skill development for quick testing before the skill is packaged ovos-skill-launcher {skill_id} [path/to/my/skill_id] Configuration \"skills\": { // blacklisted skills to not load // NB: This is the skill_id, usually the basename() of the directory where the skill lives, so if // the skill you want to blacklist is in /usr/share/mycroft/skills/mycroft-alarm.mycroftai/ // then you should write `[\"mycroft-alarm.mycroftai\"]` below. \"blacklisted_skills\": [], // fallback skill configuration (see below) \"fallbacks\": {...}, // converse stage configuration (see below) \"converse\": {...} }, Utterance Transformers NEW in ovos-core version 0.0.8 , originally developed for Neon when ovos-core receives a natural language query/ utterance from a user it is sent to a \"preprocessing stage\" The utterance transformers framework consists of any number of plugins ordered by priority (developer defined), the utterance and message. context are sent sequentially to all transformer plugins, and can be mutated by any of those plugins to enable a utterance transformer simply add it to mycroft.conf after installing it // To enable a utterance transformer plugin just add it's name with any relevant config // these plugins can mutate the utterance between STT and the Intent stage // they may also modify message.context with metadata // plugins only load if they are installed and enabled in this section \"utterance_transformers\": { \"ovos-utterance-normalizer\": {}, // cancel utterances mid command \"ovos-utterance-plugin-cancel\": {}, // define utterance fixes via fuzzy match ~/.local/share/mycroft/corrections.json // define unconditional replacements at word level ~/.local/share/mycroft/word_corrections.json \"ovos-utterance-corrections-plugin\": {}, // translation plugin \"ovos-utterance-translation-plugin\": { \"bidirectional\": true, \"verify_lang\": false, \"ignore_invalid\": true, \"translate_secondary_langs\": false } }, Metadata Transformers NEW in ovos-core version 0.0.8 Similar to utterance transformers, these plugins only transform the message.context // To enable a metadata transformer plugin just add it's name with any relevant config // these plugins can mutate the message.context between STT and the Intent stage \"metadata_transformers\": {}, Pipelines NEW in ovos-core version 0.0.8 , The concept of configurable pipelines has been introduced in ovos-core 0.0.8 and will be fully implemented as plugins after version 0.1.0 after the utterance has been transformed it is sent to various OVOS components by priority order until one can handle the query Pipelines include intent parsers, converse framework, common query framework and fallback skill framework // Intent Pipeline / plugins config \"intents\" : { // the pipeline is a ordered set of frameworks to send an utterance too // if one of the frameworks fails the next one is used, until an answer is found // NOTE: if padatious is not installed, it will be replaced with padacioso (much slower) // in the future these will become plugins, and new pipeline stages can be added by end users \"pipeline\": [ \"stop_high\", \"converse\", \"padatious_high\", \"adapt_high\", \"fallback_high\", \"stop_medium\", \"padatious_medium\", \"adapt_medium\", \"adapt_low\", \"common_qa\", \"fallback_medium\", \"fallback_low\" ] }, Most pipeline components will provide a high/medium/low variety with different confidence levels, but this is not required pipeline description notes stop_high stop command exact matches replaces OpenVoiceOS/skill-ovos-stop converse continuous conversation interception for skills padacioso_high high confidence intent matches from padacioso slow !!! disabled by default padatious_high high confidence intent matches from padatious adapt_high high confidence intent matches from adapt fallback_high high priority fallback skill matches stop_medium medium confidence stop matches replaces OpenVoiceOS/skill-ovos-stop padacioso_medium medium confidence intent matches from padacioso slow !!! disabled by default padatious_medium medium confidence intent matches from padatious adapt_medium medium confidence intent matches from adapt adapt_low low confidence intent matches from adapt common_query send utterance to common_query skills and select best match fallback_medium medium priority fallback skill matches stop_low low confidence stop matches disabled by default padacioso_low low confidence intent matches from padacioso slow !!! disabled by default padatious_low low confidence intent matches from padatious almost always wrong disabled by default fallback_low low priority fallback skill matches","title":"Core"},{"location":"core/#ovos-core","text":"OpenVoiceOS is an open source platform for smart speakers and other voice-centric devices. OpenVoiceOS is fully modular. Furthermore, common components have been repackaged as plugins. That means it isn't just a great assistant on its own, but also a pretty small library! ovos-core contains \"the brains\" of OpenVoiceOS, all the NLP components and skills are managed here","title":"ovos-core"},{"location":"core/#skills-service","text":"The skills service is responsible for loading skills and intent parsers All user queries are handled by the skills service, you can think of it as OVOS's brain All Mycroft Skills should work normally with ovos-core until version 0.1.0 , after that modernization is required! Under OpenVoiceOS skills are regular python packages, any installed skills will be loaded automatically by ovos-core Since ovos-core 0.0.8 it is also possible to launch a skill standalone via ovos-workshop , this enables individual skill containers in ovos-docker This can be also be helpful during skill development for quick testing before the skill is packaged ovos-skill-launcher {skill_id} [path/to/my/skill_id]","title":"Skills Service"},{"location":"core/#configuration","text":"\"skills\": { // blacklisted skills to not load // NB: This is the skill_id, usually the basename() of the directory where the skill lives, so if // the skill you want to blacklist is in /usr/share/mycroft/skills/mycroft-alarm.mycroftai/ // then you should write `[\"mycroft-alarm.mycroftai\"]` below. \"blacklisted_skills\": [], // fallback skill configuration (see below) \"fallbacks\": {...}, // converse stage configuration (see below) \"converse\": {...} },","title":"Configuration"},{"location":"core/#utterance-transformers","text":"NEW in ovos-core version 0.0.8 , originally developed for Neon when ovos-core receives a natural language query/ utterance from a user it is sent to a \"preprocessing stage\" The utterance transformers framework consists of any number of plugins ordered by priority (developer defined), the utterance and message. context are sent sequentially to all transformer plugins, and can be mutated by any of those plugins to enable a utterance transformer simply add it to mycroft.conf after installing it // To enable a utterance transformer plugin just add it's name with any relevant config // these plugins can mutate the utterance between STT and the Intent stage // they may also modify message.context with metadata // plugins only load if they are installed and enabled in this section \"utterance_transformers\": { \"ovos-utterance-normalizer\": {}, // cancel utterances mid command \"ovos-utterance-plugin-cancel\": {}, // define utterance fixes via fuzzy match ~/.local/share/mycroft/corrections.json // define unconditional replacements at word level ~/.local/share/mycroft/word_corrections.json \"ovos-utterance-corrections-plugin\": {}, // translation plugin \"ovos-utterance-translation-plugin\": { \"bidirectional\": true, \"verify_lang\": false, \"ignore_invalid\": true, \"translate_secondary_langs\": false } },","title":"Utterance Transformers"},{"location":"core/#metadata-transformers","text":"NEW in ovos-core version 0.0.8 Similar to utterance transformers, these plugins only transform the message.context // To enable a metadata transformer plugin just add it's name with any relevant config // these plugins can mutate the message.context between STT and the Intent stage \"metadata_transformers\": {},","title":"Metadata Transformers"},{"location":"core/#pipelines","text":"NEW in ovos-core version 0.0.8 , The concept of configurable pipelines has been introduced in ovos-core 0.0.8 and will be fully implemented as plugins after version 0.1.0 after the utterance has been transformed it is sent to various OVOS components by priority order until one can handle the query Pipelines include intent parsers, converse framework, common query framework and fallback skill framework // Intent Pipeline / plugins config \"intents\" : { // the pipeline is a ordered set of frameworks to send an utterance too // if one of the frameworks fails the next one is used, until an answer is found // NOTE: if padatious is not installed, it will be replaced with padacioso (much slower) // in the future these will become plugins, and new pipeline stages can be added by end users \"pipeline\": [ \"stop_high\", \"converse\", \"padatious_high\", \"adapt_high\", \"fallback_high\", \"stop_medium\", \"padatious_medium\", \"adapt_medium\", \"adapt_low\", \"common_qa\", \"fallback_medium\", \"fallback_low\" ] }, Most pipeline components will provide a high/medium/low variety with different confidence levels, but this is not required pipeline description notes stop_high stop command exact matches replaces OpenVoiceOS/skill-ovos-stop converse continuous conversation interception for skills padacioso_high high confidence intent matches from padacioso slow !!! disabled by default padatious_high high confidence intent matches from padatious adapt_high high confidence intent matches from adapt fallback_high high priority fallback skill matches stop_medium medium confidence stop matches replaces OpenVoiceOS/skill-ovos-stop padacioso_medium medium confidence intent matches from padacioso slow !!! disabled by default padatious_medium medium confidence intent matches from padatious adapt_medium medium confidence intent matches from adapt adapt_low low confidence intent matches from adapt common_query send utterance to common_query skills and select best match fallback_medium medium priority fallback skill matches stop_low low confidence stop matches disabled by default padacioso_low low confidence intent matches from padacioso slow !!! disabled by default padatious_low low confidence intent matches from padatious almost always wrong disabled by default fallback_low low priority fallback skill matches","title":"Pipelines"},{"location":"dev/","text":"Developer FAQ How do I know what is currently happening in the GUI? How do I stop an intent mid execution? How do I send files over the bus? How do I use OAuth in a skill? How do I manage remote skill settings? How do I share data between devices? How do I use Geolocation backend services? How do I use Weather backend services? How do I use WolframAlpha backend services? How do I know what is currently happening in the GUI? from ovos_utils.gui import GUITracker from ovos_workshop.skills import OVOSSkill from ovos_workshop.decorators import intent_handler class MyGUIEventTracker(GUITracker): # GUI event handlers # skill can/should subclass this def on_idle(self, namespace): print(\"IDLE\", namespace) timestamp = self.idle_ts def on_active(self, namespace): # NOTE: page has not been loaded yet # event will fire right after this one print(\"ACTIVE\", namespace) # check namespace values, they should all be set before this event values = self.gui_values[namespace] def on_new_page(self, page, namespace, index): print(\"NEW PAGE\", namespace, index, namespace) # check all loaded pages for n in self.gui_pages: # list of named tuples nspace = n.name # namespace / skill_id pages = n.pages # ordered list of page uris def on_gui_value(self, namespace, key, value): # WARNING this will pollute logs quite a lot, and you will get # duplicates, better to check values on a different event, # demonstrated in on_active print(\"VALUE\", namespace, key, value) class MySkill(OVOSSkill): def initialize(self): self.tracker = MyGUIEventTracker(bus=self.bus) @intent_handler(\"gui.status.intent\") def handle_status_intent(self, message): print(\"device has screen:\", self.tracker.can_display()) print(\"mycroft-gui installed:\", self.tracker.is_gui_installed()) print(\"gui connected:\", self.tracker.is_gui_connected()) # TODO - speak or something @intent_handler(\"list.idle.screens.intent\") def handle_idle_screens_intent(self, message): # check registered idle screens print(\"Registered idle screens:\") for name in self.tracker.idle_screens: skill_id = self.tracker.idle_screens[name] print(\" - \", name, \":\", skill_id) # TODO - speak or something How do I stop an intent mid execution? Sometimes you want to abort a running intent immediately, the stop method may not be enough in some circumstances we provide a killable_intent decorator in ovos_workshop that can be used to abort a running intent immediately a common use case is for GUI interfaces where the same action may be done by voice or clicking buttons, in this case you may need to abort a running get_response loop from ovos_workshop.skills import OVOSSkill from ovos_workshop.decorators import killable_intent, intent_handler from time import sleep class Test(OVOSSkill): \"\"\" send \"mycroft.skills.abort_question\" and confirm only get_response is aborted send \"mycroft.skills.abort_execution\" and confirm the full intent is aborted, except intent3 send \"my.own.abort.msg\" and confirm intent3 is aborted say \"stop\" and confirm all intents are aborted \"\"\" def __init__(self): super(Test, self).__init__(\"KillableSkill\") self.my_special_var = \"default\" def handle_intent_aborted(self): self.speak(\"I am dead\") # handle any cleanup the skill might need, since intent was killed # at an arbitrary place of code execution some variables etc. might # end up in unexpected states self.my_special_var = \"default\" @killable_intent(callback=handle_intent_aborted) @intent_handler(\"test.intent\") def handle_test_abort_intent(self, message): self.my_special_var = \"changed\" while True: sleep(1) self.speak(\"still here\") @intent_handler(\"test2.intent\") @killable_intent(callback=handle_intent_aborted) def handle_test_get_response_intent(self, message): self.my_special_var = \"CHANGED\" ans = self.get_response(\"question\", num_retries=99999) self.log.debug(\"get_response returned: \" + str(ans)) if ans is None: self.speak(\"question aborted\") @killable_intent(msg=\"my.own.abort.msg\", callback=handle_intent_aborted) @intent_handler(\"test3.intent\") def handle_test_msg_intent(self, message): if self.my_special_var != \"default\": self.speak(\"someone forgot to cleanup\") while True: sleep(1) self.speak(\"you can't abort me\") How do I send files over the bus? Sometimes you may want to send files or binary data over the messagebus, ovos_utils provides some tools to make this easy Sending a file from ovos_utils.messagebus import send_binary_file_message, decode_binary_message from ovos_workshop.skills import OVOSSkill class MySkill(OVOSSkill): def initialize(self): self.add_event(\"mycroft.binary.file\", self.receive_file) def receive_file(self, message): print(\"Receiving file\") path = message.data[\"path\"] # file path, extract filename if needed binary_data = decode_binary_message(message) # TODO process data somehow def send_file(self, my_file_path): send_binary_file_message(my_file_path) Sending binary data directly from ovos_utils.messagebus import send_binary_data_message, decode_binary_message from ovos_workshop.skills import OVOSSkill class MySkill(OVOSSkill): def initialize(self): self.add_event(\"mycroft.binary.data\", self.receive_binary) def send_data(self, binary_data): send_binary_data_message(binary_data) def receive_binary(self, message): print(\"Receiving binary data\") binary_data = decode_binary_message(message) # TODO process data somehow How do I manage remote skill settings? To interact with skill settings via DeviceApi from ovos_backend_client.settings import RemoteSkillSettings # in ovos-core skill_id is deterministic and safe s = RemoteSkillSettings(\"skill.author\") s.download() s.settings[\"existing_value\"] = True s.settings[\"new_value\"] = \"will NOT show up in UI\" s.upload() # auto generate new settings meta for all new values before uploading s.settings[\"new_value\"] = \"will show up in UI\" s.generate_meta() # now \"new_value\" is in meta s.upload() How do I share data between devices? by hijacking skill settings we allow storing arbitrary data via DeviceApi and use it across devices and skills from ovos_backend_client.cloud import SeleneCloud cloud = SeleneCloud() cloud.add_entry(\"test\", {\"secret\": \"NOT ENCRYPTED MAN\"}) data = cloud.get_entry(\"test\") an encrypted version is also supported if you don't trust the backend! from ovos_backend_client.cloud import SecretSeleneCloud k = \"D8fmXEP5VqzVw2HE\" # you need this to read back the data cloud = SecretSeleneCloud(k) cloud.add_entry(\"test\", {\"secret\": \"secret data, selene cant read this\"}) data = cloud.get_entry(\"test\") How do I use Geolocation backend services? from ovos_backend_client.api import GeolocationApi geo = GeolocationApi() data = geo.get_geolocation(\"Lisbon Portugal\")","title":"F.A.Q."},{"location":"dev/#developer-faq","text":"How do I know what is currently happening in the GUI? How do I stop an intent mid execution? How do I send files over the bus? How do I use OAuth in a skill? How do I manage remote skill settings? How do I share data between devices? How do I use Geolocation backend services? How do I use Weather backend services? How do I use WolframAlpha backend services?","title":"Developer FAQ"},{"location":"dev/#how-do-i-know-what-is-currently-happening-in-the-gui","text":"from ovos_utils.gui import GUITracker from ovos_workshop.skills import OVOSSkill from ovos_workshop.decorators import intent_handler class MyGUIEventTracker(GUITracker): # GUI event handlers # skill can/should subclass this def on_idle(self, namespace): print(\"IDLE\", namespace) timestamp = self.idle_ts def on_active(self, namespace): # NOTE: page has not been loaded yet # event will fire right after this one print(\"ACTIVE\", namespace) # check namespace values, they should all be set before this event values = self.gui_values[namespace] def on_new_page(self, page, namespace, index): print(\"NEW PAGE\", namespace, index, namespace) # check all loaded pages for n in self.gui_pages: # list of named tuples nspace = n.name # namespace / skill_id pages = n.pages # ordered list of page uris def on_gui_value(self, namespace, key, value): # WARNING this will pollute logs quite a lot, and you will get # duplicates, better to check values on a different event, # demonstrated in on_active print(\"VALUE\", namespace, key, value) class MySkill(OVOSSkill): def initialize(self): self.tracker = MyGUIEventTracker(bus=self.bus) @intent_handler(\"gui.status.intent\") def handle_status_intent(self, message): print(\"device has screen:\", self.tracker.can_display()) print(\"mycroft-gui installed:\", self.tracker.is_gui_installed()) print(\"gui connected:\", self.tracker.is_gui_connected()) # TODO - speak or something @intent_handler(\"list.idle.screens.intent\") def handle_idle_screens_intent(self, message): # check registered idle screens print(\"Registered idle screens:\") for name in self.tracker.idle_screens: skill_id = self.tracker.idle_screens[name] print(\" - \", name, \":\", skill_id) # TODO - speak or something","title":"How do I know what is currently happening in the GUI?"},{"location":"dev/#how-do-i-stop-an-intent-mid-execution","text":"Sometimes you want to abort a running intent immediately, the stop method may not be enough in some circumstances we provide a killable_intent decorator in ovos_workshop that can be used to abort a running intent immediately a common use case is for GUI interfaces where the same action may be done by voice or clicking buttons, in this case you may need to abort a running get_response loop from ovos_workshop.skills import OVOSSkill from ovos_workshop.decorators import killable_intent, intent_handler from time import sleep class Test(OVOSSkill): \"\"\" send \"mycroft.skills.abort_question\" and confirm only get_response is aborted send \"mycroft.skills.abort_execution\" and confirm the full intent is aborted, except intent3 send \"my.own.abort.msg\" and confirm intent3 is aborted say \"stop\" and confirm all intents are aborted \"\"\" def __init__(self): super(Test, self).__init__(\"KillableSkill\") self.my_special_var = \"default\" def handle_intent_aborted(self): self.speak(\"I am dead\") # handle any cleanup the skill might need, since intent was killed # at an arbitrary place of code execution some variables etc. might # end up in unexpected states self.my_special_var = \"default\" @killable_intent(callback=handle_intent_aborted) @intent_handler(\"test.intent\") def handle_test_abort_intent(self, message): self.my_special_var = \"changed\" while True: sleep(1) self.speak(\"still here\") @intent_handler(\"test2.intent\") @killable_intent(callback=handle_intent_aborted) def handle_test_get_response_intent(self, message): self.my_special_var = \"CHANGED\" ans = self.get_response(\"question\", num_retries=99999) self.log.debug(\"get_response returned: \" + str(ans)) if ans is None: self.speak(\"question aborted\") @killable_intent(msg=\"my.own.abort.msg\", callback=handle_intent_aborted) @intent_handler(\"test3.intent\") def handle_test_msg_intent(self, message): if self.my_special_var != \"default\": self.speak(\"someone forgot to cleanup\") while True: sleep(1) self.speak(\"you can't abort me\")","title":"How do I stop an intent mid execution?"},{"location":"dev/#how-do-i-send-files-over-the-bus","text":"Sometimes you may want to send files or binary data over the messagebus, ovos_utils provides some tools to make this easy Sending a file from ovos_utils.messagebus import send_binary_file_message, decode_binary_message from ovos_workshop.skills import OVOSSkill class MySkill(OVOSSkill): def initialize(self): self.add_event(\"mycroft.binary.file\", self.receive_file) def receive_file(self, message): print(\"Receiving file\") path = message.data[\"path\"] # file path, extract filename if needed binary_data = decode_binary_message(message) # TODO process data somehow def send_file(self, my_file_path): send_binary_file_message(my_file_path) Sending binary data directly from ovos_utils.messagebus import send_binary_data_message, decode_binary_message from ovos_workshop.skills import OVOSSkill class MySkill(OVOSSkill): def initialize(self): self.add_event(\"mycroft.binary.data\", self.receive_binary) def send_data(self, binary_data): send_binary_data_message(binary_data) def receive_binary(self, message): print(\"Receiving binary data\") binary_data = decode_binary_message(message) # TODO process data somehow","title":"How do I send files over the bus?"},{"location":"dev/#how-do-i-manage-remote-skill-settings","text":"To interact with skill settings via DeviceApi from ovos_backend_client.settings import RemoteSkillSettings # in ovos-core skill_id is deterministic and safe s = RemoteSkillSettings(\"skill.author\") s.download() s.settings[\"existing_value\"] = True s.settings[\"new_value\"] = \"will NOT show up in UI\" s.upload() # auto generate new settings meta for all new values before uploading s.settings[\"new_value\"] = \"will show up in UI\" s.generate_meta() # now \"new_value\" is in meta s.upload()","title":"How do I manage remote skill settings?"},{"location":"dev/#how-do-i-share-data-between-devices","text":"by hijacking skill settings we allow storing arbitrary data via DeviceApi and use it across devices and skills from ovos_backend_client.cloud import SeleneCloud cloud = SeleneCloud() cloud.add_entry(\"test\", {\"secret\": \"NOT ENCRYPTED MAN\"}) data = cloud.get_entry(\"test\") an encrypted version is also supported if you don't trust the backend! from ovos_backend_client.cloud import SecretSeleneCloud k = \"D8fmXEP5VqzVw2HE\" # you need this to read back the data cloud = SecretSeleneCloud(k) cloud.add_entry(\"test\", {\"secret\": \"secret data, selene cant read this\"}) data = cloud.get_entry(\"test\")","title":"How do I share data between devices?"},{"location":"dev/#how-do-i-use-geolocation-backend-services","text":"from ovos_backend_client.api import GeolocationApi geo = GeolocationApi() data = geo.get_geolocation(\"Lisbon Portugal\")","title":"How do I use Geolocation backend services?"},{"location":"eggscript/","text":"Eggscript Eggscript is a markup language that can be \"compiled\" into a valid OVOS Skill EXPERIMENTAL This is an experimental feature It is intended as an easy way for user to create simple skills, while offering an easy transition to regular skills It also helps getting a lot of the boilerplate done for you when getting started You can find a developer preview of eggscript in github Crash Course Example files written in eggscript hello.eggscript // this is a comment // all comments and blank lines are ignored // special interperter variables can be set with @var syntax // - @name -> skill name // - @author -> skill author // - @email -> author contact // - @license -> skill license // - @interpreter -> supported interperter, eg, cli // - @compiler -> supported compiler, eg, mycroft skill @author jarbasai @email jarbasai@mailfence.com @license MIT @name hello world @url https://github.com/author/repo @version 0.1.0 // this script can be used standalone in the cli @interpreter cli // a standalone python file can be generated @compiler cli // a mycroft skill can be generated @compiler mycroft // intent definition # hello world + hello world - hello world // you can define python code, executed after TTS ``` hello = \"world\" if hello == \"world\": print(\"python code!\") ``` dialogs.eggscript // this is a comment // all comments and blank lines are ignored // text after # is the intent name # hello world // text after + is the user utterance + hello world // text after - is mycroft's response - hello world # weather in location // you can capture variables and use them using {var} syntax + how is the weather in {location} - how am i supposed to know the weather in {location} # weather // this will create a intent file with the 3 + utterances + what is the weather like + how is the weather + how does it look outside // this will create a dialog file with the 2 - utterances - i do not know how to check the weather - stick your head ouf of the window and check for yourself # count to 10 + count to 10 // if ident level matches its an alternate dialog - i will only count to 5 - i only know how to count to 5 // use tab for identation // each ident level defines a new utterance to be spoken - 1 - 2 - 3 - 4 - 5 layers.eggscript // this is a comment // all comments and blank lines are ignored // this sample scripts show intent layers usage // the number of # in intent definition determines an intent layer # tell me about + tell me about {thing} - {thing} exists // N times + will enable layer N // to enable layer 2 ++ // use N times # for layer N // this intent is in layer 2, enabled by previous intent ## tell me more + tell me more + continue - i do not know more // N times - will disable layer N // to disable layer 2 -- Interpreters Can run a subset of eggscript directly, enough to test simple skills in the terminal from eggscript import CliInterpreter from os.path import dirname c = CliInterpreter() c.load_eggscript_file(f\"{dirname(__file__)}/dialogs.eggscript\") c.run() Compilers from eggscript import OVOSSkillCompiler from os.path import dirname c = OVOSSkillCompiler() c.load_eggscript_file(f\"{dirname(__file__)}/layers.eggscript\") c.export(\"myskill\") You can now continue extending your exported skill to add more advanced functionality","title":"Eggscript"},{"location":"eggscript/#eggscript","text":"Eggscript is a markup language that can be \"compiled\" into a valid OVOS Skill EXPERIMENTAL This is an experimental feature It is intended as an easy way for user to create simple skills, while offering an easy transition to regular skills It also helps getting a lot of the boilerplate done for you when getting started You can find a developer preview of eggscript in github","title":"Eggscript"},{"location":"eggscript/#crash-course","text":"Example files written in eggscript hello.eggscript // this is a comment // all comments and blank lines are ignored // special interperter variables can be set with @var syntax // - @name -> skill name // - @author -> skill author // - @email -> author contact // - @license -> skill license // - @interpreter -> supported interperter, eg, cli // - @compiler -> supported compiler, eg, mycroft skill @author jarbasai @email jarbasai@mailfence.com @license MIT @name hello world @url https://github.com/author/repo @version 0.1.0 // this script can be used standalone in the cli @interpreter cli // a standalone python file can be generated @compiler cli // a mycroft skill can be generated @compiler mycroft // intent definition # hello world + hello world - hello world // you can define python code, executed after TTS ``` hello = \"world\" if hello == \"world\": print(\"python code!\") ``` dialogs.eggscript // this is a comment // all comments and blank lines are ignored // text after # is the intent name # hello world // text after + is the user utterance + hello world // text after - is mycroft's response - hello world # weather in location // you can capture variables and use them using {var} syntax + how is the weather in {location} - how am i supposed to know the weather in {location} # weather // this will create a intent file with the 3 + utterances + what is the weather like + how is the weather + how does it look outside // this will create a dialog file with the 2 - utterances - i do not know how to check the weather - stick your head ouf of the window and check for yourself # count to 10 + count to 10 // if ident level matches its an alternate dialog - i will only count to 5 - i only know how to count to 5 // use tab for identation // each ident level defines a new utterance to be spoken - 1 - 2 - 3 - 4 - 5 layers.eggscript // this is a comment // all comments and blank lines are ignored // this sample scripts show intent layers usage // the number of # in intent definition determines an intent layer # tell me about + tell me about {thing} - {thing} exists // N times + will enable layer N // to enable layer 2 ++ // use N times # for layer N // this intent is in layer 2, enabled by previous intent ## tell me more + tell me more + continue - i do not know more // N times - will disable layer N // to disable layer 2 --","title":"Crash Course"},{"location":"eggscript/#interpreters","text":"Can run a subset of eggscript directly, enough to test simple skills in the terminal from eggscript import CliInterpreter from os.path import dirname c = CliInterpreter() c.load_eggscript_file(f\"{dirname(__file__)}/dialogs.eggscript\") c.run()","title":"Interpreters"},{"location":"eggscript/#compilers","text":"from eggscript import OVOSSkillCompiler from os.path import dirname c = OVOSSkillCompiler() c.load_eggscript_file(f\"{dirname(__file__)}/layers.eggscript\") c.export(\"myskill\") You can now continue extending your exported skill to add more advanced functionality","title":"Compilers"},{"location":"fallbacks/","text":"Fallback Skill Order of precedence The Fallback Skills all have a priority and will be checked in order from low priority value to high priority value. If a Fallback Skill can handle the Utterance it will create a response and return True . After this no other Fallback Skills are tried. This means the priority for Fallbacks that can handle a broad range of queries should be high (80-100) and Fallbacks that only responds to a very specific range of queries should be higher (20-80). The more specific, the lower the priority value. Fallback Handlers Import the FallbackSkill base class, create a derived class and register the handler with the fallback system Implement the fallback handler (the method that will be called to potentially handle the Utterance ). The method implements logic to determine if the Utterance can be handled and shall output speech if it can handle the query. It shall return Boolean True if the Utterance was handled and Boolean False if not. from ovos_workshop.skills.fallback import FallbackSkill class MeaningFallback(FallbackSkill): \"\"\" A Fallback skill to answer the question about the meaning of life, the universe and everything. \"\"\" def initialize(self): \"\"\" Registers the fallback handler \"\"\" self.register_fallback(self.handle_fallback, 10) # Any other initialize code you like can be placed here def handle_fallback(self, message): \"\"\" Answers question about the meaning of life, the universe and everything. \"\"\" utterance = message.data.get(\"utterance\") if 'what' in utterance and 'meaning' in utterance and ('life' in utterance or 'universe' in utterance or 'everything' in utterance): self.speak('42') return True else: return False NOTE : a FallbackSkill can register any number of fallback handlers The above example can be found here . Decorators NEW - ovos-core version 0.0.8 Alternatively, you can use decorators from ovos_workshop.decorators.fallback_handler import fallback_handler class MeaningFallback(FallbackSkill): \"\"\" A Fallback skill to answer the question about the meaning of life, the universe and everything. \"\"\" @fallback_handler(priority=10) def handle_fallback(self, message): \"\"\" Answers question about the meaning of life, the universe and everything. \"\"\" utterance = message.data.get(\"utterance\") if 'what' in utterance and 'meaning' in utterance and ('life' in utterance or 'universe' in utterance or 'everything' in utterance): self.speak('42') return True else: return False Check utterances NEW - ovos-core version 0.0.8 Fallback skills should report if they are able to answer a question, without actually executing any action. Besides providing performance improvements this allows other OVOS components to check how a utterance will be handled without side effects def can_answer(self, utterances: List[str], lang: str) -> bool: \"\"\" Check if the skill can answer the particular question. Override this method to validate whether a query can possibly be handled. By default, assumes a skill can answer if it has any registered handlers @param utterances: list of possible transcriptions to parse @param lang: BCP-47 language code associated with utterances @return: True if skill can handle the query \"\"\" return len(self._fallback_handlers) > 0 Security Just like with converse a badly designed or malicious skill can hijack the fallback skill loop, while this is not as serious as with converse some protections are also provided You can configure what skills are allowed to use the fallback mechanism, you can also modify the fallback priority to ensure skills behave well together. Since priority is defined by developers sometimes the default value is not appropriate and does not fit well with the installed skills collection \"skills\": { // fallback skill configuration \"fallbacks\": { // you can add skill_id: priority to override the developer defined // priority of those skills, this allows customization // of unknown intent handling for default_skills + user preferences \"fallback_priorities\": { // \"skill_id\": 10 }, // fallback skill handling has 3 modes of operations: // - \"accept_all\" # default mycroft-core behavior // - \"whitelist\" # only call fallback for skills in \"fallback_whitelist\" // - \"blacklist\" # only call fallback for skills NOT in \"fallback_blacklist\" \"fallback_mode\": \"accept_all\", \"fallback_whitelist\": [], \"fallback_blacklist\": [] } },","title":"Fallback Skills"},{"location":"fallbacks/#fallback-skill","text":"","title":"Fallback Skill"},{"location":"fallbacks/#order-of-precedence","text":"The Fallback Skills all have a priority and will be checked in order from low priority value to high priority value. If a Fallback Skill can handle the Utterance it will create a response and return True . After this no other Fallback Skills are tried. This means the priority for Fallbacks that can handle a broad range of queries should be high (80-100) and Fallbacks that only responds to a very specific range of queries should be higher (20-80). The more specific, the lower the priority value.","title":"Order of precedence"},{"location":"fallbacks/#fallback-handlers","text":"Import the FallbackSkill base class, create a derived class and register the handler with the fallback system Implement the fallback handler (the method that will be called to potentially handle the Utterance ). The method implements logic to determine if the Utterance can be handled and shall output speech if it can handle the query. It shall return Boolean True if the Utterance was handled and Boolean False if not. from ovos_workshop.skills.fallback import FallbackSkill class MeaningFallback(FallbackSkill): \"\"\" A Fallback skill to answer the question about the meaning of life, the universe and everything. \"\"\" def initialize(self): \"\"\" Registers the fallback handler \"\"\" self.register_fallback(self.handle_fallback, 10) # Any other initialize code you like can be placed here def handle_fallback(self, message): \"\"\" Answers question about the meaning of life, the universe and everything. \"\"\" utterance = message.data.get(\"utterance\") if 'what' in utterance and 'meaning' in utterance and ('life' in utterance or 'universe' in utterance or 'everything' in utterance): self.speak('42') return True else: return False NOTE : a FallbackSkill can register any number of fallback handlers The above example can be found here .","title":"Fallback Handlers"},{"location":"fallbacks/#decorators","text":"NEW - ovos-core version 0.0.8 Alternatively, you can use decorators from ovos_workshop.decorators.fallback_handler import fallback_handler class MeaningFallback(FallbackSkill): \"\"\" A Fallback skill to answer the question about the meaning of life, the universe and everything. \"\"\" @fallback_handler(priority=10) def handle_fallback(self, message): \"\"\" Answers question about the meaning of life, the universe and everything. \"\"\" utterance = message.data.get(\"utterance\") if 'what' in utterance and 'meaning' in utterance and ('life' in utterance or 'universe' in utterance or 'everything' in utterance): self.speak('42') return True else: return False","title":"Decorators"},{"location":"fallbacks/#check-utterances","text":"NEW - ovos-core version 0.0.8 Fallback skills should report if they are able to answer a question, without actually executing any action. Besides providing performance improvements this allows other OVOS components to check how a utterance will be handled without side effects def can_answer(self, utterances: List[str], lang: str) -> bool: \"\"\" Check if the skill can answer the particular question. Override this method to validate whether a query can possibly be handled. By default, assumes a skill can answer if it has any registered handlers @param utterances: list of possible transcriptions to parse @param lang: BCP-47 language code associated with utterances @return: True if skill can handle the query \"\"\" return len(self._fallback_handlers) > 0","title":"Check utterances"},{"location":"fallbacks/#security","text":"Just like with converse a badly designed or malicious skill can hijack the fallback skill loop, while this is not as serious as with converse some protections are also provided You can configure what skills are allowed to use the fallback mechanism, you can also modify the fallback priority to ensure skills behave well together. Since priority is defined by developers sometimes the default value is not appropriate and does not fit well with the installed skills collection \"skills\": { // fallback skill configuration \"fallbacks\": { // you can add skill_id: priority to override the developer defined // priority of those skills, this allows customization // of unknown intent handling for default_skills + user preferences \"fallback_priorities\": { // \"skill_id\": 10 }, // fallback skill handling has 3 modes of operations: // - \"accept_all\" # default mycroft-core behavior // - \"whitelist\" # only call fallback for skills in \"fallback_whitelist\" // - \"blacklist\" # only call fallback for skills NOT in \"fallback_blacklist\" \"fallback_mode\": \"accept_all\", \"fallback_whitelist\": [], \"fallback_blacklist\": [] } },","title":"Security"},{"location":"g2p_plugins/","text":"Grapheme to Phoneme Plugins Grapheme to Phoneme is the process of converting text into a set of \"sound units\" called phonemes In ovos-audio these plugins are used to auto generate mouth movements / visemes in the TTS stage. They can also be used to help configuring wake words or to facilitate training of TTS systems These plugins can provide phonemes either in ARPA or IPA alphabets, an automatic conversion will happen behind the scenes when needed Visemes Visemes are representations of the shape of a human mouth when speaking. Mouth movements are generated via a mapping of ARPA to VISEMES, TTS plugins may provide this natively, or a G2P plugin may be used to estimate it directly from text Visemes are predefined mouth positions, timing per phonemes is crucial for a natural mouth movement The Mycroft Mark 1 uses this to make his \"lips\" match his speech. OpenVoiceOS uses six basic visemes. viseme mouth position 0 wide open 1 pursed 2 open 3 narrow lips 4 closed lips 5 parted lips 6 barely open lips Mapping based on Jeffers phoneme to viseme map, seen in table 1 , partially based on the \"12 mouth shapes visuals seen here List of G2P plugins Plugin Type Duration ovos-g2p-plugin-mimic ARPA \u2714\ufe0f ovos-g2p-plugin-heuristic-arpa ARPA \u274c ovos-g2p-plugin-espeak IPA \u274c neon-g2p-cmudict-plugin ARPA \u274c neon-g2p-phoneme-guesser-plugin ARPA \u274c neon-g2p-gruut-plugin IPA \u274c Standalone Usage All G2P plugins can be used as follows utterance = \"hello world\" word = \"hello\" lang = \"en-us\" plug = G2pPlugin() # convert a word into a list of phonemes phones = plug.get_ipa(word, lang) assert phones == ['h', '\u028c', 'l', 'o\u028a'] phones = plug.get_arpa(word, lang) assert phones == ['HH', 'AH', 'L', 'OW'] # convert a utterance into a list of phonemes phones = plug.utterance2arpa(utterance, lang) assert phones == ['HH', 'AH', 'L', 'OW', '.', 'W', 'ER', 'L', 'D'] phones = plug.utterance2ipa(utterance, lang) assert phones == ['h', '\u028c', 'l', 'o\u028a', '.', 'w', '\u025d', 'l', 'd'] # convert a utterance into a list of viseme, duration pairs visemes = plug.utterance2visemes(utterance, lang) assert visemes == [('0', 0.0775), ('0', 0.155), ('3', 0.2325), ('2', 0.31), ('2', 0.434), ('2', 0.558), ('3', 0.682), ('3', 0.806)] Plugin Template from ovos_plugin_manager.templates.g2p import Grapheme2PhonemePlugin from ovos_utils.lang.visimes import VISIMES # base plugin class class MyARPAG2PPlugin(Grapheme2PhonemePlugin): def __init__(self, config=None): self.config = config or {} def get_arpa(self, word, lang, ignore_oov=False): phones = [] # TODO implement return phones def get_durations(self, utterance, lang=\"en\", default_dur=0.4): words = utterance.split() phones = [self.get_arpa(w, lang) for w in utterance.split()] dur = default_dur # TODO this is plugin specific return [(pho, dur) for pho in phones] def utterance2visemes(self, utterance, lang=\"en\", default_dur=0.4): phonemes = self.get_durations(utterance, lang, default_dur) return [(VISIMES.get(pho[0].lower(), '4'), float(pho[1])) for pho in phonemes] If your plugin uses IPA instead of ARPA simply replace get_arpa with get_ipa from ovos_plugin_manager.templates.g2p import Grapheme2PhonemePlugin from ovos_utils.lang.visimes import VISIMES # base plugin class class MyIPAG2PPlugin(Grapheme2PhonemePlugin): def __init__(self, config=None): self.config = config or {} def get_ipa(self, word, lang, ignore_oov=False): phones = [] # TODO implement return phones def get_durations(self, utterance, lang=\"en\", default_dur=0.4): # auto converted to arpa if ipa is implemented phones = [self.get_arpa(w, lang) for w in utterance.split()] dur = default_dur # TODO this is plugin specific return [(pho, dur) for pho in phones] def utterance2visemes(self, utterance, lang=\"en\", default_dur=0.4): phonemes = self.get_durations(utterance, lang, default_dur) return [(VISIMES.get(pho[0].lower(), '4'), float(pho[1])) for pho in phonemes]","title":"Grapheme to Phoneme"},{"location":"g2p_plugins/#grapheme-to-phoneme-plugins","text":"Grapheme to Phoneme is the process of converting text into a set of \"sound units\" called phonemes In ovos-audio these plugins are used to auto generate mouth movements / visemes in the TTS stage. They can also be used to help configuring wake words or to facilitate training of TTS systems These plugins can provide phonemes either in ARPA or IPA alphabets, an automatic conversion will happen behind the scenes when needed","title":"Grapheme to Phoneme Plugins"},{"location":"g2p_plugins/#visemes","text":"Visemes are representations of the shape of a human mouth when speaking. Mouth movements are generated via a mapping of ARPA to VISEMES, TTS plugins may provide this natively, or a G2P plugin may be used to estimate it directly from text Visemes are predefined mouth positions, timing per phonemes is crucial for a natural mouth movement The Mycroft Mark 1 uses this to make his \"lips\" match his speech. OpenVoiceOS uses six basic visemes. viseme mouth position 0 wide open 1 pursed 2 open 3 narrow lips 4 closed lips 5 parted lips 6 barely open lips Mapping based on Jeffers phoneme to viseme map, seen in table 1 , partially based on the \"12 mouth shapes visuals seen here","title":"Visemes"},{"location":"g2p_plugins/#list-of-g2p-plugins","text":"Plugin Type Duration ovos-g2p-plugin-mimic ARPA \u2714\ufe0f ovos-g2p-plugin-heuristic-arpa ARPA \u274c ovos-g2p-plugin-espeak IPA \u274c neon-g2p-cmudict-plugin ARPA \u274c neon-g2p-phoneme-guesser-plugin ARPA \u274c neon-g2p-gruut-plugin IPA \u274c","title":"List of G2P plugins"},{"location":"g2p_plugins/#standalone-usage","text":"All G2P plugins can be used as follows utterance = \"hello world\" word = \"hello\" lang = \"en-us\" plug = G2pPlugin() # convert a word into a list of phonemes phones = plug.get_ipa(word, lang) assert phones == ['h', '\u028c', 'l', 'o\u028a'] phones = plug.get_arpa(word, lang) assert phones == ['HH', 'AH', 'L', 'OW'] # convert a utterance into a list of phonemes phones = plug.utterance2arpa(utterance, lang) assert phones == ['HH', 'AH', 'L', 'OW', '.', 'W', 'ER', 'L', 'D'] phones = plug.utterance2ipa(utterance, lang) assert phones == ['h', '\u028c', 'l', 'o\u028a', '.', 'w', '\u025d', 'l', 'd'] # convert a utterance into a list of viseme, duration pairs visemes = plug.utterance2visemes(utterance, lang) assert visemes == [('0', 0.0775), ('0', 0.155), ('3', 0.2325), ('2', 0.31), ('2', 0.434), ('2', 0.558), ('3', 0.682), ('3', 0.806)]","title":"Standalone Usage"},{"location":"g2p_plugins/#plugin-template","text":"from ovos_plugin_manager.templates.g2p import Grapheme2PhonemePlugin from ovos_utils.lang.visimes import VISIMES # base plugin class class MyARPAG2PPlugin(Grapheme2PhonemePlugin): def __init__(self, config=None): self.config = config or {} def get_arpa(self, word, lang, ignore_oov=False): phones = [] # TODO implement return phones def get_durations(self, utterance, lang=\"en\", default_dur=0.4): words = utterance.split() phones = [self.get_arpa(w, lang) for w in utterance.split()] dur = default_dur # TODO this is plugin specific return [(pho, dur) for pho in phones] def utterance2visemes(self, utterance, lang=\"en\", default_dur=0.4): phonemes = self.get_durations(utterance, lang, default_dur) return [(VISIMES.get(pho[0].lower(), '4'), float(pho[1])) for pho in phonemes] If your plugin uses IPA instead of ARPA simply replace get_arpa with get_ipa from ovos_plugin_manager.templates.g2p import Grapheme2PhonemePlugin from ovos_utils.lang.visimes import VISIMES # base plugin class class MyIPAG2PPlugin(Grapheme2PhonemePlugin): def __init__(self, config=None): self.config = config or {} def get_ipa(self, word, lang, ignore_oov=False): phones = [] # TODO implement return phones def get_durations(self, utterance, lang=\"en\", default_dur=0.4): # auto converted to arpa if ipa is implemented phones = [self.get_arpa(w, lang) for w in utterance.split()] dur = default_dur # TODO this is plugin specific return [(pho, dur) for pho in phones] def utterance2visemes(self, utterance, lang=\"en\", default_dur=0.4): phonemes = self.get_durations(utterance, lang, default_dur) return [(VISIMES.get(pho[0].lower(), '4'), float(pho[1])) for pho in phonemes]","title":"Plugin Template"},{"location":"gui_protocol/","text":"Protocol The gui service in ovos-core will expose a websocket to the GUI clients following the protocol outlined in this page The transport protocol works between gui service and the gui clients, OpenVoiceOS does not directly use the protocol but instead communicates with ovos-gui via the standard messagebus The QT library which implements the protocol lives in the mycroft-gui-qt5 repository. Specification This protocol defines how ovos-gui communicates with connected clients CONNECTION - mycroft.gui.connected NAMESPACES Active Skills - mycroft.system.active_skills PAGES - mycroft.gui.list.xxx Insert new page at position Move pages within the list Remove pages from the list EVENTS - mycroft.events.triggered SPECIAL EVENT: page_gained_focus SKILL DATA - mycroft.session.xxx Sets a new key/value in the sessionData dictionary Deletes a key/value pair from the sessionData dictionary Lists Inserts new items at position Updates item values starting at the given position, as many items as there are in the array Move items within the list Remove items from the list CONNECTION - mycroft.gui.connected on connection gui clients announce themselves This is an extension by OVOS to the original mycroft protocol { \"type\": \"mycroft.gui.connected\", \"gui_id\": \"unique_identifier_provided_by_client\" } NAMESPACES ovos-gui maintains a list of namespaces with GUI data, namespaces usually correspond to a skill_id Every message in the gui protocol specifies a namespace it belongs to gui clients usualy display all namespaces, but can be requested to display a single one, eg, have a dedicated window to show a skill as a traditional desktop app Active Skills - mycroft.system.active_skills a reserved namespace is \"mycroft.system.active_skills\", the data contained in this namespace defines the namespace display priority Recent skills are ordered from the last used to the oldest, so the first item of the list will always be the the one showing any GUI page, if available. see the section about lists if you need to modify active skills PAGES - mycroft.gui.list.xxx Each active skill is associated with a list of uris to the QML files of all gui items that are supposed to be visible. Non QT GUIS get sent other file extensions such as .jsx or .html using the same message format Insert new page at position { \"type\": \"mycroft.gui.list.insert\", \"namespace\": \"mycroft.weather\" \"position\": 2 \"values\": [{\"url\": \"file://..../currentWeather.qml\"}, ...] //values must always be in array form } Move pages within the list { \"type\": \"mycroft.gui.list.move\", \"namespace\": \"mycroft.weather\" \"from\": 2 \"to\": 5 \"items_number\": 2 //optional in case we want to move a big chunk of list at once } Remove pages from the list { \"type\": \"mycroft.gui.list.remove\", \"namespace\": \"mycroft.weather\" \"position\": 2 \"items_number\": 5 //optional in case we want to get rid a big chunk of list at once } EVENTS - mycroft.events.triggered Events can either be emitted by a gui client (eg, some element clicked) or by the skill (eg, in response to a voice command) { \"type\": \"mycroft.events.triggered\" \"namespace\": \"my_skill_id\" \"event_name\": \"my.gui.event\", \"parameters\": {\"item\": 3} } SPECIAL EVENT: page_gained_focus This event is used when the ovos-gui wants a page of a particular skill to gain user attention focus and become the current active view and \"focus of attention\" of the user. when a GUI client receives it, it should render the requested GUI page GUI clients can also emit this event, if a new page was rendered (eg, in response to a user swipping left) NOTE: for responsiveness it is recommened this message is only emitted after the rendering has actually been done, skills may be waiting for this event to initiate some actons { \"type\": \"mycroft.events.triggered\", \"namespace\": \"mycroft.weather\", \"event_name\": \"page_gained_focus\", \"data\": {\"number\": 0} } The parameter \"number\" is the position (starting from zero) of the page SKILL DATA - mycroft.session.xxx At the center of data sharing there is a key/value dictionary that is kept synchronized between ovos-gui and the GUI client. Values can either be simple strings, numbers and booleans or be more complicated data types this event can be sent from gui clients (eg, in response to a dropdown selection) or from skills (eg, change weather data) NOTE: Once a new gui client connects to ovos-gui, all existing session data is sent to the client, after that the client gets live updates via these events Sets a new key/value in the sessionData dictionary Either sets a new key/value pair or replace an existing old value. { \"type\": \"mycroft.session.set\", \"namespace\": \"weather.mycroft\" \"data\": { \"temperature\": \"28\", \"icon\": \"cloudy\", \"forecast\": [{...},...] //if it's a list see below for more message types } } Deletes a key/value pair from the sessionData dictionary { \"type\": \"mycroft.session.delete\", \"namespace\": \"weather.mycroft\" \"property\": \"temperature\" } Lists Inserts new items at position { \"type\": \"mycroft.session.list.insert\", \"namespace\": \"weather.mycroft\" \"property\": \"forecast\" //the key of the main data map this list in contained into \"position\": 2 \"values\": [{\"date\": \"tomorrow\", \"temperature\" : 13, ...}, ...] //values must always be in array form } Updates item values starting at the given position, as many items as there are in the array { \"type\": \"mycroft.session.list.update\", \"namespace\": \"weather.mycroft\" \"property\": \"forecast\" \"position\": 2 \"values\": [{\"date\": \"tomorrow\", \"temperature\" : 13, ...}, ...] //values must always be in array form } Move items within the list { \"type\": \"mycroft.session.list.move\", \"namespace\": \"weather.mycroft\" \"property\": \"forecast\" \"from\": 2 \"to\": 5 \"items_number\": 2 //optional in case we want to move a big chunk of list at once } Remove items from the list { \"type\": \"mycroft.session.list.remove\", \"namespace\": \"weather.mycroft\" \"property\": \"forecast\" \"position\": 2 \"items_number\": 5 //optional in case we want to get rid a big chunk of list at once }","title":"Protocol"},{"location":"gui_protocol/#protocol","text":"The gui service in ovos-core will expose a websocket to the GUI clients following the protocol outlined in this page The transport protocol works between gui service and the gui clients, OpenVoiceOS does not directly use the protocol but instead communicates with ovos-gui via the standard messagebus The QT library which implements the protocol lives in the mycroft-gui-qt5 repository.","title":"Protocol"},{"location":"gui_protocol/#specification","text":"This protocol defines how ovos-gui communicates with connected clients CONNECTION - mycroft.gui.connected NAMESPACES Active Skills - mycroft.system.active_skills PAGES - mycroft.gui.list.xxx Insert new page at position Move pages within the list Remove pages from the list EVENTS - mycroft.events.triggered SPECIAL EVENT: page_gained_focus SKILL DATA - mycroft.session.xxx Sets a new key/value in the sessionData dictionary Deletes a key/value pair from the sessionData dictionary Lists Inserts new items at position Updates item values starting at the given position, as many items as there are in the array Move items within the list Remove items from the list","title":"Specification"},{"location":"gui_protocol/#connection-mycroftguiconnected","text":"on connection gui clients announce themselves This is an extension by OVOS to the original mycroft protocol { \"type\": \"mycroft.gui.connected\", \"gui_id\": \"unique_identifier_provided_by_client\" }","title":"CONNECTION - mycroft.gui.connected"},{"location":"gui_protocol/#namespaces","text":"ovos-gui maintains a list of namespaces with GUI data, namespaces usually correspond to a skill_id Every message in the gui protocol specifies a namespace it belongs to gui clients usualy display all namespaces, but can be requested to display a single one, eg, have a dedicated window to show a skill as a traditional desktop app","title":"NAMESPACES"},{"location":"gui_protocol/#active-skills-mycroftsystemactive_skills","text":"a reserved namespace is \"mycroft.system.active_skills\", the data contained in this namespace defines the namespace display priority Recent skills are ordered from the last used to the oldest, so the first item of the list will always be the the one showing any GUI page, if available. see the section about lists if you need to modify active skills","title":"Active Skills - mycroft.system.active_skills"},{"location":"gui_protocol/#pages-mycroftguilistxxx","text":"Each active skill is associated with a list of uris to the QML files of all gui items that are supposed to be visible. Non QT GUIS get sent other file extensions such as .jsx or .html using the same message format","title":"PAGES - mycroft.gui.list.xxx"},{"location":"gui_protocol/#insert-new-page-at-position","text":"{ \"type\": \"mycroft.gui.list.insert\", \"namespace\": \"mycroft.weather\" \"position\": 2 \"values\": [{\"url\": \"file://..../currentWeather.qml\"}, ...] //values must always be in array form }","title":"Insert new page at position"},{"location":"gui_protocol/#move-pages-within-the-list","text":"{ \"type\": \"mycroft.gui.list.move\", \"namespace\": \"mycroft.weather\" \"from\": 2 \"to\": 5 \"items_number\": 2 //optional in case we want to move a big chunk of list at once }","title":"Move pages within the list"},{"location":"gui_protocol/#remove-pages-from-the-list","text":"{ \"type\": \"mycroft.gui.list.remove\", \"namespace\": \"mycroft.weather\" \"position\": 2 \"items_number\": 5 //optional in case we want to get rid a big chunk of list at once }","title":"Remove pages from the list"},{"location":"gui_protocol/#events-mycrofteventstriggered","text":"Events can either be emitted by a gui client (eg, some element clicked) or by the skill (eg, in response to a voice command) { \"type\": \"mycroft.events.triggered\" \"namespace\": \"my_skill_id\" \"event_name\": \"my.gui.event\", \"parameters\": {\"item\": 3} }","title":"EVENTS - mycroft.events.triggered"},{"location":"gui_protocol/#special-event-page_gained_focus","text":"This event is used when the ovos-gui wants a page of a particular skill to gain user attention focus and become the current active view and \"focus of attention\" of the user. when a GUI client receives it, it should render the requested GUI page GUI clients can also emit this event, if a new page was rendered (eg, in response to a user swipping left) NOTE: for responsiveness it is recommened this message is only emitted after the rendering has actually been done, skills may be waiting for this event to initiate some actons { \"type\": \"mycroft.events.triggered\", \"namespace\": \"mycroft.weather\", \"event_name\": \"page_gained_focus\", \"data\": {\"number\": 0} } The parameter \"number\" is the position (starting from zero) of the page","title":"SPECIAL EVENT: page_gained_focus"},{"location":"gui_protocol/#skill-data-mycroftsessionxxx","text":"At the center of data sharing there is a key/value dictionary that is kept synchronized between ovos-gui and the GUI client. Values can either be simple strings, numbers and booleans or be more complicated data types this event can be sent from gui clients (eg, in response to a dropdown selection) or from skills (eg, change weather data) NOTE: Once a new gui client connects to ovos-gui, all existing session data is sent to the client, after that the client gets live updates via these events","title":"SKILL DATA - mycroft.session.xxx"},{"location":"gui_protocol/#sets-a-new-keyvalue-in-the-sessiondata-dictionary","text":"Either sets a new key/value pair or replace an existing old value. { \"type\": \"mycroft.session.set\", \"namespace\": \"weather.mycroft\" \"data\": { \"temperature\": \"28\", \"icon\": \"cloudy\", \"forecast\": [{...},...] //if it's a list see below for more message types } }","title":"Sets a new key/value in the sessionData dictionary"},{"location":"gui_protocol/#deletes-a-keyvalue-pair-from-the-sessiondata-dictionary","text":"{ \"type\": \"mycroft.session.delete\", \"namespace\": \"weather.mycroft\" \"property\": \"temperature\" }","title":"Deletes a key/value pair from the sessionData dictionary"},{"location":"gui_protocol/#lists","text":"","title":"Lists"},{"location":"gui_protocol/#inserts-new-items-at-position","text":"{ \"type\": \"mycroft.session.list.insert\", \"namespace\": \"weather.mycroft\" \"property\": \"forecast\" //the key of the main data map this list in contained into \"position\": 2 \"values\": [{\"date\": \"tomorrow\", \"temperature\" : 13, ...}, ...] //values must always be in array form }","title":"Inserts new items at position"},{"location":"gui_protocol/#updates-item-values-starting-at-the-given-position-as-many-items-as-there-are-in-the-array","text":"{ \"type\": \"mycroft.session.list.update\", \"namespace\": \"weather.mycroft\" \"property\": \"forecast\" \"position\": 2 \"values\": [{\"date\": \"tomorrow\", \"temperature\" : 13, ...}, ...] //values must always be in array form }","title":"Updates item values starting at the given position, as many items as there are in the array"},{"location":"gui_protocol/#move-items-within-the-list","text":"{ \"type\": \"mycroft.session.list.move\", \"namespace\": \"weather.mycroft\" \"property\": \"forecast\" \"from\": 2 \"to\": 5 \"items_number\": 2 //optional in case we want to move a big chunk of list at once }","title":"Move items within the list"},{"location":"gui_protocol/#remove-items-from-the-list","text":"{ \"type\": \"mycroft.session.list.remove\", \"namespace\": \"weather.mycroft\" \"property\": \"forecast\" \"position\": 2 \"items_number\": 5 //optional in case we want to get rid a big chunk of list at once }","title":"Remove items from the list"},{"location":"gui_service/","text":"GUI Service OVOS devices with displays provide skill developers the opportunity to create skills that can be empowered by both voice and screen interaction. ovos-gui , aka, The GUI Service, is responsible for keeping track of what should be rendered, but does not perform the rendering itself The GUI service provides a websocket for gui clients to connect to, it is responsible for implementing the gui protocol under ovos-core . You can find indepth documentation of the GUI protocol in the dedicated GUI section of these docs Architecture The GUI state is defined by namespaces , usually corresponding to a skill_id , each with any number of pages . users are expected to be able to \"swipe left\" and \"swipe right\" to switch between pages within a namespace OpenVoiceOS components interact with the GUI by defining session data and active pages, gui-clients may also send back events to indicate interactions. pages are ordered and, usually, only 1 page is rendered at a time. If the screen size allows it platform specific gui client applications are free to render all pages into view. The GUI clients may be implemented in any programming language, the default page templates provided to skills via GUIInterface should be implemented and provided by all alternative clients. QML tip : - set fillWidth: true under Mycroft.Delegate to have a page always using the full screen Active Namespaces In the context of a smartspeaker, when the GUI is idle a homescreen may be displayed, eg. an animated face or clock Whenever a page is displayed by a skill, ovos-gui tracks it and sets it's namespace to active, then tells the gui clients to render it. The active namespace and how long a page stays up are managed by ovos-gui , usually via platform specific plugins. ovos-gui will decide when a namespace is no longer active, and then the next namespace will be rendered, Skills using the GUIInterface can indicate how long they want a page to remain active Example: OVOS is idle - homescreen is the active namespace you ask OVOS to play music and the music page shows up - music player page is the active namespace you ask OVOS a question and wolfram alpha page shows up - wolfram page is the active namespace wolfram alpha times out - music player page is the active namespace music ends and page times out - homescreen is the active namespace NOTE : GUI does not yet support Session, in the future namespaces will be tracked per Session allowing remote clients to each have their own GUI state GUI Plugins NEW in ovos-core version 0.0.8 The active namespace is tracked by ovos-gui and manages the homescreen skill , desktop implementations like Plasma Bigscreen do not have a homescreen, instead they manage each skill in their own window A single GUI plugin can be loaded in ovos-gui to handle bus events and provide companion python code to GUI clients, this is usually done by your OS and specific to a use case. plugin description notes ovos-gui-plugin-shell-companion extra functionality for ovos-shell message specs ovos-gui-plugin-bigscreen window management for Plasma Bigscreen Voice Apps UNMAINTED message specs backup fork in life support ovos-plasma-bigscreen-qt5 ovos-gui-plugin-plasmoid Plasma integration of different Mycroft AI services UNMAINTED never migrated to OVOS Configuration The gui service has a few sections in mycroft.conf \"gui\": { \"idle_display_skill\": \"skill-ovos-homescreen.openvoiceos\", \"extension\": \"generic\", \"generic\": { \"homescreen_supported\": false } }, \"gui_websocket\": { \"host\": \"0.0.0.0\", \"base_port\": 18181, \"route\": \"/gui\", \"ssl\": false },","title":"GUI"},{"location":"gui_service/#gui-service","text":"OVOS devices with displays provide skill developers the opportunity to create skills that can be empowered by both voice and screen interaction. ovos-gui , aka, The GUI Service, is responsible for keeping track of what should be rendered, but does not perform the rendering itself The GUI service provides a websocket for gui clients to connect to, it is responsible for implementing the gui protocol under ovos-core . You can find indepth documentation of the GUI protocol in the dedicated GUI section of these docs","title":"GUI Service"},{"location":"gui_service/#architecture","text":"The GUI state is defined by namespaces , usually corresponding to a skill_id , each with any number of pages . users are expected to be able to \"swipe left\" and \"swipe right\" to switch between pages within a namespace OpenVoiceOS components interact with the GUI by defining session data and active pages, gui-clients may also send back events to indicate interactions. pages are ordered and, usually, only 1 page is rendered at a time. If the screen size allows it platform specific gui client applications are free to render all pages into view. The GUI clients may be implemented in any programming language, the default page templates provided to skills via GUIInterface should be implemented and provided by all alternative clients. QML tip : - set fillWidth: true under Mycroft.Delegate to have a page always using the full screen","title":"Architecture"},{"location":"gui_service/#active-namespaces","text":"In the context of a smartspeaker, when the GUI is idle a homescreen may be displayed, eg. an animated face or clock Whenever a page is displayed by a skill, ovos-gui tracks it and sets it's namespace to active, then tells the gui clients to render it. The active namespace and how long a page stays up are managed by ovos-gui , usually via platform specific plugins. ovos-gui will decide when a namespace is no longer active, and then the next namespace will be rendered, Skills using the GUIInterface can indicate how long they want a page to remain active Example: OVOS is idle - homescreen is the active namespace you ask OVOS to play music and the music page shows up - music player page is the active namespace you ask OVOS a question and wolfram alpha page shows up - wolfram page is the active namespace wolfram alpha times out - music player page is the active namespace music ends and page times out - homescreen is the active namespace NOTE : GUI does not yet support Session, in the future namespaces will be tracked per Session allowing remote clients to each have their own GUI state","title":"Active Namespaces"},{"location":"gui_service/#gui-plugins","text":"NEW in ovos-core version 0.0.8 The active namespace is tracked by ovos-gui and manages the homescreen skill , desktop implementations like Plasma Bigscreen do not have a homescreen, instead they manage each skill in their own window A single GUI plugin can be loaded in ovos-gui to handle bus events and provide companion python code to GUI clients, this is usually done by your OS and specific to a use case. plugin description notes ovos-gui-plugin-shell-companion extra functionality for ovos-shell message specs ovos-gui-plugin-bigscreen window management for Plasma Bigscreen Voice Apps UNMAINTED message specs backup fork in life support ovos-plasma-bigscreen-qt5 ovos-gui-plugin-plasmoid Plasma integration of different Mycroft AI services UNMAINTED never migrated to OVOS","title":"GUI Plugins"},{"location":"gui_service/#configuration","text":"The gui service has a few sections in mycroft.conf \"gui\": { \"idle_display_skill\": \"skill-ovos-homescreen.openvoiceos\", \"extension\": \"generic\", \"generic\": { \"homescreen_supported\": false } }, \"gui_websocket\": { \"host\": \"0.0.0.0\", \"base_port\": 18181, \"route\": \"/gui\", \"ssl\": false },","title":"Configuration"},{"location":"guidelines/","text":"Voice User Interface Design Guidelines Through these guidelines you will learn how to use principles of Voice User Interface Design to build more effective skills. These tools will help define and validate the features of the skill before diving deep into development. This guide will cover some methods to use that can help plan, prototype and test your skill during the early design stages. CREDITS - Voice User Interface Design Guidelines based on the original work of Derick Schweppe Interactions Intents Let's start with an example. A user in Melbourne, Australia might want to know about the weather. To ask for this information, they might say: \"Hey Mycroft, what's today's weather like?\" \"Hey Mycroft, what's the weather like in Melbourne?\" \"Hey Mycroft, weather\" Even though these are three different expressions, for most of us they probably have roughly the same meaning. In each case we would assume the user expects OVOS to respond with today's weather for their current location. It is up us as Skill creators to teach OVOS the variety of ways that a user might express the same intent. This is a key part of the design process. It is the key difference between a Skill that kind of works if you know what to say, and a Skill that feels intuitive and natural to talk to. This is handled by an intent parser whose job it is to learn from your Skill what intents it can handle, and extract from the user's speech and key information that might be useful for your Skill. In this case it might include the specified date and location. Statements and Prompts You can think of Prompts as questions and Statements as providing information to the user that does not need a follow-up response. For example a weather forecast like this would be considered a statement: Today\u2019s forecast is sunny with a high of 60 and a low of 45. Statements For a lot of skills the conversation might end with a simple statement from OVOS, and no further action is necessary. Try to imagine what the user is trying to accomplish, if a simple statement gets the job done there is no reason to keep the conversation rolling, and in fact a follow-up might annoy the user with unnecessary interaction. Prompts It may be tempting to always give users specific instructions like traditional automated phones systems (Interactive Voice Response). Many phone systems are notorious for being too verbose and difficult to follow. With OVOS we\u2019re trying to break that mold and make the interaction natural. If you follow the phone system method you may be giving the user the exact phrasing to say, but you\u2019re also taking up valuable time and training them to think the system is very narrow in capability. In the event that the user does give a response that your skill can not handle, create follow-up prompts that steer the user back on track. Remember, there are no errors in a cooperative conversation. Avoid Speaker Mycroft How many people are playing? For example, you can say 2 players. Better Speaker Mycroft How many players? User My Brother and Myself Mycroft I\u2019m sorry, what was the number of players? User Two In the first example the user is told explicitly what they can say, but the prompt is unnatural, we don\u2019t typically suggest responses to a conversation partner in real life. These long-winded prompts can become tiresome if they are used repeatedly throughout the skill. Remember the phone system example, typically poorly designed automated phone systems inundate the user with many options and additional instructions at every step of the interaction. In the second example we see a better prompt, although the user gives a response that is easy for a human to understand it is more difficult to design a skill to understand. Instead, the skill designer can apply a re-prompt strategy that steers the user back on track and doesn't require them to start the interaction over. In this case the re-prompt changes the wording slightly to make it clear that the user needs to say a number. The next time the user interacts with this Skill, they will likely say a number in their first interaction. Determining whether to respond to the user with a statement or a prompt can be a bit tricky. When the user is somewhat vague it is tempting to assume the user\u2019s intent in order to speed along the interaction. Sometimes this is the right approach and sometimes it is not. If you are very confident in the user\u2019s input then it is alright to assume, and respond with a statement. For example in the Wikipedia Skill the Wikipedia API is used to confidently select the best response. Wikipedia Skill Speaker User Tell me about Abraham Lincoln Mycroft Abraham Lincoln was an American statesman and lawyer who served as the 16th president of the UnitedStates from 1861 to 1865. Lincoln led the nation through its greatest moral, constitutional, and political crisis in the American Civil War . In contrast, let\u2019s look at an example of where a follow-up prompt is a better approach. You\u2019re working on a skill that gives important information about birds, such as wingspan, laden and unladen airspeed, etc.. Avoid Speaker **** User what is the airspeed velocity of an unladen swallow? Mycroft The unladen airspeed of the european swallow is 20.1 miles per hour. Better Speaker User What is the airspeed velocity of an unladen Swallow? Mycroft What do you mean, African or European Swallow? User European Swallow. Mycroft The unladen airspeed of the European Swallow is 20.1 miles per hour. In the first example Mycroft assumes the user is referring to the European Swallow, however there is nothing to give confidence to that assumption. In this case it\u2019s dangerous to assume the User meant the European Swallow since it is just as likely they meant the African Swallow. When there is no confidence in one answer versus another, OVOS should follow up with a prompt as demonstrated in the second example. Another approach to solving this problem is to offer the most likely answer and inform the user that there are other answers available with a statement. Confirmations Confirmation approaches can also be defined by Statements or Prompts , but when we talk about them in the context of confirmations we call them Implicit and Explicit. Implicit Confirmation This type of confirmation is also a statement. The idea is to parrot the information back to the user to confirm that it was correct, but not require additional input from the user. The implicit confirmation can be used in a majority of situations. For example Mycroft\u2019s Timer Skill uses implicit confirmation when creating new timers. Timer Skill Speaker User Hey Mycroft set a timer for 5 minutes Mycroft Alright, I\u2019m starting a 5 minute timer Avoid Speaker User Hey Mycroft, add bananas to my shopping list Mycroft done Better Speaker User Hey Mycroft, add bananas to my shopping list Mycroft Adding bananas Explicit Confirmation This type of confirmation requires an input from the user to verify everything is correct. Only use explicit confirmation when needed as it can be tedious for the user to always confirm their input. This strategy is best when there are multiple steps, and it would be difficult to fix a misstep without starting over, or when deleting or modifying an existing multistep input. Avoid Speaker User Hey Mycroft, set an alarm for 9:00 am tomorrow Mycroft Ok, I\u2019ve set an alarm for 9:00 am tomorrow, is that correct? User Yes Mycroft 9:00 am alarm set Better Speaker User Hey Mycroft, set an alarm for 9:00 am tomorrow Mycroft I've set an alarm for tomorrow at 9:00 am, 16 hours from now The first example requires the user to confirm the alarm. Since the interaction is quite short it isn\u2019t necessary to use explicit confirmation. Avoid Speaker User Hey Mycroft, set a calendar event for a meeting with Chris Stevens Mycroft When is the event? User Tomorrow at 11:00 am Mycroft should I send an invite to Chris Stevens? User Yes Mycroft Alright, I've created a meeting for Tomorrow Mycroft Alright, I\u2019ve created a meeting for Tomorrow at 11:00 am with Chris Stevens User Cancel my 11:00 am meeting tomorrow Mycroft Done Better Speaker User Hey Mycroft, set a calendar event for a meeting with Chris Stevens Mycroft When is the event? User Tomorrow at 11:00 am Mycroft should I send an invite to Chris Stevens? User Yes Mycroft Alright, I\u2019ve created a meeting for Tomorrow at 11:00 am with Chris Stevens User Cancel my 11:00 am meeting tomorrow Mycroft Are you sure you want to delete your meeting tomorrow at 11:00 am? User Yes Both examples use explicit and implicit confirmation correctly until the end of the interaction when the user wants to cancel the event. Unlike setting a simple timer or an alarm creating this calendar event requires multiple steps and would require reasonable effort to recreate. There may also be significant consequences if the request was misinterpreted and the meeting was cancelled unintentionally. Conversations Any time the user needs to input a lot of information or the user needs to sort through a variety of options a conversation will be needed. Users may be used to systems that require them to separate input into different chunks. For example, many personal navigation devices (GPS units) require users to input the street number, street name, city, and state separately, like this: Speaker Navigation Device What is the street number User 123 Navigation Device What is the street name User Main Street Navigation Device What is the city User Springfield Navigation Device Oregon This was a limitation of the technology, both the STT and intent parser were not capable of handling an entire address. With OVOS you do not need to separate the address into multiple inputs. It\u2019s also easy to think like a traditional GUI and translate what are typically separate fields into prompts for the user, but that will only slow down the interaction. The best solution is to allow the user to input information both ways, all at once, or in pieces. For example the Alarm skill allows the user to set a complicated alarm all in one go, or in steps. In Steps: Speaker User Hey Mycroft, set a recurring alarm Mycroft For what day? User weekdays Mycroft For what time? User 7 am Mycroft I've set a recurring Alarm for 7 am on weekdays All at once: Speaker User Hey Mycroft, set a recurring alarm for weekdays at 7 am Mycroft I've set a recurring Alarm for 7 am on weekdays Context Conversational context is a given when speaking to another person, but not always when interacting with a computer based system. Many skills are designed with only a quick back and forth interaction in mind, and can operate well without conversational context, but conversational context can be a powerful addition to a skill and can make the interaction much more life like. Here are some examples of how to use context in conversation. Speaker User Who is Shaquille O'Neal Mycroft Shaquille O'Neal is an American former professional basketball player who is a sports analyst on the television program Inside the NBA on TNT. He is considered one of the greatest players in National Basketball Association history User How tall is he? Mycroft seven foot 1 inches tall User when did he retire Mycroft 2011 Error Handling Inevitably, the user will say something that your skill can\u2019t handle. It\u2019s best not to think of these as errors on the part of the user, remember there aren\u2019t really errors in conversations. Conversations are naturally cooperative with both parties seeking to continue to a successful outcome. Keeping that in mind you should do your best to avoid Error Handling that is a dead-end. Avoid Speaker User Timer Mycroft I'm sorry I can't help you with that Better Speaker Use r Timer Mycroft A timer for how long? User 5 minutes In the first example OVOS does not give the user any options to finish the job they set out to do. Try to avoid situations where the user has to start the conversation over.","title":"Design Guidelines"},{"location":"guidelines/#voice-user-interface-design-guidelines","text":"Through these guidelines you will learn how to use principles of Voice User Interface Design to build more effective skills. These tools will help define and validate the features of the skill before diving deep into development. This guide will cover some methods to use that can help plan, prototype and test your skill during the early design stages. CREDITS - Voice User Interface Design Guidelines based on the original work of Derick Schweppe","title":"Voice User Interface Design Guidelines"},{"location":"guidelines/#interactions","text":"","title":"Interactions"},{"location":"guidelines/#intents","text":"Let's start with an example. A user in Melbourne, Australia might want to know about the weather. To ask for this information, they might say: \"Hey Mycroft, what's today's weather like?\" \"Hey Mycroft, what's the weather like in Melbourne?\" \"Hey Mycroft, weather\" Even though these are three different expressions, for most of us they probably have roughly the same meaning. In each case we would assume the user expects OVOS to respond with today's weather for their current location. It is up us as Skill creators to teach OVOS the variety of ways that a user might express the same intent. This is a key part of the design process. It is the key difference between a Skill that kind of works if you know what to say, and a Skill that feels intuitive and natural to talk to. This is handled by an intent parser whose job it is to learn from your Skill what intents it can handle, and extract from the user's speech and key information that might be useful for your Skill. In this case it might include the specified date and location.","title":"Intents"},{"location":"guidelines/#statements-and-prompts","text":"You can think of Prompts as questions and Statements as providing information to the user that does not need a follow-up response. For example a weather forecast like this would be considered a statement: Today\u2019s forecast is sunny with a high of 60 and a low of 45.","title":"Statements and Prompts"},{"location":"guidelines/#statements","text":"For a lot of skills the conversation might end with a simple statement from OVOS, and no further action is necessary. Try to imagine what the user is trying to accomplish, if a simple statement gets the job done there is no reason to keep the conversation rolling, and in fact a follow-up might annoy the user with unnecessary interaction.","title":"Statements"},{"location":"guidelines/#prompts","text":"It may be tempting to always give users specific instructions like traditional automated phones systems (Interactive Voice Response). Many phone systems are notorious for being too verbose and difficult to follow. With OVOS we\u2019re trying to break that mold and make the interaction natural. If you follow the phone system method you may be giving the user the exact phrasing to say, but you\u2019re also taking up valuable time and training them to think the system is very narrow in capability. In the event that the user does give a response that your skill can not handle, create follow-up prompts that steer the user back on track. Remember, there are no errors in a cooperative conversation. Avoid Speaker Mycroft How many people are playing? For example, you can say 2 players. Better Speaker Mycroft How many players? User My Brother and Myself Mycroft I\u2019m sorry, what was the number of players? User Two In the first example the user is told explicitly what they can say, but the prompt is unnatural, we don\u2019t typically suggest responses to a conversation partner in real life. These long-winded prompts can become tiresome if they are used repeatedly throughout the skill. Remember the phone system example, typically poorly designed automated phone systems inundate the user with many options and additional instructions at every step of the interaction. In the second example we see a better prompt, although the user gives a response that is easy for a human to understand it is more difficult to design a skill to understand. Instead, the skill designer can apply a re-prompt strategy that steers the user back on track and doesn't require them to start the interaction over. In this case the re-prompt changes the wording slightly to make it clear that the user needs to say a number. The next time the user interacts with this Skill, they will likely say a number in their first interaction. Determining whether to respond to the user with a statement or a prompt can be a bit tricky. When the user is somewhat vague it is tempting to assume the user\u2019s intent in order to speed along the interaction. Sometimes this is the right approach and sometimes it is not. If you are very confident in the user\u2019s input then it is alright to assume, and respond with a statement. For example in the Wikipedia Skill the Wikipedia API is used to confidently select the best response. Wikipedia Skill Speaker User Tell me about Abraham Lincoln Mycroft Abraham Lincoln was an American statesman and lawyer who served as the 16th president of the UnitedStates from 1861 to 1865. Lincoln led the nation through its greatest moral, constitutional, and political crisis in the American Civil War . In contrast, let\u2019s look at an example of where a follow-up prompt is a better approach. You\u2019re working on a skill that gives important information about birds, such as wingspan, laden and unladen airspeed, etc.. Avoid Speaker **** User what is the airspeed velocity of an unladen swallow? Mycroft The unladen airspeed of the european swallow is 20.1 miles per hour. Better Speaker User What is the airspeed velocity of an unladen Swallow? Mycroft What do you mean, African or European Swallow? User European Swallow. Mycroft The unladen airspeed of the European Swallow is 20.1 miles per hour. In the first example Mycroft assumes the user is referring to the European Swallow, however there is nothing to give confidence to that assumption. In this case it\u2019s dangerous to assume the User meant the European Swallow since it is just as likely they meant the African Swallow. When there is no confidence in one answer versus another, OVOS should follow up with a prompt as demonstrated in the second example. Another approach to solving this problem is to offer the most likely answer and inform the user that there are other answers available with a statement.","title":"Prompts"},{"location":"guidelines/#confirmations","text":"Confirmation approaches can also be defined by Statements or Prompts , but when we talk about them in the context of confirmations we call them Implicit and Explicit.","title":"Confirmations"},{"location":"guidelines/#implicit-confirmation","text":"This type of confirmation is also a statement. The idea is to parrot the information back to the user to confirm that it was correct, but not require additional input from the user. The implicit confirmation can be used in a majority of situations. For example Mycroft\u2019s Timer Skill uses implicit confirmation when creating new timers. Timer Skill Speaker User Hey Mycroft set a timer for 5 minutes Mycroft Alright, I\u2019m starting a 5 minute timer Avoid Speaker User Hey Mycroft, add bananas to my shopping list Mycroft done Better Speaker User Hey Mycroft, add bananas to my shopping list Mycroft Adding bananas","title":"Implicit Confirmation"},{"location":"guidelines/#explicit-confirmation","text":"This type of confirmation requires an input from the user to verify everything is correct. Only use explicit confirmation when needed as it can be tedious for the user to always confirm their input. This strategy is best when there are multiple steps, and it would be difficult to fix a misstep without starting over, or when deleting or modifying an existing multistep input. Avoid Speaker User Hey Mycroft, set an alarm for 9:00 am tomorrow Mycroft Ok, I\u2019ve set an alarm for 9:00 am tomorrow, is that correct? User Yes Mycroft 9:00 am alarm set Better Speaker User Hey Mycroft, set an alarm for 9:00 am tomorrow Mycroft I've set an alarm for tomorrow at 9:00 am, 16 hours from now The first example requires the user to confirm the alarm. Since the interaction is quite short it isn\u2019t necessary to use explicit confirmation. Avoid Speaker User Hey Mycroft, set a calendar event for a meeting with Chris Stevens Mycroft When is the event? User Tomorrow at 11:00 am Mycroft should I send an invite to Chris Stevens? User Yes Mycroft Alright, I've created a meeting for Tomorrow Mycroft Alright, I\u2019ve created a meeting for Tomorrow at 11:00 am with Chris Stevens User Cancel my 11:00 am meeting tomorrow Mycroft Done Better Speaker User Hey Mycroft, set a calendar event for a meeting with Chris Stevens Mycroft When is the event? User Tomorrow at 11:00 am Mycroft should I send an invite to Chris Stevens? User Yes Mycroft Alright, I\u2019ve created a meeting for Tomorrow at 11:00 am with Chris Stevens User Cancel my 11:00 am meeting tomorrow Mycroft Are you sure you want to delete your meeting tomorrow at 11:00 am? User Yes Both examples use explicit and implicit confirmation correctly until the end of the interaction when the user wants to cancel the event. Unlike setting a simple timer or an alarm creating this calendar event requires multiple steps and would require reasonable effort to recreate. There may also be significant consequences if the request was misinterpreted and the meeting was cancelled unintentionally.","title":"Explicit Confirmation"},{"location":"guidelines/#conversations","text":"Any time the user needs to input a lot of information or the user needs to sort through a variety of options a conversation will be needed. Users may be used to systems that require them to separate input into different chunks. For example, many personal navigation devices (GPS units) require users to input the street number, street name, city, and state separately, like this: Speaker Navigation Device What is the street number User 123 Navigation Device What is the street name User Main Street Navigation Device What is the city User Springfield Navigation Device Oregon This was a limitation of the technology, both the STT and intent parser were not capable of handling an entire address. With OVOS you do not need to separate the address into multiple inputs. It\u2019s also easy to think like a traditional GUI and translate what are typically separate fields into prompts for the user, but that will only slow down the interaction. The best solution is to allow the user to input information both ways, all at once, or in pieces. For example the Alarm skill allows the user to set a complicated alarm all in one go, or in steps. In Steps: Speaker User Hey Mycroft, set a recurring alarm Mycroft For what day? User weekdays Mycroft For what time? User 7 am Mycroft I've set a recurring Alarm for 7 am on weekdays All at once: Speaker User Hey Mycroft, set a recurring alarm for weekdays at 7 am Mycroft I've set a recurring Alarm for 7 am on weekdays","title":"Conversations"},{"location":"guidelines/#context","text":"Conversational context is a given when speaking to another person, but not always when interacting with a computer based system. Many skills are designed with only a quick back and forth interaction in mind, and can operate well without conversational context, but conversational context can be a powerful addition to a skill and can make the interaction much more life like. Here are some examples of how to use context in conversation. Speaker User Who is Shaquille O'Neal Mycroft Shaquille O'Neal is an American former professional basketball player who is a sports analyst on the television program Inside the NBA on TNT. He is considered one of the greatest players in National Basketball Association history User How tall is he? Mycroft seven foot 1 inches tall User when did he retire Mycroft 2011","title":"Context"},{"location":"guidelines/#error-handling","text":"Inevitably, the user will say something that your skill can\u2019t handle. It\u2019s best not to think of these as errors on the part of the user, remember there aren\u2019t really errors in conversations. Conversations are naturally cooperative with both parties seeking to continue to a successful outcome. Keeping that in mind you should do your best to avoid Error Handling that is a dead-end. Avoid Speaker User Timer Mycroft I'm sorry I can't help you with that Better Speaker Use r Timer Mycroft A timer for how long? User 5 minutes In the first example OVOS does not give the user any options to finish the job they set out to do. Try to avoid situations where the user has to start the conversation over.","title":"Error Handling"},{"location":"homescreen/","text":"OpenVoiceOS Home Screen The home screen is the central place for all your tasks. It is what your device displays when it is idle Configuration Select a homescreen in mycroft.conf or via ovos-shell \"gui\": { \"idle_display_skill\": \"skill-ovos-homescreen.openvoiceos\" } Resting Faces The resting face API provides skill authors the ability to extend their skills to supply their own customized IDLE screens that will be displayed when there is no activity on the screen. import requests from ovos_workshop.skills import OVOSSkill from ovos_workshop.descorators import intent_handler, resting_screen_handler class CatSkill(OVOSSkill): def update_cat(self): r = requests.get('https://api.thecatapi.com/v1/images/search') return r.json()[0]['url'] @resting_screen_handler(\"Cat Image\") def idle(self, message): img = self.update_cat() self.gui.show_image(img) @intent_handler('show_cat.intent') def cat_handler(self, message): img = self.update_cat() self.gui.show_image(img) self.speak_dialog('mjau') A more advanced example, refreshing a webpage on a timer from ovos_workshop.skills import OVOSSkill from ovos_workshop.descorators import intent_handler, resting_screen_handler class WebpageHomescreen(OVOSSkill): def initialize(self): \"\"\"Perform final setup of Skill.\"\"\" # Disable manual refresh until this Homepage is made active. self.is_active = False self.disable_intent(\"refresh-homepage.intent\") self.settings_change_callback = self.refresh_homescreen def get_intro_message(self): \"\"\"Provide instructions on first install.\"\"\" self.speak_dialog(\"setting-url\") self.speak_dialog(\"selecting-homescreen\") @resting_screen_handler(\"Webpage Homescreen\") def handle_request_to_use_homescreen(self, message: Message): \"\"\"Handler for requests from GUI to use this Homescreen.\"\"\" self.is_active = True self.display_homescreen() self.refresh_homescreen(message) self.enable_intent(\"refresh-homepage.intent\") def display_homescreen(self): \"\"\"Display the selected webpage as the Homescreen.\"\"\" default_url = \"https://openvoiceos.github.io/status\" url = self.settings.get(\"homepage_url\", default_url) self.gui.show_url(url) @intent_handler(\"refresh-homepage.intent\") def refresh_homescreen(self, message: Message): \"\"\"Update refresh rate of homescreen and refresh screen. Defaults to 600 seconds / 10 minutes. \"\"\" self.cancel_scheduled_event(\"refresh-webpage-homescreen\") if self.is_active: self.schedule_repeating_event( self.display_homescreen, 0, self.settings.get(\"refresh_frequency\", 600), name=\"refresh-webpage-homescreen\", ) def shutdown(self): \"\"\"Actions to perform when Skill is shutting down.\"\"\" self.is_active = False self.cancel_all_repeating_events()","title":"Homescreen"},{"location":"homescreen/#openvoiceos-home-screen","text":"The home screen is the central place for all your tasks. It is what your device displays when it is idle","title":"OpenVoiceOS Home Screen"},{"location":"homescreen/#configuration","text":"Select a homescreen in mycroft.conf or via ovos-shell \"gui\": { \"idle_display_skill\": \"skill-ovos-homescreen.openvoiceos\" }","title":"Configuration"},{"location":"homescreen/#resting-faces","text":"The resting face API provides skill authors the ability to extend their skills to supply their own customized IDLE screens that will be displayed when there is no activity on the screen. import requests from ovos_workshop.skills import OVOSSkill from ovos_workshop.descorators import intent_handler, resting_screen_handler class CatSkill(OVOSSkill): def update_cat(self): r = requests.get('https://api.thecatapi.com/v1/images/search') return r.json()[0]['url'] @resting_screen_handler(\"Cat Image\") def idle(self, message): img = self.update_cat() self.gui.show_image(img) @intent_handler('show_cat.intent') def cat_handler(self, message): img = self.update_cat() self.gui.show_image(img) self.speak_dialog('mjau') A more advanced example, refreshing a webpage on a timer from ovos_workshop.skills import OVOSSkill from ovos_workshop.descorators import intent_handler, resting_screen_handler class WebpageHomescreen(OVOSSkill): def initialize(self): \"\"\"Perform final setup of Skill.\"\"\" # Disable manual refresh until this Homepage is made active. self.is_active = False self.disable_intent(\"refresh-homepage.intent\") self.settings_change_callback = self.refresh_homescreen def get_intro_message(self): \"\"\"Provide instructions on first install.\"\"\" self.speak_dialog(\"setting-url\") self.speak_dialog(\"selecting-homescreen\") @resting_screen_handler(\"Webpage Homescreen\") def handle_request_to_use_homescreen(self, message: Message): \"\"\"Handler for requests from GUI to use this Homescreen.\"\"\" self.is_active = True self.display_homescreen() self.refresh_homescreen(message) self.enable_intent(\"refresh-homepage.intent\") def display_homescreen(self): \"\"\"Display the selected webpage as the Homescreen.\"\"\" default_url = \"https://openvoiceos.github.io/status\" url = self.settings.get(\"homepage_url\", default_url) self.gui.show_url(url) @intent_handler(\"refresh-homepage.intent\") def refresh_homescreen(self, message: Message): \"\"\"Update refresh rate of homescreen and refresh screen. Defaults to 600 seconds / 10 minutes. \"\"\" self.cancel_scheduled_event(\"refresh-webpage-homescreen\") if self.is_active: self.schedule_repeating_event( self.display_homescreen, 0, self.settings.get(\"refresh_frequency\", 600), name=\"refresh-webpage-homescreen\", ) def shutdown(self): \"\"\"Actions to perform when Skill is shutting down.\"\"\" self.is_active = False self.cancel_all_repeating_events()","title":"Resting Faces"},{"location":"intents/","text":"Intents A user can accomplish the same task by expressing their intent in multiple ways. The role of the intent parser is to extract from the user's speech key data elements that specify their intent in more detail. This data can then be passed to other services, such as Skills to help the user accomplish their intended task. Example : Julie wants to know about today's weather in her current location, which is Melbourne, Australia. \"hey mycroft, what's today's weather like?\" \"hey mycroft, what's the weather like in Melbourne?\" \"hey mycroft, weather\" Even though these are three different expressions, for most of us they probably have roughly the same meaning. In each case we would assume the user expects OVOS to respond with today's weather for their current location. The role of an intent parser is to determine what this intent is. In the example above, we might extract data elements like: weather - we know that Julie wants to know about the weather, but she has not been specific about the type of weather, such as wind , precipitation , snowfall or the risk of fire danger from bushfires. Melbourne, Australia rarely experiences snowfall, but falls under bushfire risk every summer. location - Julie has stipulated her location as Melbourne, but she does not state that she means Melbourne, Australia. How do we distinguish this from Melbourne, Florida, United States? date - Julie has been specific about the timeframe she wants weather data for - today. But how do we know what today means in Julie's timezone. Melbourne, Australia is between 14-18 hours ahead of the United States. We don't want to give Julie yesterday's weather, particularly as Melbourne is renowned for having changeable weather. OVOS has two separate Intent parsing engines each with their own strengths. Each of these can be used in most situations, however they will process the utterance in different ways. Example based intents are trained on whole phrases. These intents are generally more accurate however require you to include sample phrases that cover the breadth of ways that a User may ask about something. **Keyword / Rule based ** these intents look for specific required keywords. They are more flexible, but since these are essentially rule based this can result in a lot of false matches. A badly designed intent may totally throw the intent parser off guard. The main advantage of keyword based intents is the integration with conversational context , they facilitate continuous dialogs OVOS is moving towards a plugin system for intent engines, currently only the default MycroftAI intent parsers are supported Padatious is a light-weight neural network that is trained on whole phrases. You can find the official documentation here Adapt is a keyword based parser. You can find the official documentation here We will now look at each in more detail, including how to use them in a Skill. Keyword Intents Keyword based intent parsers determine user intent based on a list of keywords or entities contained within a user's utterance. Defining keywords and entities Vocab (.voc) Files Vocab files define keywords that the intent parser will look for in a Users utterance to determine their intent. These files can be located in either the vocab/lang-code/ or locale/lang-code/ directories of a Skill. They can have one or more lines to list synonyms or terms that have the same meaning in the context of this Skill. OVOS will match any of these keywords with the Intent. Consider a simple Potato.voc . Within this file we might include: potato potatoes spud If the User speaks either : potato or potatoes or spud OVOS will match this to any Keyword Intents that are using the Potato keyword. Regular Expression (.rx) Files Regular expressions (or regex) allow us to capture entities based on the structure of an utterance. We strongly recommend you avoid using regex, it is very hard to make portable across languages, hard to translate and the reported confidence of the intents is not great. We suggest using example based intents instead if you find yourself needing regex These files can be located in either the regex/lang-code/ or locale/lang-code/ directories of a Skill. They can have one or more lines to provide different ways that an entity may be referenced. OVOS will execute these lines in the order they appear and return the first result as an entity to the Intent Handler. Let's consider a type.rx file to extract the type of potato we are interested in. Within this file we might include: .* about (?P<Type>.*) potatoes .* (make|like) (?P<Type>.*) potato What is this regex doing? .* matches zero, one or more of any single character. (?P<Type>.*) is known as a Named Capturing Group. The variable name is defined between the \"<>\", and what is captured is defined after this name. In this case we use .* to capture anything. Learn more about Regular Expressions . So our first line would match an utterance such as: Tell me about sweet potatoes Whilst the second line will match either: Do you like deep fried potato or How do I make mashed potato From these three utterances, what will the extracted Type be:\\ 1. sweet \\ 2. deep fried \\ 3. mashed This Type will be available to use in your Skill's Intent Handler on the message object. We can access this using: message.data.get('Type') Using Keyword Intents in a Skill Now that we have a Vocab and Regular Expression defined, let's look at how to use these in a simple Skill. For the following example we will use the two files we outlined above: Potato.voc Type.rx We will also add some new .voc files: Like.voc - containing a single line \"like\" You.voc - containing a single line \"you\" I.voc - containing a single line \"I\" Creating the Intent Handler To construct a Keyword Intent, we use the intent_handler() _decorator_ and pass in the IntentBuilder helper class. Learn more about decorators in Python . Both of these must be imported before we can use them: from ovos_workshop.intents import IntentBuilder from ovos_workshop.decorators import intent_handler The IntentBuilder is then passed the name of the Intent as a string, followed by one or more parameters that correspond with one of our .voc or .rx files. @intent_handler(IntentBuilder('IntentName') .require('Potato') .require('Like') .optionally('Type') .one_of('You', 'I')) In this example: the Potato and Like keywords are required. It must be present for the intent to match. the Type entity is optional. A stronger match will be made if this is found, but it is not required. we require at least one of the You or I keywords. What are some utterances that would match this intent? Do you like potato? Do you like fried potato? Will I like mashed potato? Do you think I would like potato? What are some utterances that would not match the intent? How do I make mashed potato? The required Like keyword is not found. Is it like a potato? Neither the You nor I keyword is found. Including it in a Skill Now we can create our Potato Skill: from ovos_workshop.intents import IntentBuilder from ovos_workshop.skills import OVOSSkill from ovos_workshop.decorators import intent_handler class PotatoSkill(OVOSSkill): @intent_handler(IntentBuilder('WhatIsPotato').require('What') .require('Potato')) def handle_what_is(self, message): self.speak_dialog('potato.description') @intent_handler(IntentBuilder('DoYouLikePotato').require('Potato') .require('Like').optionally('Type').one_of('You', 'I')) def handle_do_you_like(self, message): potato_type = message.data.get('Type') if potato_type is not None: self.speak_dialog('like.potato.type', {'type': potato_type}) else: self.speak_dialog('like.potato.generic') You can download this entire Potato Skill from Github , or see another Keyword Intent handler example in the Hello World Skill Common Problems More vocab! One of the most common mistakes when getting started with Skills is that the vocab file doesn't include all the keywords or terms that a User might use to trigger the intent. It is important to map out your Skill and test the interactions with others to see how they might ask questions differently. I have added new phrases in the .voc file, but Mycroft isn't recognizing them Compound words like \"don't\", \"won't\", \"shouldn't\" etc. are normalized by OVOS - so they become \"do not\", \"will not\", \"should not\". You should use the normalized words in your .voc files. Similarly, definite articles like the word \"the\" are removed in the normalization process, so avoid using them in your .voc or .rx files as well. Tab != 4 Spaces, sometimes your text editor or IDE automatically replaces tabs with spaces or vice versa. This may lead to an indentation error. So make sure there's no extra tabs and that your editor doesn't replace your spaces! Wrong order of files directories is a very common mistake. You have to make a language sub-folder inside the dialog, vocab or locale folders such as skill-dir/locale/en-us/somefile.dialog . So make sure that your .voc files and .dialog files inside a language subfolder. I am unable to match against the utterance string The utterance string received from the speech-to-text engine is received all lowercase. As such any string matching you are trying to do should also be converted to lowercase. For example: @intent_handler(IntentBuilder('Example').require('Example').require('Intent')) def handle_example(self, message): utterance = message.data.get('utterance') if 'Proper Noun'.lower() in utterance: self.speak('Found it') Example based Intents Example based parsers have a number of key benefits over other intent parsing technologies. Intents are easy to create You can easily extract entities and then use these in Skills. For example, \"Find the nearest gas station\" -> { \"place\":\"gas station\"} Disambiguation between intents is easier Harder to create a bad intent that throws the intent parser off Creating Intents Most example based intent parsers use a series of example sentences to train a machine learning model to identify an intent. Regex can also be used behind the scenes for example to extract entities The examples are stored in a Skill's vocab/lang or local/lang directory, in files ending in the file extension .intent . For example, if you were to create a tomato Skill to respond to questions about a tomato , you would create the file vocab/en-us/what.is.a.tomato.intent This file would contain examples of questions asking what a tomato is. what would you say a tomato is what is a tomato describe a tomato what defines a tomato These sample phrases do not require punctuation like a question mark. We can also leave out contractions such as \"what's\", as this will be automatically expanded to \"what is\" by OVOS before the utterance is parsed. Each file should contain at least 4 examples for good modeling. The above example allows us to map many phrases to a single intent, however often we need to extract specific data from an utterance. This might be a date, location, category, or some other entity . Defining entities Let's now find out OVOS's opinion on different types of tomatoes. To do this we will create a new intent file: vocab/en-us/do.you.like.intent with examples of questions about mycroft's opinion about tomatoes: are you fond of tomatoes do you like tomatoes what are your thoughts on tomatoes are you fond of {type} tomatoes do you like {type} tomatoes what are your thoughts on {type} tomatoes Note the {type} in the above examples. These are wild-cards where matching content is forwarded to the skill's intent handler. Specific Entities In the above example, {type} will match anything. While this makes the intent flexible, it will also match if we say something like Do you like eating tomatoes?. It would think the type of tomato is \"eating\" which doesn't make much sense. Instead, we can specify what type of things the {type} of tomato should be. We do this by defining the type entity file here: vocab/en-us/type.entity which might contain something like: red reddish green greenish yellow yellowish ripe unripe pale This must be registered in the Skill before use - most commonly in the initialize() method: from ovos_workshop.skills import OVOSSkill from ovos_workshop.decorators import intent_handler class TomatoSkill(OVOSSkill): def initialize(self): self.register_entity_file('type.entity') Now, we can say things like \"do you like greenish tomatoes?\" and it will tag type as: \"greenish\" . However, if we say \"do you like eating tomatoes?\" - the phrase will not match as \"eating\" is not included in our type.entity file. Number matching Let's say you are writing an Intent to call a phone number. You can make it only match specific formats of numbers by writing out possible arrangements using # where a number would go. For example, with the following intent: Call {number}. Call the phone number {number}. the number.entity could be written as: +### (###) ###-#### +## (###) ###-#### +# (###) ###-#### (###) ###-#### ###-#### ###-###-#### ###.###.#### ### ### #### ########## Entities with unknown tokens Let's say you wanted to create an intent to match places: Directions to {place}. Navigate me to {place}. Open maps to {place}. Show me how to get to {place}. How do I get to {place}? This alone will work, but it will still get a high confidence with a phrase like \"How do I get to the boss in my game?\". We can try creating a .entity file with things like: New York City #### Georgia Street San Francisco The problem is, now anything that is not specifically a mix of New York City, San Francisco, or something on Georgia Street won't match. Instead, we can specify an unknown word with :0. This would be written as: :0 :0 City #### :0 Street :0 :0 Now, while this will still match quite a lot, it will match things like \"Directions to Baldwin City\" more than \"How do I get to the boss in my game?\" NOTE: Currently, the number of :0 words is not fully taken into consideration so the above might match quite liberally, but this will change in the future. Parentheses Expansion Sometimes you might find yourself writing a lot of variations of the same thing. For example, to write a skill that orders food, you might write the following intent: Order some {food}. Order some {food} from {place}. Grab some {food}. Grab some {food} from {place}. Rather than writing out all combinations of possibilities, you can embed them into one or more lines by writing each possible option inside parentheses with | in between each part. For example, that same intent above could be written as: (Order | Grab) some {food} (Order | Grab) some {food} from {place} or even on a single-line: (Order | Grab) some {food} (from {place} | ) Nested parentheses are supported to create even more complex combinations, such as the following: (Look (at | for) | Find) {object}. Which would expand to: Look at {object} Look for {object} Find {object} There is no performance benefit to using parentheses expansion. When used appropriately, this syntax can be much clearer to read. However, more complex structures should be broken down into multiple lines to aid readability and reduce false utterances being included in the model. Overuse can even result in the model training timing out, rendering the Skill unusable. Using it in a Skill The intent_handler() decorator can be used to create an examples based intent handler by passing in the filename of the .intent file as a string. You may also see the @intent_file_handler decorator used in Skills. This has been deprecated and you can now replace any instance of this with the simpler @intent_handler decorator. From our first example above, we created a file vocab/en-us/what.is.a.tomato.intent . To register an intent using this file we can use: @intent_handler('what.is.a.tomato.intent') This decorator must be imported before it is used: from ovos_workshop.decorators import intent_handler Learn more about decorators in Python . Now we can create our Tomato Skill: from ovos_workshop.skills import OVOSSkill from ovos_workshop.decorators import intent_handler class TomatoSkill(OVOSSkill): def initialize(self): self.register_entity_file('type.entity') @intent_handler('what.is.a.tomato.intent') def handle_what_is(self, message): self.speak_dialog('tomato.description') @intent_handler('do.you.like.intent') def handle_do_you_like(self, message): tomato_type = message.data.get('type') if tomato_type is not None: self.speak_dialog('like.tomato.type', {'type': tomato_type}) else: self.speak_dialog('like.tomato.generic') def stop(self): pass def create_skill(): return TomatoSkill() See a Padatious intent handler example in the Hello World Skill Common Problems I am unable to match against the utterance string The utterance string received from the speech-to-text engine is received all lowercase. As such any string matching you are trying to do should also be converted to lowercase. For example: @intent_handler('example.intent') def handle_example(self, message): utterance = message.data.get('utterance') if 'Proper Noun'.lower() in utterance: self.speak('Found it')","title":"Intents"},{"location":"intents/#intents","text":"A user can accomplish the same task by expressing their intent in multiple ways. The role of the intent parser is to extract from the user's speech key data elements that specify their intent in more detail. This data can then be passed to other services, such as Skills to help the user accomplish their intended task. Example : Julie wants to know about today's weather in her current location, which is Melbourne, Australia. \"hey mycroft, what's today's weather like?\" \"hey mycroft, what's the weather like in Melbourne?\" \"hey mycroft, weather\" Even though these are three different expressions, for most of us they probably have roughly the same meaning. In each case we would assume the user expects OVOS to respond with today's weather for their current location. The role of an intent parser is to determine what this intent is. In the example above, we might extract data elements like: weather - we know that Julie wants to know about the weather, but she has not been specific about the type of weather, such as wind , precipitation , snowfall or the risk of fire danger from bushfires. Melbourne, Australia rarely experiences snowfall, but falls under bushfire risk every summer. location - Julie has stipulated her location as Melbourne, but she does not state that she means Melbourne, Australia. How do we distinguish this from Melbourne, Florida, United States? date - Julie has been specific about the timeframe she wants weather data for - today. But how do we know what today means in Julie's timezone. Melbourne, Australia is between 14-18 hours ahead of the United States. We don't want to give Julie yesterday's weather, particularly as Melbourne is renowned for having changeable weather. OVOS has two separate Intent parsing engines each with their own strengths. Each of these can be used in most situations, however they will process the utterance in different ways. Example based intents are trained on whole phrases. These intents are generally more accurate however require you to include sample phrases that cover the breadth of ways that a User may ask about something. **Keyword / Rule based ** these intents look for specific required keywords. They are more flexible, but since these are essentially rule based this can result in a lot of false matches. A badly designed intent may totally throw the intent parser off guard. The main advantage of keyword based intents is the integration with conversational context , they facilitate continuous dialogs OVOS is moving towards a plugin system for intent engines, currently only the default MycroftAI intent parsers are supported Padatious is a light-weight neural network that is trained on whole phrases. You can find the official documentation here Adapt is a keyword based parser. You can find the official documentation here We will now look at each in more detail, including how to use them in a Skill.","title":"Intents"},{"location":"intents/#keyword-intents","text":"Keyword based intent parsers determine user intent based on a list of keywords or entities contained within a user's utterance.","title":"Keyword Intents"},{"location":"intents/#defining-keywords-and-entities","text":"","title":"Defining keywords and entities"},{"location":"intents/#vocab-voc-files","text":"Vocab files define keywords that the intent parser will look for in a Users utterance to determine their intent. These files can be located in either the vocab/lang-code/ or locale/lang-code/ directories of a Skill. They can have one or more lines to list synonyms or terms that have the same meaning in the context of this Skill. OVOS will match any of these keywords with the Intent. Consider a simple Potato.voc . Within this file we might include: potato potatoes spud If the User speaks either : potato or potatoes or spud OVOS will match this to any Keyword Intents that are using the Potato keyword.","title":"Vocab (.voc) Files"},{"location":"intents/#regular-expression-rx-files","text":"Regular expressions (or regex) allow us to capture entities based on the structure of an utterance. We strongly recommend you avoid using regex, it is very hard to make portable across languages, hard to translate and the reported confidence of the intents is not great. We suggest using example based intents instead if you find yourself needing regex These files can be located in either the regex/lang-code/ or locale/lang-code/ directories of a Skill. They can have one or more lines to provide different ways that an entity may be referenced. OVOS will execute these lines in the order they appear and return the first result as an entity to the Intent Handler. Let's consider a type.rx file to extract the type of potato we are interested in. Within this file we might include: .* about (?P<Type>.*) potatoes .* (make|like) (?P<Type>.*) potato What is this regex doing? .* matches zero, one or more of any single character. (?P<Type>.*) is known as a Named Capturing Group. The variable name is defined between the \"<>\", and what is captured is defined after this name. In this case we use .* to capture anything. Learn more about Regular Expressions . So our first line would match an utterance such as: Tell me about sweet potatoes Whilst the second line will match either: Do you like deep fried potato or How do I make mashed potato From these three utterances, what will the extracted Type be:\\ 1. sweet \\ 2. deep fried \\ 3. mashed This Type will be available to use in your Skill's Intent Handler on the message object. We can access this using: message.data.get('Type')","title":"Regular Expression (.rx) Files"},{"location":"intents/#using-keyword-intents-in-a-skill","text":"Now that we have a Vocab and Regular Expression defined, let's look at how to use these in a simple Skill. For the following example we will use the two files we outlined above: Potato.voc Type.rx We will also add some new .voc files: Like.voc - containing a single line \"like\" You.voc - containing a single line \"you\" I.voc - containing a single line \"I\"","title":"Using Keyword Intents in a Skill"},{"location":"intents/#creating-the-intent-handler","text":"To construct a Keyword Intent, we use the intent_handler() _decorator_ and pass in the IntentBuilder helper class. Learn more about decorators in Python . Both of these must be imported before we can use them: from ovos_workshop.intents import IntentBuilder from ovos_workshop.decorators import intent_handler The IntentBuilder is then passed the name of the Intent as a string, followed by one or more parameters that correspond with one of our .voc or .rx files. @intent_handler(IntentBuilder('IntentName') .require('Potato') .require('Like') .optionally('Type') .one_of('You', 'I')) In this example: the Potato and Like keywords are required. It must be present for the intent to match. the Type entity is optional. A stronger match will be made if this is found, but it is not required. we require at least one of the You or I keywords. What are some utterances that would match this intent? Do you like potato? Do you like fried potato? Will I like mashed potato? Do you think I would like potato? What are some utterances that would not match the intent? How do I make mashed potato? The required Like keyword is not found. Is it like a potato? Neither the You nor I keyword is found.","title":"Creating the Intent Handler"},{"location":"intents/#including-it-in-a-skill","text":"Now we can create our Potato Skill: from ovos_workshop.intents import IntentBuilder from ovos_workshop.skills import OVOSSkill from ovos_workshop.decorators import intent_handler class PotatoSkill(OVOSSkill): @intent_handler(IntentBuilder('WhatIsPotato').require('What') .require('Potato')) def handle_what_is(self, message): self.speak_dialog('potato.description') @intent_handler(IntentBuilder('DoYouLikePotato').require('Potato') .require('Like').optionally('Type').one_of('You', 'I')) def handle_do_you_like(self, message): potato_type = message.data.get('Type') if potato_type is not None: self.speak_dialog('like.potato.type', {'type': potato_type}) else: self.speak_dialog('like.potato.generic') You can download this entire Potato Skill from Github , or see another Keyword Intent handler example in the Hello World Skill","title":"Including it in a Skill"},{"location":"intents/#common-problems","text":"","title":"Common Problems"},{"location":"intents/#more-vocab","text":"One of the most common mistakes when getting started with Skills is that the vocab file doesn't include all the keywords or terms that a User might use to trigger the intent. It is important to map out your Skill and test the interactions with others to see how they might ask questions differently.","title":"More vocab!"},{"location":"intents/#i-have-added-new-phrases-in-the-voc-file-but-mycroft-isnt-recognizing-them","text":"Compound words like \"don't\", \"won't\", \"shouldn't\" etc. are normalized by OVOS - so they become \"do not\", \"will not\", \"should not\". You should use the normalized words in your .voc files. Similarly, definite articles like the word \"the\" are removed in the normalization process, so avoid using them in your .voc or .rx files as well. Tab != 4 Spaces, sometimes your text editor or IDE automatically replaces tabs with spaces or vice versa. This may lead to an indentation error. So make sure there's no extra tabs and that your editor doesn't replace your spaces! Wrong order of files directories is a very common mistake. You have to make a language sub-folder inside the dialog, vocab or locale folders such as skill-dir/locale/en-us/somefile.dialog . So make sure that your .voc files and .dialog files inside a language subfolder.","title":"I have added new phrases in the .voc file, but Mycroft isn't recognizing them"},{"location":"intents/#i-am-unable-to-match-against-the-utterance-string","text":"The utterance string received from the speech-to-text engine is received all lowercase. As such any string matching you are trying to do should also be converted to lowercase. For example: @intent_handler(IntentBuilder('Example').require('Example').require('Intent')) def handle_example(self, message): utterance = message.data.get('utterance') if 'Proper Noun'.lower() in utterance: self.speak('Found it')","title":"I am unable to match against the utterance string"},{"location":"intents/#example-based-intents","text":"Example based parsers have a number of key benefits over other intent parsing technologies. Intents are easy to create You can easily extract entities and then use these in Skills. For example, \"Find the nearest gas station\" -> { \"place\":\"gas station\"} Disambiguation between intents is easier Harder to create a bad intent that throws the intent parser off","title":"Example based Intents"},{"location":"intents/#creating-intents","text":"Most example based intent parsers use a series of example sentences to train a machine learning model to identify an intent. Regex can also be used behind the scenes for example to extract entities The examples are stored in a Skill's vocab/lang or local/lang directory, in files ending in the file extension .intent . For example, if you were to create a tomato Skill to respond to questions about a tomato , you would create the file vocab/en-us/what.is.a.tomato.intent This file would contain examples of questions asking what a tomato is. what would you say a tomato is what is a tomato describe a tomato what defines a tomato These sample phrases do not require punctuation like a question mark. We can also leave out contractions such as \"what's\", as this will be automatically expanded to \"what is\" by OVOS before the utterance is parsed. Each file should contain at least 4 examples for good modeling. The above example allows us to map many phrases to a single intent, however often we need to extract specific data from an utterance. This might be a date, location, category, or some other entity .","title":"Creating Intents"},{"location":"intents/#defining-entities","text":"Let's now find out OVOS's opinion on different types of tomatoes. To do this we will create a new intent file: vocab/en-us/do.you.like.intent with examples of questions about mycroft's opinion about tomatoes: are you fond of tomatoes do you like tomatoes what are your thoughts on tomatoes are you fond of {type} tomatoes do you like {type} tomatoes what are your thoughts on {type} tomatoes Note the {type} in the above examples. These are wild-cards where matching content is forwarded to the skill's intent handler.","title":"Defining entities"},{"location":"intents/#specific-entities","text":"In the above example, {type} will match anything. While this makes the intent flexible, it will also match if we say something like Do you like eating tomatoes?. It would think the type of tomato is \"eating\" which doesn't make much sense. Instead, we can specify what type of things the {type} of tomato should be. We do this by defining the type entity file here: vocab/en-us/type.entity which might contain something like: red reddish green greenish yellow yellowish ripe unripe pale This must be registered in the Skill before use - most commonly in the initialize() method: from ovos_workshop.skills import OVOSSkill from ovos_workshop.decorators import intent_handler class TomatoSkill(OVOSSkill): def initialize(self): self.register_entity_file('type.entity') Now, we can say things like \"do you like greenish tomatoes?\" and it will tag type as: \"greenish\" . However, if we say \"do you like eating tomatoes?\" - the phrase will not match as \"eating\" is not included in our type.entity file.","title":"Specific Entities"},{"location":"intents/#number-matching","text":"Let's say you are writing an Intent to call a phone number. You can make it only match specific formats of numbers by writing out possible arrangements using # where a number would go. For example, with the following intent: Call {number}. Call the phone number {number}. the number.entity could be written as: +### (###) ###-#### +## (###) ###-#### +# (###) ###-#### (###) ###-#### ###-#### ###-###-#### ###.###.#### ### ### #### ##########","title":"Number matching"},{"location":"intents/#entities-with-unknown-tokens","text":"Let's say you wanted to create an intent to match places: Directions to {place}. Navigate me to {place}. Open maps to {place}. Show me how to get to {place}. How do I get to {place}? This alone will work, but it will still get a high confidence with a phrase like \"How do I get to the boss in my game?\". We can try creating a .entity file with things like: New York City #### Georgia Street San Francisco The problem is, now anything that is not specifically a mix of New York City, San Francisco, or something on Georgia Street won't match. Instead, we can specify an unknown word with :0. This would be written as: :0 :0 City #### :0 Street :0 :0 Now, while this will still match quite a lot, it will match things like \"Directions to Baldwin City\" more than \"How do I get to the boss in my game?\" NOTE: Currently, the number of :0 words is not fully taken into consideration so the above might match quite liberally, but this will change in the future.","title":"Entities with unknown tokens"},{"location":"intents/#parentheses-expansion","text":"Sometimes you might find yourself writing a lot of variations of the same thing. For example, to write a skill that orders food, you might write the following intent: Order some {food}. Order some {food} from {place}. Grab some {food}. Grab some {food} from {place}. Rather than writing out all combinations of possibilities, you can embed them into one or more lines by writing each possible option inside parentheses with | in between each part. For example, that same intent above could be written as: (Order | Grab) some {food} (Order | Grab) some {food} from {place} or even on a single-line: (Order | Grab) some {food} (from {place} | ) Nested parentheses are supported to create even more complex combinations, such as the following: (Look (at | for) | Find) {object}. Which would expand to: Look at {object} Look for {object} Find {object} There is no performance benefit to using parentheses expansion. When used appropriately, this syntax can be much clearer to read. However, more complex structures should be broken down into multiple lines to aid readability and reduce false utterances being included in the model. Overuse can even result in the model training timing out, rendering the Skill unusable.","title":"Parentheses Expansion"},{"location":"intents/#using-it-in-a-skill","text":"The intent_handler() decorator can be used to create an examples based intent handler by passing in the filename of the .intent file as a string. You may also see the @intent_file_handler decorator used in Skills. This has been deprecated and you can now replace any instance of this with the simpler @intent_handler decorator. From our first example above, we created a file vocab/en-us/what.is.a.tomato.intent . To register an intent using this file we can use: @intent_handler('what.is.a.tomato.intent') This decorator must be imported before it is used: from ovos_workshop.decorators import intent_handler Learn more about decorators in Python . Now we can create our Tomato Skill: from ovos_workshop.skills import OVOSSkill from ovos_workshop.decorators import intent_handler class TomatoSkill(OVOSSkill): def initialize(self): self.register_entity_file('type.entity') @intent_handler('what.is.a.tomato.intent') def handle_what_is(self, message): self.speak_dialog('tomato.description') @intent_handler('do.you.like.intent') def handle_do_you_like(self, message): tomato_type = message.data.get('type') if tomato_type is not None: self.speak_dialog('like.tomato.type', {'type': tomato_type}) else: self.speak_dialog('like.tomato.generic') def stop(self): pass def create_skill(): return TomatoSkill() See a Padatious intent handler example in the Hello World Skill","title":"Using it in a Skill"},{"location":"intents/#common-problems_1","text":"","title":"Common Problems"},{"location":"intents/#i-am-unable-to-match-against-the-utterance-string_1","text":"The utterance string received from the speech-to-text engine is received all lowercase. As such any string matching you are trying to do should also be converted to lowercase. For example: @intent_handler('example.intent') def handle_example(self, message): utterance = message.data.get('utterance') if 'Proper Noun'.lower() in utterance: self.speak('Found it')","title":"I am unable to match against the utterance string"},{"location":"jurebes/","text":"jurebes J.U.R.E.B.E.S: Joint Universal Rule-based Engine and Bagging Ensemble-based System This acronym reflects a combined approach of using rule-based techniques along with a bagging ensemble-based approach for intent parsing in the JUREBES engine, written in Python with the use of NLTK and scikit-learn libraries. Usage from jurebes import JurebesIntentContainer hello = [\"hello human\", \"hello there\", \"hey\", \"hello\", \"hi\"] name = [\"my name is {name}\", \"call me {name}\", \"I am {name}\", \"the name is {name}\", \"{name} is my name\", \"{name} is my name\"] joke = [\"tell me a joke\", \"say a joke\", \"tell joke\"] engine = JurebesIntentContainer() engine.add_entity(\"name\", [\"jarbas\", \"bob\", \"Jo\u00e3o Casimiro Ferreira\"]) engine.add_intent(\"hello\", hello) engine.add_intent(\"name\", name) engine.add_intent(\"joke\", joke) engine.train() test_set = {\"name\": [\"I am groot\", \"my name is jarbas\", \"jarbas is the name\", \"they call me Ana Ferreira\"], \"hello\": [\"hello beautiful\", \"hello bob\", \"hello world\"], \"joke\": [\"say joke\", \"make me laugh\", \"do you know any joke\"]} for intent, sents in test_set.items(): for sent in sents: # optionally pass a threshold to return None instead of low confidence matches print(sent, engine.calc_intent(sent, threshold=0.5)) # I am groot IntentMatch(intent_name='name', confidence=1.0, entities={'name': 'groot'}) # my name is jarbas IntentMatch(intent_name='name', confidence=1.0, entities={'name': 'jarbas'}) # jarbas is the name IntentMatch(intent_name='name', confidence=0.9201351734080562, entities={'name': 'jarbas'}) # call me Ana Ferreira IntentMatch(intent_name='name', confidence=1.0, entities={'name': 'ferreira'}) # hello beautiful IntentMatch(intent_name='hello', confidence=0.8716522106345048, entities={}) # hello bob IntentMatch(intent_name='hello', confidence=0.5400801051648911, entities={'name': 'bob'}) # hello world IntentMatch(intent_name='hello', confidence=0.8716522106345048, entities={}) # say joke IntentMatch(intent_name='joke', confidence=0.9785338275012387, entities={}) # make me laugh IntentMatch(intent_name='name', confidence=0.725778770677012, entities={}) # do you know any joke IntentMatch(intent_name='joke', confidence=0.917960967116358, entities={}) Advanced Usage you can select the classifiers or enable fuzzy matching and influence predictions, jurebes is stateful from jurebes import JurebesIntentContainer # single clf clf = SVC(probability=True) # any scikit-learn clf # multiple classifiers will use soft voting to select prediction # clf = [SVC(probability=True), LogisticRegression(), DecisionTreeClassifier()] / default if not in args tagger = OVOSNgramTagger(default_tag=\"O\") # classic nltk / default if not in args #tagger = SVC(probability=True) # any scikit-learn clf #tagger = [SVC(probability=True), LogisticRegression(), DecisionTreeClassifier()] # pre defined pipelines from ovos-classifiers clf_pipeline = \"tfidf\" # default if not in args tagger_pipeline = \"words\" # default if not in args engine = JurebesIntentContainer(clf, tagger, clf_pipeline, tagger_pipeline) (...) # register intents # fuzzy matching engine.enable_fuzzy() sent = \"they call me Ana Ferreira\" print(engine.calc_intent(sent)) # IntentMatch(intent_name='name', confidence=0.8716633619210677, entities={'name': 'ana ferreira'}) engine.disable_fuzzy() print(engine.calc_intent(sent)) # IntentMatch(intent_name='name', confidence=0.8282293617609358, entities={'name': 'ferreira'}) # temporarily disable a intent engine.detach_intent(\"name\") print(engine.calc_intent(sent)) # IntentMatch(intent_name='hello', confidence=0.06113697262028985, entities={'name': 'ferreira'}) engine.reatach_intent(\"name\") print(engine.calc_intent(sent)) # IntentMatch(intent_name='name', confidence=0.8548664325189478, entities={'name': 'ferreira'}) # force correct predictions engine.exclude_keywords(\"name\", [\"laugh\"]) print(engine.calc_intent(\"make me laugh\")) # IntentMatch(intent_name='joke', confidence=0.5125373111690074, entities={}) engine.exclude_keywords(\"hello\", [\"laugh\"]) print(engine.calc_intent(\"make me laugh\")) # IntentMatch(intent_name='joke', confidence=1.0, entities={}) # inject context engine.set_context(\"joke\", \"joke_type\", \"chuck_norris\") # if a value is passed it will populate entities print(engine.calc_intent(\"tell me a chuch norris joke\")) # IntentMatch(intent_name='joke', confidence=0.9707841337857908, entities={'joke_type': 'chuck_norris'}) # require context engine.require_context(\"joke\", \"joke_type\") engine.unset_context(\"joke\", \"joke_type\") print(engine.calc_intent(\"tell me a chuch norris joke\")) # IntentMatch(intent_name='hello', confidence=0.060199275248566525, entities={}) engine.unrequire_context(\"joke\", \"joke_type\") print(engine.calc_intent(\"tell me a chuch norris joke\")) # IntentMatch(intent_name='joke', confidence=0.9462089582801377, entities={}) # exclude intent matches based on context engine.exclude_context(\"hello\", \"said_hello\") print(engine.calc_intent(\"hello\")) # IntentMatch(intent_name='hello', confidence=1, entities={}) engine.set_context(\"hello\", \"said_hello\") # now wont predict hello intent print(engine.calc_intent(\"hello\")) # IntentMatch(intent_name='joke', confidence=0.06986199472674888, entities={})","title":"Jurebes"},{"location":"jurebes/#jurebes","text":"J.U.R.E.B.E.S: Joint Universal Rule-based Engine and Bagging Ensemble-based System This acronym reflects a combined approach of using rule-based techniques along with a bagging ensemble-based approach for intent parsing in the JUREBES engine, written in Python with the use of NLTK and scikit-learn libraries.","title":"jurebes"},{"location":"jurebes/#usage","text":"from jurebes import JurebesIntentContainer hello = [\"hello human\", \"hello there\", \"hey\", \"hello\", \"hi\"] name = [\"my name is {name}\", \"call me {name}\", \"I am {name}\", \"the name is {name}\", \"{name} is my name\", \"{name} is my name\"] joke = [\"tell me a joke\", \"say a joke\", \"tell joke\"] engine = JurebesIntentContainer() engine.add_entity(\"name\", [\"jarbas\", \"bob\", \"Jo\u00e3o Casimiro Ferreira\"]) engine.add_intent(\"hello\", hello) engine.add_intent(\"name\", name) engine.add_intent(\"joke\", joke) engine.train() test_set = {\"name\": [\"I am groot\", \"my name is jarbas\", \"jarbas is the name\", \"they call me Ana Ferreira\"], \"hello\": [\"hello beautiful\", \"hello bob\", \"hello world\"], \"joke\": [\"say joke\", \"make me laugh\", \"do you know any joke\"]} for intent, sents in test_set.items(): for sent in sents: # optionally pass a threshold to return None instead of low confidence matches print(sent, engine.calc_intent(sent, threshold=0.5)) # I am groot IntentMatch(intent_name='name', confidence=1.0, entities={'name': 'groot'}) # my name is jarbas IntentMatch(intent_name='name', confidence=1.0, entities={'name': 'jarbas'}) # jarbas is the name IntentMatch(intent_name='name', confidence=0.9201351734080562, entities={'name': 'jarbas'}) # call me Ana Ferreira IntentMatch(intent_name='name', confidence=1.0, entities={'name': 'ferreira'}) # hello beautiful IntentMatch(intent_name='hello', confidence=0.8716522106345048, entities={}) # hello bob IntentMatch(intent_name='hello', confidence=0.5400801051648911, entities={'name': 'bob'}) # hello world IntentMatch(intent_name='hello', confidence=0.8716522106345048, entities={}) # say joke IntentMatch(intent_name='joke', confidence=0.9785338275012387, entities={}) # make me laugh IntentMatch(intent_name='name', confidence=0.725778770677012, entities={}) # do you know any joke IntentMatch(intent_name='joke', confidence=0.917960967116358, entities={})","title":"Usage"},{"location":"jurebes/#advanced-usage","text":"you can select the classifiers or enable fuzzy matching and influence predictions, jurebes is stateful from jurebes import JurebesIntentContainer # single clf clf = SVC(probability=True) # any scikit-learn clf # multiple classifiers will use soft voting to select prediction # clf = [SVC(probability=True), LogisticRegression(), DecisionTreeClassifier()] / default if not in args tagger = OVOSNgramTagger(default_tag=\"O\") # classic nltk / default if not in args #tagger = SVC(probability=True) # any scikit-learn clf #tagger = [SVC(probability=True), LogisticRegression(), DecisionTreeClassifier()] # pre defined pipelines from ovos-classifiers clf_pipeline = \"tfidf\" # default if not in args tagger_pipeline = \"words\" # default if not in args engine = JurebesIntentContainer(clf, tagger, clf_pipeline, tagger_pipeline) (...) # register intents # fuzzy matching engine.enable_fuzzy() sent = \"they call me Ana Ferreira\" print(engine.calc_intent(sent)) # IntentMatch(intent_name='name', confidence=0.8716633619210677, entities={'name': 'ana ferreira'}) engine.disable_fuzzy() print(engine.calc_intent(sent)) # IntentMatch(intent_name='name', confidence=0.8282293617609358, entities={'name': 'ferreira'}) # temporarily disable a intent engine.detach_intent(\"name\") print(engine.calc_intent(sent)) # IntentMatch(intent_name='hello', confidence=0.06113697262028985, entities={'name': 'ferreira'}) engine.reatach_intent(\"name\") print(engine.calc_intent(sent)) # IntentMatch(intent_name='name', confidence=0.8548664325189478, entities={'name': 'ferreira'}) # force correct predictions engine.exclude_keywords(\"name\", [\"laugh\"]) print(engine.calc_intent(\"make me laugh\")) # IntentMatch(intent_name='joke', confidence=0.5125373111690074, entities={}) engine.exclude_keywords(\"hello\", [\"laugh\"]) print(engine.calc_intent(\"make me laugh\")) # IntentMatch(intent_name='joke', confidence=1.0, entities={}) # inject context engine.set_context(\"joke\", \"joke_type\", \"chuck_norris\") # if a value is passed it will populate entities print(engine.calc_intent(\"tell me a chuch norris joke\")) # IntentMatch(intent_name='joke', confidence=0.9707841337857908, entities={'joke_type': 'chuck_norris'}) # require context engine.require_context(\"joke\", \"joke_type\") engine.unset_context(\"joke\", \"joke_type\") print(engine.calc_intent(\"tell me a chuch norris joke\")) # IntentMatch(intent_name='hello', confidence=0.060199275248566525, entities={}) engine.unrequire_context(\"joke\", \"joke_type\") print(engine.calc_intent(\"tell me a chuch norris joke\")) # IntentMatch(intent_name='joke', confidence=0.9462089582801377, entities={}) # exclude intent matches based on context engine.exclude_context(\"hello\", \"said_hello\") print(engine.calc_intent(\"hello\")) # IntentMatch(intent_name='hello', confidence=1, entities={}) engine.set_context(\"hello\", \"said_hello\") # now wont predict hello intent print(engine.calc_intent(\"hello\")) # IntentMatch(intent_name='joke', confidence=0.06986199472674888, entities={})","title":"Advanced Usage"},{"location":"lang_plugins/","text":"Language Detection/Translation Plugins These plugins can be used to detect the language of text and to translate it They are used internally by Universal Skills and integrated with external tools such as solvers List of Language plugins Plugin Detect Tx Offline Type ovos-translate-plugin-server \u2714\ufe0f \u2714\ufe0f \u274c API (self hosted) ovos-translate-plugin-nllb \u274c\ufe0f \u2714\ufe0f \u2714\ufe0f FOSS ovos-lang-detector-fasttext-plugin \u2714\ufe0f \u274c \u2714\ufe0f FOSS ovos-lang-detect-ngram-lm \u2714\ufe0f \u274c \u2714\ufe0f FOSS ovos-lang-detector-plugin-lingua-podre \u2714\ufe0f \u274c \u2714\ufe0f FOSS ovos-lang-detector-plugin-voter \u2714\ufe0f \u274c \u2714\ufe0f FOSS ovos-lang-detector-plugin-cld2 \u2714\ufe0f \u274c \u2714\ufe0f FOSS ovos-lang-detector-plugin-cld3 \u2714\ufe0f \u274c \u2714\ufe0f FOSS ovos-lang-detector-plugin-fastlang \u2714\ufe0f \u274c \u2714\ufe0f FOSS ovos-lang-detector-plugin-langdetect \u2714\ufe0f \u274c \u2714\ufe0f FOSS ovos-google-translate-plugin \u2714\ufe0f \u2714\ufe0f \u274c API (free) neon-lang-plugin-libretranslate \u2714\ufe0f \u2714\ufe0f \u274c API (self hosted) neon-lang-plugin-amazon_translate \u2714\ufe0f \u2714\ufe0f \u274c API (key) Standalone Usage TODO Plugin Template TODO","title":"Language Detection/Translation"},{"location":"lang_plugins/#language-detectiontranslation-plugins","text":"These plugins can be used to detect the language of text and to translate it They are used internally by Universal Skills and integrated with external tools such as solvers","title":"Language Detection/Translation Plugins"},{"location":"lang_plugins/#list-of-language-plugins","text":"Plugin Detect Tx Offline Type ovos-translate-plugin-server \u2714\ufe0f \u2714\ufe0f \u274c API (self hosted) ovos-translate-plugin-nllb \u274c\ufe0f \u2714\ufe0f \u2714\ufe0f FOSS ovos-lang-detector-fasttext-plugin \u2714\ufe0f \u274c \u2714\ufe0f FOSS ovos-lang-detect-ngram-lm \u2714\ufe0f \u274c \u2714\ufe0f FOSS ovos-lang-detector-plugin-lingua-podre \u2714\ufe0f \u274c \u2714\ufe0f FOSS ovos-lang-detector-plugin-voter \u2714\ufe0f \u274c \u2714\ufe0f FOSS ovos-lang-detector-plugin-cld2 \u2714\ufe0f \u274c \u2714\ufe0f FOSS ovos-lang-detector-plugin-cld3 \u2714\ufe0f \u274c \u2714\ufe0f FOSS ovos-lang-detector-plugin-fastlang \u2714\ufe0f \u274c \u2714\ufe0f FOSS ovos-lang-detector-plugin-langdetect \u2714\ufe0f \u274c \u2714\ufe0f FOSS ovos-google-translate-plugin \u2714\ufe0f \u2714\ufe0f \u274c API (free) neon-lang-plugin-libretranslate \u2714\ufe0f \u2714\ufe0f \u274c API (self hosted) neon-lang-plugin-amazon_translate \u2714\ufe0f \u2714\ufe0f \u274c API (key)","title":"List of Language plugins"},{"location":"lang_plugins/#standalone-usage","text":"TODO","title":"Standalone Usage"},{"location":"lang_plugins/#plugin-template","text":"TODO","title":"Plugin Template"},{"location":"layers/","text":"Intent Layers WARNING : Skills using these features might not play well with HiveMind due to shared state across satellites Managing Intents Sometimes you might want to manually enable or disable an intent, in OVOSSkills you can do this explicitly to create stateful interactions class RotatingIntentsSkill(OVOSSkill): def initialize(self): # NOTE: this must be done in initialize, not in __init__ self.disable_intent(\"B.intent\") self.disable_intent(\"C.intent\") @intent_handler(\"A.intent\") def handle_A_intent(self, message): # do stuff self.enable_intent(\"B.intent\") self.disable_intent(\"A.intent\") @intent_handler(\"B.intent\") def handle_B_intent(self, message): # do stuff self.enable_intent(\"C.intent\") self.disable_intent(\"B.intent\") @intent_handler(\"C.intent\") def handle_C_intent(self, message): # do stuff self.enable_intent(\"A.intent\") self.disable_intent(\"C.intent\") NOTE : Intent states are currently shared across Sessions State Machines Another utils provided by ovos-workshop is IntentLayers , to manage groups of intent together IntentLayers lend themselves well to implement state machines. The Manual way In this example we implement the Konami Code , doing everything the manual way instead of using decorators class KonamiCodeSkill(OVOSSkill): def initialize(self): self.counter = 0 self.top_fails = 3 up_intent = IntentBuilder('KonamiUpIntent').require(\"KonamiUpKeyword\").build() down_intent = IntentBuilder('KonamiDownIntent').require(\"KonamiDownKeyword\").build() left_intent = IntentBuilder('KonamiLeftIntent').require(\"KonamiLeftKeyword\").build() right_intent = IntentBuilder('KonamiRightIntent').require(\"KonamiRightKeyword\").build() b_intent = IntentBuilder('KonamiBIntent').require(\"KonamiBKeyword\").build() a_intent = IntentBuilder('KonamiAIntent').require(\"KonamiAKeyword\").build() self.register_intent(up_intent, self.handle_up_intent) self.register_intent(down_intent, self.handle_down_intent) self.register_intent(left_intent, self.handle_left_intent) self.register_intent(right_intent, self.handle_right_intent) self.register_intent(b_intent, self.handle_b_intent) self.register_intent(a_intent, self.handle_a_intent) def build_intent_layers(self): self.intent_layers.update_layer(\"up1\", [\"KonamiUpIntent\"]) self.intent_layers.update_layer(\"up2\", [\"KonamiUpIntent\"]) self.intent_layers.update_layer(\"down1\", [\"KonamiDownIntent\"]) self.intent_layers.update_layer(\"down2\", [\"KonamiDownIntent\"]) self.intent_layers.update_layer(\"left1\", [\"KonamiLeftIntent\"]) self.intent_layers.update_layer(\"right1\",[\"KonamiRightIntent\"]) self.intent_layers.update_layer(\"left2\", [\"KonamiLeftIntent\"]) self.intent_layers.update_layer(\"right2\",[\"KonamiRightIntent\"]) self.intent_layers.update_layer(\"B\",[\"KonamiBIntent\"]) self.intent_layers.update_layer(\"A\",[\"KonamiAIntent\"]) self.intent_layers.activate_layer(\"up1\") def reset(self): self.active = False self.counter = 0 self.intent_layers.disable() self.intent_layers.activate_layer(\"up1\") def handle_up_intent(self, message): if self.intent_layers.is_active(\"up1\"): self.intent_layers.deactivate_layer(\"up1\") self.intent_layers.activate_layer(\"up2\") else: self.intent_layers.activate_layer(\"down1\") self.intent_layers.deactivate_layer(\"up2\") self.acknowledge() def handle_down_intent(self, message): if self.intent_layers.is_active(\"down1\"): self.intent_layers.deactivate_layer(\"down1\") self.intent_layers.activate_layer(\"down2\") else: self.intent_layers.activate_layer(\"left1\") self.intent_layers.deactivate_layer(\"down2\") self.acknowledge() def handle_left_intent(self, message): if self.intent_layers.is_active(\"left1\"): self.intent_layers.deactivate_layer(\"left1\") self.intent_layers.activate_layer(\"right1\") else: self.intent_layers.deactivate_layer(\"left2\") self.intent_layers.activate_layer(\"right2\") self.acknowledge() def handle_right_intent(self, message): if self.intent_layers.is_active(\"right1\"): self.intent_layers.deactivate_layer(\"right1\") self.intent_layers.activate_layer(\"left2\") else: self.intent_layers.activate_layer(\"B\") self.intent_layers.deactivate_layer(\"right2\") self.acknowledge() def handle_b_intent(self, message): self.intent_layers.activate_layer(\"A\") self.intent_layers.deactivate_layer(\"B\") self.acknowledge() def handle_a_intent(self, message): self.play_audio(\"power_up.mp3\") self.reset() def stop(self): if self.active: self.reset() def converse(self, message): if self.active: if not any(self.voc_match(utt, kw) for kw in [\"KonamiUpKeyword\", \"KonamiDownKeyword\", \"KonamiLeftKeyword\", \"KonamiRightKeyword\", \"KonamiBKeyword\", \"KonamiAKeyword\"]): self.counter += 1 if self.counter > self.top_fails: self.speak(\"Wrong cheat code\") self.reset() else: self.speak(\"Wrong! Try again\") return True return False Decorators When you have many complex chained intents IntentLayers often makes your life easier, a layer is a named group of intents that you can manage at once. Slightly more complex than the previous example, we may want to offer several \"forks\" on the intent execution, enabling different intent groups depending on previous interactions skill-moon-game is an example full voice game implemented this way An excerpt from the game to illustrate usage of IntentLayer decorators NOTE : IntentLayers do not yet support Session, in this example all voice satellites would join the game from ovos_workshop.skills.decorators import layer_intent, enables_layer, \\ disables_layer, resets_layers class Apollo11GameSkill(OVOSSkill): def initialize(self): # start with all game states disabled self.intent_layers.disable() @intent_handler(IntentBuilder(\"StartApollo11Intent\"). \\ optionally(\"startKeyword\"). \\ require(\"MoonGameKeyword\")) def handle_start_intent(self, message=None): if not self.playing: self.playing = True self.speak_dialog(\"start.game\") self.handle_intro() else: self.speak_dialog(\"already.started\") @layer_intent(IntentBuilder(\"StopApollo11Intent\"). \\ require(\"stopKeyword\"). \\ optionally(\"MoonGameKeyword\"), layer_name=\"stop_game\") @resets_layers() def handle_game_over(self, message=None): if self.playing: self.speak_dialog(\"stop.game\") @enables_layer(layer_name=\"guard\") @enables_layer(layer_name=\"stop_game\") def handle_intro(self): self.speak_dialog(\"reach_gate\") self.speak_dialog(\"guard\") self.speak_dialog(\"present_id\", expect_response=True) @layer_intent(IntentBuilder(\"Yes1Apollo11Intent\").require(\"yesKeyword\"), layer_name=\"guard\") def handle_yes1(self, message=None): self.speak_dialog(\"guard_yes\") self.briefing_question1() @layer_intent(IntentBuilder(\"No1Apollo11Intent\").require(\"noKeyword\"), layer_name=\"guard\") @enables_layer(layer_name=\"guard2\") @disables_layer(layer_name=\"guard\") def handle_no1(self, message=None): self.speak_dialog(\"guard_no\") self.speak_dialog(\"present_id\", expect_response=True) # (...) more intent layers def converse(self, message): if not self.playing: return False # (...) # take corrective action when no intent matched if self.intent_layers.is_active(\"guard\") or \\ self.intent_layers.is_active(\"guard2\"): self.speak_dialog(\"guard_dead\") self.handle_game_over() # (...) else: self.speak_dialog(\"invalid.command\", expect_response=True) return True","title":"Intent Layers"},{"location":"layers/#intent-layers","text":"WARNING : Skills using these features might not play well with HiveMind due to shared state across satellites","title":"Intent Layers"},{"location":"layers/#managing-intents","text":"Sometimes you might want to manually enable or disable an intent, in OVOSSkills you can do this explicitly to create stateful interactions class RotatingIntentsSkill(OVOSSkill): def initialize(self): # NOTE: this must be done in initialize, not in __init__ self.disable_intent(\"B.intent\") self.disable_intent(\"C.intent\") @intent_handler(\"A.intent\") def handle_A_intent(self, message): # do stuff self.enable_intent(\"B.intent\") self.disable_intent(\"A.intent\") @intent_handler(\"B.intent\") def handle_B_intent(self, message): # do stuff self.enable_intent(\"C.intent\") self.disable_intent(\"B.intent\") @intent_handler(\"C.intent\") def handle_C_intent(self, message): # do stuff self.enable_intent(\"A.intent\") self.disable_intent(\"C.intent\") NOTE : Intent states are currently shared across Sessions","title":"Managing Intents"},{"location":"layers/#state-machines","text":"Another utils provided by ovos-workshop is IntentLayers , to manage groups of intent together IntentLayers lend themselves well to implement state machines.","title":"State Machines"},{"location":"layers/#the-manual-way","text":"In this example we implement the Konami Code , doing everything the manual way instead of using decorators class KonamiCodeSkill(OVOSSkill): def initialize(self): self.counter = 0 self.top_fails = 3 up_intent = IntentBuilder('KonamiUpIntent').require(\"KonamiUpKeyword\").build() down_intent = IntentBuilder('KonamiDownIntent').require(\"KonamiDownKeyword\").build() left_intent = IntentBuilder('KonamiLeftIntent').require(\"KonamiLeftKeyword\").build() right_intent = IntentBuilder('KonamiRightIntent').require(\"KonamiRightKeyword\").build() b_intent = IntentBuilder('KonamiBIntent').require(\"KonamiBKeyword\").build() a_intent = IntentBuilder('KonamiAIntent').require(\"KonamiAKeyword\").build() self.register_intent(up_intent, self.handle_up_intent) self.register_intent(down_intent, self.handle_down_intent) self.register_intent(left_intent, self.handle_left_intent) self.register_intent(right_intent, self.handle_right_intent) self.register_intent(b_intent, self.handle_b_intent) self.register_intent(a_intent, self.handle_a_intent) def build_intent_layers(self): self.intent_layers.update_layer(\"up1\", [\"KonamiUpIntent\"]) self.intent_layers.update_layer(\"up2\", [\"KonamiUpIntent\"]) self.intent_layers.update_layer(\"down1\", [\"KonamiDownIntent\"]) self.intent_layers.update_layer(\"down2\", [\"KonamiDownIntent\"]) self.intent_layers.update_layer(\"left1\", [\"KonamiLeftIntent\"]) self.intent_layers.update_layer(\"right1\",[\"KonamiRightIntent\"]) self.intent_layers.update_layer(\"left2\", [\"KonamiLeftIntent\"]) self.intent_layers.update_layer(\"right2\",[\"KonamiRightIntent\"]) self.intent_layers.update_layer(\"B\",[\"KonamiBIntent\"]) self.intent_layers.update_layer(\"A\",[\"KonamiAIntent\"]) self.intent_layers.activate_layer(\"up1\") def reset(self): self.active = False self.counter = 0 self.intent_layers.disable() self.intent_layers.activate_layer(\"up1\") def handle_up_intent(self, message): if self.intent_layers.is_active(\"up1\"): self.intent_layers.deactivate_layer(\"up1\") self.intent_layers.activate_layer(\"up2\") else: self.intent_layers.activate_layer(\"down1\") self.intent_layers.deactivate_layer(\"up2\") self.acknowledge() def handle_down_intent(self, message): if self.intent_layers.is_active(\"down1\"): self.intent_layers.deactivate_layer(\"down1\") self.intent_layers.activate_layer(\"down2\") else: self.intent_layers.activate_layer(\"left1\") self.intent_layers.deactivate_layer(\"down2\") self.acknowledge() def handle_left_intent(self, message): if self.intent_layers.is_active(\"left1\"): self.intent_layers.deactivate_layer(\"left1\") self.intent_layers.activate_layer(\"right1\") else: self.intent_layers.deactivate_layer(\"left2\") self.intent_layers.activate_layer(\"right2\") self.acknowledge() def handle_right_intent(self, message): if self.intent_layers.is_active(\"right1\"): self.intent_layers.deactivate_layer(\"right1\") self.intent_layers.activate_layer(\"left2\") else: self.intent_layers.activate_layer(\"B\") self.intent_layers.deactivate_layer(\"right2\") self.acknowledge() def handle_b_intent(self, message): self.intent_layers.activate_layer(\"A\") self.intent_layers.deactivate_layer(\"B\") self.acknowledge() def handle_a_intent(self, message): self.play_audio(\"power_up.mp3\") self.reset() def stop(self): if self.active: self.reset() def converse(self, message): if self.active: if not any(self.voc_match(utt, kw) for kw in [\"KonamiUpKeyword\", \"KonamiDownKeyword\", \"KonamiLeftKeyword\", \"KonamiRightKeyword\", \"KonamiBKeyword\", \"KonamiAKeyword\"]): self.counter += 1 if self.counter > self.top_fails: self.speak(\"Wrong cheat code\") self.reset() else: self.speak(\"Wrong! Try again\") return True return False","title":"The Manual way"},{"location":"layers/#decorators","text":"When you have many complex chained intents IntentLayers often makes your life easier, a layer is a named group of intents that you can manage at once. Slightly more complex than the previous example, we may want to offer several \"forks\" on the intent execution, enabling different intent groups depending on previous interactions skill-moon-game is an example full voice game implemented this way An excerpt from the game to illustrate usage of IntentLayer decorators NOTE : IntentLayers do not yet support Session, in this example all voice satellites would join the game from ovos_workshop.skills.decorators import layer_intent, enables_layer, \\ disables_layer, resets_layers class Apollo11GameSkill(OVOSSkill): def initialize(self): # start with all game states disabled self.intent_layers.disable() @intent_handler(IntentBuilder(\"StartApollo11Intent\"). \\ optionally(\"startKeyword\"). \\ require(\"MoonGameKeyword\")) def handle_start_intent(self, message=None): if not self.playing: self.playing = True self.speak_dialog(\"start.game\") self.handle_intro() else: self.speak_dialog(\"already.started\") @layer_intent(IntentBuilder(\"StopApollo11Intent\"). \\ require(\"stopKeyword\"). \\ optionally(\"MoonGameKeyword\"), layer_name=\"stop_game\") @resets_layers() def handle_game_over(self, message=None): if self.playing: self.speak_dialog(\"stop.game\") @enables_layer(layer_name=\"guard\") @enables_layer(layer_name=\"stop_game\") def handle_intro(self): self.speak_dialog(\"reach_gate\") self.speak_dialog(\"guard\") self.speak_dialog(\"present_id\", expect_response=True) @layer_intent(IntentBuilder(\"Yes1Apollo11Intent\").require(\"yesKeyword\"), layer_name=\"guard\") def handle_yes1(self, message=None): self.speak_dialog(\"guard_yes\") self.briefing_question1() @layer_intent(IntentBuilder(\"No1Apollo11Intent\").require(\"noKeyword\"), layer_name=\"guard\") @enables_layer(layer_name=\"guard2\") @disables_layer(layer_name=\"guard\") def handle_no1(self, message=None): self.speak_dialog(\"guard_no\") self.speak_dialog(\"present_id\", expect_response=True) # (...) more intent layers def converse(self, message): if not self.playing: return False # (...) # take corrective action when no intent matched if self.intent_layers.is_active(\"guard\") or \\ self.intent_layers.is_active(\"guard2\"): self.speak_dialog(\"guard_dead\") self.handle_game_over() # (...) else: self.speak_dialog(\"invalid.command\", expect_response=True) return True","title":"Decorators"},{"location":"license/","text":"License We have a universal donor policy, our code should be able to be used anywhere by anyone, no ifs or conditions attached. OVOS is predominately Apache2 or BSD licensed. There are only a few exceptions to this, which are all licensed under other compatible open source licenses. Individual plugins or skills may have their own license, for example mimic3 is AGPL, so we can not change the license of our plugin. We are committed to maintain all core components fully free, any code that we have no control over the license will live in an optional plugin and be flagged as such. This includes avoiding LGPL code for reasons explained here . Our license policy has the following properties: It gives you, the user of the software, complete and unrestrained access to the software, such that you may inspect, modify, and redistribute your changes Inspection - Anyone may inspect the software for security vulnerabilities Modification - Anyone may modify the software to fix issues or add features Redistribution - Anyone may redistribute the software on their terms It is compatible with GPL licenses - Projects licensed as GPL can be distributed with OVOS It allows for the incorporation of GPL-incompatible free software, such as software that is CDDL licensed The license does not restrict the software that may run on OVOS, however -- and thanks to the plugin architecture, even traditionally tightly-coupled components such as drivers can be distributed separately, so maintainers are free to choose whatever license they like for their projects. Notable licensing exceptions The following repositories do not respect our universal donor policy, please ensure their licenses are compatible before you use them Repository License Reason ovos-intent-plugin-padatious Apache2.0 padatious license might not be valid, depends on libfann2 (LGPL) ovos-tts-plugin-mimic3 AGPL depends on mimic3 (AGPL) ovos-tts-plugin-espeakng GPL depends on espeak-ng (GPL) ovos-g2p-plugin-espeak GPL depends on espeak-phonemizer (GPL) ovos-tts-plugin-SAM ? reverse engineered abandonware","title":"License"},{"location":"license/#license","text":"We have a universal donor policy, our code should be able to be used anywhere by anyone, no ifs or conditions attached. OVOS is predominately Apache2 or BSD licensed. There are only a few exceptions to this, which are all licensed under other compatible open source licenses. Individual plugins or skills may have their own license, for example mimic3 is AGPL, so we can not change the license of our plugin. We are committed to maintain all core components fully free, any code that we have no control over the license will live in an optional plugin and be flagged as such. This includes avoiding LGPL code for reasons explained here . Our license policy has the following properties: It gives you, the user of the software, complete and unrestrained access to the software, such that you may inspect, modify, and redistribute your changes Inspection - Anyone may inspect the software for security vulnerabilities Modification - Anyone may modify the software to fix issues or add features Redistribution - Anyone may redistribute the software on their terms It is compatible with GPL licenses - Projects licensed as GPL can be distributed with OVOS It allows for the incorporation of GPL-incompatible free software, such as software that is CDDL licensed The license does not restrict the software that may run on OVOS, however -- and thanks to the plugin architecture, even traditionally tightly-coupled components such as drivers can be distributed separately, so maintainers are free to choose whatever license they like for their projects.","title":"License"},{"location":"license/#notable-licensing-exceptions","text":"The following repositories do not respect our universal donor policy, please ensure their licenses are compatible before you use them Repository License Reason ovos-intent-plugin-padatious Apache2.0 padatious license might not be valid, depends on libfann2 (LGPL) ovos-tts-plugin-mimic3 AGPL depends on mimic3 (AGPL) ovos-tts-plugin-espeakng GPL depends on espeak-ng (GPL) ovos-g2p-plugin-espeak GPL depends on espeak-phonemizer (GPL) ovos-tts-plugin-SAM ? reverse engineered abandonware","title":"Notable licensing exceptions"},{"location":"media_plugins/","text":"Media Plugins these plugins handle media playback. OCP virtual player delegates media playback to these plugins plugin audio video web remote notes ovos-media-plugin-simple \u2714\ufe0f \u274c \u274c \u274c default for audio ovos-media-plugin-qt5 \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c WIP - recommended for embedded ovos-shell ovos-media-plugin-mplayer \u2714\ufe0f \u2714\ufe0f \u274c \u274c recommended for video ovos-media-plugin-vlc \u2714\ufe0f \u2714\ufe0f \u274c \u274c ovos-media-plugin-chromecast \u2714\ufe0f \u2714\ufe0f \u274c \u2714\ufe0f extra: cast_control for MPRIS interface ovos-media-plugin-spotify \u2714\ufe0f \u274c \u274c \u2714\ufe0f needs premium account extra: spotifyd for native spotify player ovos-media-plugin-xdg \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c xdg-open is for use inside a desktop session only ovos-media-plugin-webbrowser \u274c \u274c \u2714\ufe0f \u274c webbrowser is for use inside a desktop session only Standalone Usage TODO Plugin Template TODO","title":"Media Playback"},{"location":"media_plugins/#media-plugins","text":"these plugins handle media playback. OCP virtual player delegates media playback to these plugins plugin audio video web remote notes ovos-media-plugin-simple \u2714\ufe0f \u274c \u274c \u274c default for audio ovos-media-plugin-qt5 \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c WIP - recommended for embedded ovos-shell ovos-media-plugin-mplayer \u2714\ufe0f \u2714\ufe0f \u274c \u274c recommended for video ovos-media-plugin-vlc \u2714\ufe0f \u2714\ufe0f \u274c \u274c ovos-media-plugin-chromecast \u2714\ufe0f \u2714\ufe0f \u274c \u2714\ufe0f extra: cast_control for MPRIS interface ovos-media-plugin-spotify \u2714\ufe0f \u274c \u274c \u2714\ufe0f needs premium account extra: spotifyd for native spotify player ovos-media-plugin-xdg \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c xdg-open is for use inside a desktop session only ovos-media-plugin-webbrowser \u274c \u274c \u2714\ufe0f \u274c webbrowser is for use inside a desktop session only","title":"Media Plugins"},{"location":"media_plugins/#standalone-usage","text":"TODO","title":"Standalone Usage"},{"location":"media_plugins/#plugin-template","text":"TODO","title":"Plugin Template"},{"location":"mic_plugins/","text":"Microphone Plugins NEW in ovos-core version 0.0.8 Microphone plugins are responsible for feeding audio to the listener plugin notes operating systems ovos-microphone-plugin-alsa based on pyalsaaudio , best performance linux ovos-microphone-plugin-pyaudio based on PyAudio linux ovos-microphone-plugin-sounddevice based on python-sounddevice linux mac windows ovos-microphone-plugin-files audio files as input linux mac windows ovos-microphone-plugin-arecord uses arecord via subprocess linux ovos-microphone-plugin-socket demo plugin for remote microphone linux mac windows Standalone Usage TODO Plugin Template TODO","title":"Microphone"},{"location":"mic_plugins/#microphone-plugins","text":"NEW in ovos-core version 0.0.8 Microphone plugins are responsible for feeding audio to the listener plugin notes operating systems ovos-microphone-plugin-alsa based on pyalsaaudio , best performance linux ovos-microphone-plugin-pyaudio based on PyAudio linux ovos-microphone-plugin-sounddevice based on python-sounddevice linux mac windows ovos-microphone-plugin-files audio files as input linux mac windows ovos-microphone-plugin-arecord uses arecord via subprocess linux ovos-microphone-plugin-socket demo plugin for remote microphone linux mac windows","title":"Microphone Plugins"},{"location":"mic_plugins/#standalone-usage","text":"TODO","title":"Standalone Usage"},{"location":"mic_plugins/#plugin-template","text":"TODO","title":"Plugin Template"},{"location":"mk1_api/","text":"Enclosure Api The EnclosureApi is an abstraction over an hypothetical \"body\" housing OVOS eg, The Mark 1 Device is housed in an Enclosure . The Enclosure is the shell that houses a Device that runs OVOS. from ovos_bus_client.apis.enclosure import EnclosureApi api = EnclosureApi(bus) The Mark 1 Enclosure capabilities The Mark 1 mouth and eyes can be controlled by Skills using the self.enclosure object inherited from the OVOSSkill base class. This object acts as an interface to the Enclosure and allows the Skill creator to draw to the mouth display. This is how the mouth and eyes are made to change during operations such as audio playback. Dedicated utils for fine grained control over the mark 1 can be found at ovos-mark1-utils Drawing to the mouth display Drawing text to the mouth display Text can be sent to the display using the mouth_text() method of the enclosure object. self.enclosure.mouth_text('The meaning of life, the universe and everything is 42') If the text is too long to fit on the display, the text will scroll. @TODO how many characters will fit on the display before it will scroll? Drawing images to the mouth display Clearing an existing image from the mouth display Before writing an image to the mouth display, you should clear any previous image. self.enclosure.mouth_display(img_code=\"HIAAAAAAAAAAAAAA\", refresh=False) self.enclosure.mouth_display(img_code=\"HIAAAAAAAAAAAAAA\", x=24, refresh=False) How images are drawn on the mouth display The mouth display is a grid, 32 pixels wide and 8 pixels high. There are two ways to draw an image on the mouth display. Addressing each pixel using a string encoding You can draw an image to the mouth display by binary encoding pixel information in a string. The binary encoding is straightforward value substitution. Letter Value Pixel value A 0 B 1 C 2 D 3 E 4 F 5 G 6 H 7 I 8 and so on. self.enclosure.mouth_display(img_code=\"HIAAAAAAAAAAAAAA\", refresh=False) self.enclosure.mouth_display(img_code=\"HIAAAAAAAAAAAAAA\", x=24, refresh=False) The code above clears the image by sending a string consisting of HI which stands for a Width of 7 and a height of 8 and each A stands for a segment of 4 pixels in the off state. @TODO we really need a grid image here to show how it works - to make it easier to understand. Sending a PNG image to the mouth display Another way to draw an image on the mouth display is to create a PNG-formatted image with a width of 32 pixels and a height of 8 pixels, then use the mouth_display_png() method of the enclosure object. The image should be black and white, with white meaning a dark pixel, and black indicating an illuminated pixel. mouth_display_png() expects the first argument to be the image absolute path. Optional arguments are threshold : The value at which a pixel should be considered 'dark' or 'illuminated' invert : Treat white in the image as illuminated pixels, and black as dark pixels x : The x position (horizontal) at which the image should be displaye, in pixels y : The y position (vertical) at which the image should be displayed, in pixels refresh : clear the display before writing to it @TODO all the above needs to be validated - the information is educated guesswork self.mouth_display_png('/path/to/image.png', threshold=70, invert=False, x=0, y=0, refresh=True) Example image: Tools for converting PNG image representations to string representations If you don't want to convert PNG files at runtime (for example when creating simple animations) this short python script will convert PNG files to strings compatible with the img_code of self.enclosure.mouth_display() . Resetting the display to the default state When the Skill is finished, you should reset the Enclosure to the default state using self.enclosure.reset() This will clear the screen and blink the Mark 1's eyes once.","title":"Enclosure Api"},{"location":"mk1_api/#enclosure-api","text":"The EnclosureApi is an abstraction over an hypothetical \"body\" housing OVOS eg, The Mark 1 Device is housed in an Enclosure . The Enclosure is the shell that houses a Device that runs OVOS. from ovos_bus_client.apis.enclosure import EnclosureApi api = EnclosureApi(bus)","title":"Enclosure Api"},{"location":"mk1_api/#the-mark-1-enclosure-capabilities","text":"The Mark 1 mouth and eyes can be controlled by Skills using the self.enclosure object inherited from the OVOSSkill base class. This object acts as an interface to the Enclosure and allows the Skill creator to draw to the mouth display. This is how the mouth and eyes are made to change during operations such as audio playback. Dedicated utils for fine grained control over the mark 1 can be found at ovos-mark1-utils","title":"The Mark 1 Enclosure capabilities"},{"location":"mk1_api/#drawing-to-the-mouth-display","text":"","title":"Drawing to the mouth display"},{"location":"mk1_api/#drawing-text-to-the-mouth-display","text":"Text can be sent to the display using the mouth_text() method of the enclosure object. self.enclosure.mouth_text('The meaning of life, the universe and everything is 42') If the text is too long to fit on the display, the text will scroll. @TODO how many characters will fit on the display before it will scroll?","title":"Drawing text to the mouth display"},{"location":"mk1_api/#drawing-images-to-the-mouth-display","text":"Clearing an existing image from the mouth display Before writing an image to the mouth display, you should clear any previous image. self.enclosure.mouth_display(img_code=\"HIAAAAAAAAAAAAAA\", refresh=False) self.enclosure.mouth_display(img_code=\"HIAAAAAAAAAAAAAA\", x=24, refresh=False)","title":"Drawing images to the mouth display"},{"location":"mk1_api/#how-images-are-drawn-on-the-mouth-display","text":"The mouth display is a grid, 32 pixels wide and 8 pixels high. There are two ways to draw an image on the mouth display. Addressing each pixel using a string encoding You can draw an image to the mouth display by binary encoding pixel information in a string. The binary encoding is straightforward value substitution. Letter Value Pixel value A 0 B 1 C 2 D 3 E 4 F 5 G 6 H 7 I 8 and so on. self.enclosure.mouth_display(img_code=\"HIAAAAAAAAAAAAAA\", refresh=False) self.enclosure.mouth_display(img_code=\"HIAAAAAAAAAAAAAA\", x=24, refresh=False) The code above clears the image by sending a string consisting of HI which stands for a Width of 7 and a height of 8 and each A stands for a segment of 4 pixels in the off state. @TODO we really need a grid image here to show how it works - to make it easier to understand. Sending a PNG image to the mouth display Another way to draw an image on the mouth display is to create a PNG-formatted image with a width of 32 pixels and a height of 8 pixels, then use the mouth_display_png() method of the enclosure object. The image should be black and white, with white meaning a dark pixel, and black indicating an illuminated pixel. mouth_display_png() expects the first argument to be the image absolute path. Optional arguments are threshold : The value at which a pixel should be considered 'dark' or 'illuminated' invert : Treat white in the image as illuminated pixels, and black as dark pixels x : The x position (horizontal) at which the image should be displaye, in pixels y : The y position (vertical) at which the image should be displayed, in pixels refresh : clear the display before writing to it @TODO all the above needs to be validated - the information is educated guesswork self.mouth_display_png('/path/to/image.png', threshold=70, invert=False, x=0, y=0, refresh=True) Example image: Tools for converting PNG image representations to string representations If you don't want to convert PNG files at runtime (for example when creating simple animations) this short python script will convert PNG files to strings compatible with the img_code of self.enclosure.mouth_display() .","title":"How images are drawn on the mouth display"},{"location":"mk1_api/#resetting-the-display-to-the-default-state","text":"When the Skill is finished, you should reset the Enclosure to the default state using self.enclosure.reset() This will clear the screen and blink the Mark 1's eyes once.","title":"Resetting the display to the default state"},{"location":"mk1_utils/","text":"Mark1 Utils small library to interact with a Mycroft Mark1 faceplate via the messagebus gives you full control of the faceplate and eyes pixel by pixel Animate the eyes from ovos_mark1.eyes import Eyes from ovos_bus_client.utils import get_mycroft_bus bus = get_mycroft_bus(\"0.0.0.0\") eyes = Eyes(bus) eyes.hue_spin() Faceplate Icons from ovos_mark1.faceplate import BlackScreen class MusicIcon(BlackScreen): str_grid = \"\"\" XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX XXXXXXXXXXXXXX XXXXXXXXXXXXX XXXXXXXXXXXXXX XXXXXXXXXXXXX XXXXXXXXXXXXXX XXX XXXXXXXXXXXXX XXXXXXXXXXXXXX XXX XXXXXXXXXXXXX XXXXXXXXXXXXX XX XXXXXXXXXXXXX XXXXXXXXXXXX X XXXXXXXXXXXXX XXXXXXXXXXXXX XXX XXXXXXXXXXXXXX \"\"\" icon = MusicIcon() icon.print() # show in terminal icon.display() # show in mark1 Faceplate Animations # it's snowing ! class FallingDots(FacePlateAnimation): def __init__(self, n=10, bus=None): super().__init__(bus=bus) self._create = True assert 0 < n < 32 self.n = n @property def n_dots(self): n = 0 for y in range(self.height): for x in range(self.width): if self.grid[y][x]: n += 1 return n def animate(self): self.move_down() if self._create: if random.choice([True, False]): self._create = False x = random.randint(0, self.width - 1) self.grid[0][x] = 1 if self.n_dots < self.n: self._create = True Pre made animations from ovos_mark1.faceplate.animations import ParticleBox from ovos_bus_client.utils import get_mycroft_bus from time import sleep bus = get_mycroft_bus(\"0.0.0.0\") for faceplate in ParticleBox(bus=bus): faceplate.display(invert=False) sleep(0.5) from ovos_mark1.faceplate.cellular_automaton import Rule110 a = Rule110(bus=bus) for grid in a: grid.print() # animate in terminal grid.display(invert=False) sleep(0.5)","title":"Mark1 Utils"},{"location":"mk1_utils/#mark1-utils","text":"small library to interact with a Mycroft Mark1 faceplate via the messagebus gives you full control of the faceplate and eyes pixel by pixel","title":"Mark1 Utils"},{"location":"mk1_utils/#animate-the-eyes","text":"from ovos_mark1.eyes import Eyes from ovos_bus_client.utils import get_mycroft_bus bus = get_mycroft_bus(\"0.0.0.0\") eyes = Eyes(bus) eyes.hue_spin()","title":"Animate the eyes"},{"location":"mk1_utils/#faceplate-icons","text":"from ovos_mark1.faceplate import BlackScreen class MusicIcon(BlackScreen): str_grid = \"\"\" XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX XXXXXXXXXXXXXX XXXXXXXXXXXXX XXXXXXXXXXXXXX XXXXXXXXXXXXX XXXXXXXXXXXXXX XXX XXXXXXXXXXXXX XXXXXXXXXXXXXX XXX XXXXXXXXXXXXX XXXXXXXXXXXXX XX XXXXXXXXXXXXX XXXXXXXXXXXX X XXXXXXXXXXXXX XXXXXXXXXXXXX XXX XXXXXXXXXXXXXX \"\"\" icon = MusicIcon() icon.print() # show in terminal icon.display() # show in mark1","title":"Faceplate Icons"},{"location":"mk1_utils/#faceplate-animations","text":"# it's snowing ! class FallingDots(FacePlateAnimation): def __init__(self, n=10, bus=None): super().__init__(bus=bus) self._create = True assert 0 < n < 32 self.n = n @property def n_dots(self): n = 0 for y in range(self.height): for x in range(self.width): if self.grid[y][x]: n += 1 return n def animate(self): self.move_down() if self._create: if random.choice([True, False]): self._create = False x = random.randint(0, self.width - 1) self.grid[0][x] = 1 if self.n_dots < self.n: self._create = True Pre made animations from ovos_mark1.faceplate.animations import ParticleBox from ovos_bus_client.utils import get_mycroft_bus from time import sleep bus = get_mycroft_bus(\"0.0.0.0\") for faceplate in ParticleBox(bus=bus): faceplate.display(invert=False) sleep(0.5) from ovos_mark1.faceplate.cellular_automaton import Rule110 a = Rule110(bus=bus) for grid in a: grid.print() # animate in terminal grid.display(invert=False) sleep(0.5)","title":"Faceplate Animations"},{"location":"mycroft/","text":"ovos-core vs mycroft-core Speech Client Feature Mycroft OVOS Description Wake Word (listen) yes yes Only transcribe speech (STT) after a certain word is spoken Wake Up Word (sleep mode) yes yes When in sleep mode only listen for \"wake up\" (no STT) Hotword (bus event) no yes Emit bus events when a hotword is detected (no STT) Multiple Wake Words no yes Load multiple hotword engines/models simultaneously Fallback STT no yes fallback STT if the main one fails (eg, internet outage) Instant Listen no yes Do not pause between wake word detection and recording start Hybrid Listen no WIP Do not require wake word for follow up questions Continuous Listen no WIP Do not require wake word, always listen using VAD Recording mode no WIP Save audio instead of processing speech Wake Word Plugins yes yes Supports 3rd party integrations for hotword detection STT Plugins yes yes Supports 3rd party integrations for STT VAD plugins no * yes Supports 3rd party integrations for voice activity detection NOTES: HiveMind Voice Satellite uses ovos-core and supports the same features Pyaudio has a bug in python 3.10, you may need to use this fork (ovos-core and mk2 only) VAD is supported in mycroft mark2 branch, but is hardcoded for silero Sleep mode loop has been rewritten in ovos-core and is much more responsive than mycroft Mic handling logic has been ported from mk2 branch and is much more responsive than mycroft dev branch Instant / Hybrid / Continuous listen settings are experimental, good microphone and AEC are highly recommended (such as a mark2) in ovos-core this functionality has been refactored and moved to the new mycroft.listener module Audio Feature Mycroft OVOS Description MPRIS integration no yes Integrate with MPRIS protocol NOTES: OCP can be used with mycroft-core, but not mk2 OCP can be controlled via MPRIS, e.g. KDEConnect OCP can control MPRIS enabled players, e.g. spotify Skills Feature Mycroft OVOS Description Skill Plugins no yes skills can be packaged like standard python projects and installed via setup.py (eg. with pip or your package manager) User Resources no yes Users can override resource files, eg. customize dialogs for installed skills Skill permissions no WIP Users can limit converse and fallback functionality per skill and configure the order in which skills are executed Intent Plugins no WIP Supports 3rd party integrations for Intent Matching Hardware Feature Mycroft OVOS Description System Plugins no yes Support for 3rd party hardware (eg. mk2-plugin ) and OS level integrations (eg. wifi-setup ) NOTES: PHAL can be used with mycroft-core Misc Feature Mycroft OVOS Description Offline usage no yes Can be configured to work without internet connectivity MultiLingual no WIP Can be configured to work in multiple languages at the same time HiveMind support WIP WIP Supports HiveMind for a distributed/remote mycroft experience XDG compliance WIP yes All resources respect XDG standards and support multiple users Usage as a lib no yes Packaged as a library, supports derivative voice assistants NOTES: HiveMind is being developed against ovos-core, development under mycroft-core is stalled, see the hivemind wiki for details XDG support includes multiple skill directories, all skill data, all configuration files, and all cached files You can build your own assistant on top of ovos-core, multiple assistants can co-exist in the same machine and use their own configuration files , ovos-core is packaged like a regular python package and can be handled as a requirement by package managers examples projects: neon-core , hivemind-voice-sat","title":"ovos-core vs mycroft-core"},{"location":"mycroft/#ovos-core-vs-mycroft-core","text":"","title":"ovos-core vs mycroft-core"},{"location":"mycroft/#speech-client","text":"Feature Mycroft OVOS Description Wake Word (listen) yes yes Only transcribe speech (STT) after a certain word is spoken Wake Up Word (sleep mode) yes yes When in sleep mode only listen for \"wake up\" (no STT) Hotword (bus event) no yes Emit bus events when a hotword is detected (no STT) Multiple Wake Words no yes Load multiple hotword engines/models simultaneously Fallback STT no yes fallback STT if the main one fails (eg, internet outage) Instant Listen no yes Do not pause between wake word detection and recording start Hybrid Listen no WIP Do not require wake word for follow up questions Continuous Listen no WIP Do not require wake word, always listen using VAD Recording mode no WIP Save audio instead of processing speech Wake Word Plugins yes yes Supports 3rd party integrations for hotword detection STT Plugins yes yes Supports 3rd party integrations for STT VAD plugins no * yes Supports 3rd party integrations for voice activity detection NOTES: HiveMind Voice Satellite uses ovos-core and supports the same features Pyaudio has a bug in python 3.10, you may need to use this fork (ovos-core and mk2 only) VAD is supported in mycroft mark2 branch, but is hardcoded for silero Sleep mode loop has been rewritten in ovos-core and is much more responsive than mycroft Mic handling logic has been ported from mk2 branch and is much more responsive than mycroft dev branch Instant / Hybrid / Continuous listen settings are experimental, good microphone and AEC are highly recommended (such as a mark2) in ovos-core this functionality has been refactored and moved to the new mycroft.listener module","title":"Speech Client"},{"location":"mycroft/#audio","text":"Feature Mycroft OVOS Description MPRIS integration no yes Integrate with MPRIS protocol NOTES: OCP can be used with mycroft-core, but not mk2 OCP can be controlled via MPRIS, e.g. KDEConnect OCP can control MPRIS enabled players, e.g. spotify","title":"Audio"},{"location":"mycroft/#skills","text":"Feature Mycroft OVOS Description Skill Plugins no yes skills can be packaged like standard python projects and installed via setup.py (eg. with pip or your package manager) User Resources no yes Users can override resource files, eg. customize dialogs for installed skills Skill permissions no WIP Users can limit converse and fallback functionality per skill and configure the order in which skills are executed Intent Plugins no WIP Supports 3rd party integrations for Intent Matching","title":"Skills"},{"location":"mycroft/#hardware","text":"Feature Mycroft OVOS Description System Plugins no yes Support for 3rd party hardware (eg. mk2-plugin ) and OS level integrations (eg. wifi-setup ) NOTES: PHAL can be used with mycroft-core","title":"Hardware"},{"location":"mycroft/#misc","text":"Feature Mycroft OVOS Description Offline usage no yes Can be configured to work without internet connectivity MultiLingual no WIP Can be configured to work in multiple languages at the same time HiveMind support WIP WIP Supports HiveMind for a distributed/remote mycroft experience XDG compliance WIP yes All resources respect XDG standards and support multiple users Usage as a lib no yes Packaged as a library, supports derivative voice assistants NOTES: HiveMind is being developed against ovos-core, development under mycroft-core is stalled, see the hivemind wiki for details XDG support includes multiple skill directories, all skill data, all configuration files, and all cached files You can build your own assistant on top of ovos-core, multiple assistants can co-exist in the same machine and use their own configuration files , ovos-core is packaged like a regular python package and can be handled as a requirement by package managers examples projects: neon-core , hivemind-voice-sat","title":"Misc"},{"location":"nlp_plugins/","text":"NLP plugins Several NLP tasks are exposed as plugins, this allows to configure how to solve these tasks centrally NEW in ovos-core version 0.0.8 Keyword Extraction Extract keywords from utterances Plugin Description ovos-keyword-extractor-heuristic ovos-keyword-extractor-rake nltk data dependent (stopwords) Tokenization Split utterances into tokens Plugin Description ovos-tokenization-plugin-quebrafrases heuristic based tokenizer Sentence Segmentation Split utterances into sub-commands Plugin Description ovos-segmentation-plugin-quebrafrases heuristic based sentence segmentation Coreference Resolution Replace coreferences (pronouns) with their entities Plugin Description ovos-coref-solver-heuristic heuristic based coref solver ovos-classifiers-coref-solver models trained with ovos-classifiers Postag Plugin Description ovos-postag-plugin-nltk using nltk default postag ovos-classifiers-postag-plugin models trained with ovos-classifiers","title":"NLP Plugins"},{"location":"nlp_plugins/#nlp-plugins","text":"Several NLP tasks are exposed as plugins, this allows to configure how to solve these tasks centrally NEW in ovos-core version 0.0.8","title":"NLP plugins"},{"location":"nlp_plugins/#keyword-extraction","text":"Extract keywords from utterances Plugin Description ovos-keyword-extractor-heuristic ovos-keyword-extractor-rake nltk data dependent (stopwords)","title":"Keyword Extraction"},{"location":"nlp_plugins/#tokenization","text":"Split utterances into tokens Plugin Description ovos-tokenization-plugin-quebrafrases heuristic based tokenizer","title":"Tokenization"},{"location":"nlp_plugins/#sentence-segmentation","text":"Split utterances into sub-commands Plugin Description ovos-segmentation-plugin-quebrafrases heuristic based sentence segmentation","title":"Sentence Segmentation"},{"location":"nlp_plugins/#coreference-resolution","text":"Replace coreferences (pronouns) with their entities Plugin Description ovos-coref-solver-heuristic heuristic based coref solver ovos-classifiers-coref-solver models trained with ovos-classifiers","title":"Coreference Resolution"},{"location":"nlp_plugins/#postag","text":"Plugin Description ovos-postag-plugin-nltk using nltk default postag ovos-classifiers-postag-plugin models trained with ovos-classifiers","title":"Postag"},{"location":"ocp_plugins/","text":"OCP Plugins handle extracting playable streams and metadata, skills might require specific plugins and will be ignored if plugins are missing these plugins are used when a sei// is requested explicitly by a skill, or when a url pattern matches plugin descripton Stream Extractor Ids url pattern ovos-ocp-rss-plugin rss feeds rss// ovos-ocp-bandcamp-plugin bandcamp urls bandcamp// \"bandcamp.\" in url ovos-ocp-youtube-plugin youtube urls youtube// , ydl// , youtube.channel.live// \"youtube.com/\" in url or \"youtu.be/\" in url ovos-ocp-m3u-plugin .pls and .m3u formats m3u// , pls// \".pls\" in uri or \".m3u\" in uri ovos-ocp-news-plugin dedicated news websites news// any([uri.startswith(url) for url in URL_MAPPINGS]) Standalone Usage TODO Plugin Template TODO","title":"OCP Stream Extractors"},{"location":"ocp_plugins/#ocp-plugins","text":"handle extracting playable streams and metadata, skills might require specific plugins and will be ignored if plugins are missing these plugins are used when a sei// is requested explicitly by a skill, or when a url pattern matches plugin descripton Stream Extractor Ids url pattern ovos-ocp-rss-plugin rss feeds rss// ovos-ocp-bandcamp-plugin bandcamp urls bandcamp// \"bandcamp.\" in url ovos-ocp-youtube-plugin youtube urls youtube// , ydl// , youtube.channel.live// \"youtube.com/\" in url or \"youtu.be/\" in url ovos-ocp-m3u-plugin .pls and .m3u formats m3u// , pls// \".pls\" in uri or \".m3u\" in uri ovos-ocp-news-plugin dedicated news websites news// any([uri.startswith(url) for url in URL_MAPPINGS])","title":"OCP Plugins"},{"location":"ocp_plugins/#standalone-usage","text":"TODO","title":"Standalone Usage"},{"location":"ocp_plugins/#plugin-template","text":"TODO","title":"Plugin Template"},{"location":"osm/","text":"OVOS skills manager Install skills from any appstore! The mycroft-skills-manager alternative that is not vendor locked , this means you must use it responsibly! Do not install random skills, different appstores have different policies! Keep in mind any skill you install can modify mycroft-core at runtime , and very likely has root access if you are running on a raspberry pi Supported stores OVOS - this one is really a proof of concept for now, stay tuned! Mycroft Marketplace - the official mycroft skills store, all skills are reviewed by humans! Pling - the official plasma bigscreen skills store, skills are accepted by default and only removed if flagged as malicious Andlo's skill list - not a real appstore, this is a web scrapped automatically generated list of 900+ skills from all over github, there is no review at all, it will catch malicious skills Install pip install ovos-skills-manager Usage osm provides a few command line utilities, explained below Install Install a mycroft skill! Either pass a search query or a github url (.venv) user@hostname:~$ osm install --help Usage: osm install [OPTIONS] Options: --skill TEXT skill to install --branch TEXT select skill github branch to use --folder TEXT path where skill will be installed, default /opt/mycroft/skills --search search appstores, otherwise assume it's a github url --appstore [ovos|mycroft|pling|andlo|default|all] search a specific appstore, default search appstores enabled in config file --method [all|name|url|category|author|tag|description] match this metadata field when searching --fuzzy / --exact exact or fuzzy matching, default fuzzy --thresh INTEGER RANGE fuzzy matching threshold from 0 (everything is a match) to 100 (exact match), default 80 --no-ignore-case ignore upper/lower case, default ignore --help Show this message and exit. Enable Enable a new skills store (.venv) user@hostname:~$ osm enable --help Usage: osm enable [OPTIONS] Options: --appstore [ovos|mycroft|pling|andlo|all] enable a specific appstore --help Show this message and exit. Disable Disable a skills store (.venv) user@hostname:~$ osm disable --help Usage: osm disable [OPTIONS] Options: --appstore [ovos|mycroft|pling|andlo|all] disable a specific appstore --help Show this message and exit. Sync Sync skill list for a skills store Suggestion: set a cronjob for this (.venv) user@hostname:~$ osm sync --help Usage: osm sync [OPTIONS] Options: --appstore [ovos|mycroft|pling|andlo|default|all] sync a specific appstore, default syncs appstores enabled in config file --rebuild rebuild skill database, if not set only sync data for new skills --merge merge skill data, if not set replaces skill entries --github augment skill data from github, by default only saves data provided directly by the appstore --help Show this message and exit. Priority Change priority of a skills store, this will affect order of results and have impact in the OSM-skill (coming soon) (.venv) user@hostname:~$ osm priority --help Usage: osm priority [OPTIONS] Options: --appstore [ovos|mycroft|pling|andlo] change priority of a specific appstore --priority INTEGER RANGE appstore priority, from 0 (highest) to 100 (lowest) --help Show this message and exit. Print config print current configuration of osm, config file can be found at ~/.config/OpenVoiceOS/OVOS-SkillsManager.json (.venv) user@hostname:~$ osm print --help Usage: osm print [OPTIONS] Options: --appstore [ovos|mycroft|pling|andlo|all|default] print config of a specific appstore --help Show this message and exit. Search Search skills and print results, searching can be done according any number of criteria, this is useful for discovery (.venv) user@hostname:~$ osm search --help Usage: osm search [OPTIONS] Options: --query TEXT Search a skill with this query --method [all|name|url|category|author|tag|description] match this metadata field when searching --appstore [ovos|mycroft|pling|andlo|default|all] search a specific appstore, by default searches appstores enabled in config file --fuzzy / --exact exact or fuzzy matching --thresh INTEGER RANGE fuzzy matching threshold from 0 (everything is a match) to 100 (exact match) --no-ignore-case ignore upper/lower case --help Show this message and exit.","title":"OVOS skills manager"},{"location":"osm/#ovos-skills-manager","text":"Install skills from any appstore! The mycroft-skills-manager alternative that is not vendor locked , this means you must use it responsibly! Do not install random skills, different appstores have different policies! Keep in mind any skill you install can modify mycroft-core at runtime , and very likely has root access if you are running on a raspberry pi","title":"OVOS skills manager"},{"location":"osm/#supported-stores","text":"OVOS - this one is really a proof of concept for now, stay tuned! Mycroft Marketplace - the official mycroft skills store, all skills are reviewed by humans! Pling - the official plasma bigscreen skills store, skills are accepted by default and only removed if flagged as malicious Andlo's skill list - not a real appstore, this is a web scrapped automatically generated list of 900+ skills from all over github, there is no review at all, it will catch malicious skills","title":"Supported stores"},{"location":"osm/#install","text":"pip install ovos-skills-manager","title":"Install"},{"location":"osm/#usage","text":"osm provides a few command line utilities, explained below","title":"Usage"},{"location":"osm/#install_1","text":"Install a mycroft skill! Either pass a search query or a github url (.venv) user@hostname:~$ osm install --help Usage: osm install [OPTIONS] Options: --skill TEXT skill to install --branch TEXT select skill github branch to use --folder TEXT path where skill will be installed, default /opt/mycroft/skills --search search appstores, otherwise assume it's a github url --appstore [ovos|mycroft|pling|andlo|default|all] search a specific appstore, default search appstores enabled in config file --method [all|name|url|category|author|tag|description] match this metadata field when searching --fuzzy / --exact exact or fuzzy matching, default fuzzy --thresh INTEGER RANGE fuzzy matching threshold from 0 (everything is a match) to 100 (exact match), default 80 --no-ignore-case ignore upper/lower case, default ignore --help Show this message and exit.","title":"Install"},{"location":"osm/#enable","text":"Enable a new skills store (.venv) user@hostname:~$ osm enable --help Usage: osm enable [OPTIONS] Options: --appstore [ovos|mycroft|pling|andlo|all] enable a specific appstore --help Show this message and exit.","title":"Enable"},{"location":"osm/#disable","text":"Disable a skills store (.venv) user@hostname:~$ osm disable --help Usage: osm disable [OPTIONS] Options: --appstore [ovos|mycroft|pling|andlo|all] disable a specific appstore --help Show this message and exit.","title":"Disable"},{"location":"osm/#sync","text":"Sync skill list for a skills store Suggestion: set a cronjob for this (.venv) user@hostname:~$ osm sync --help Usage: osm sync [OPTIONS] Options: --appstore [ovos|mycroft|pling|andlo|default|all] sync a specific appstore, default syncs appstores enabled in config file --rebuild rebuild skill database, if not set only sync data for new skills --merge merge skill data, if not set replaces skill entries --github augment skill data from github, by default only saves data provided directly by the appstore --help Show this message and exit.","title":"Sync"},{"location":"osm/#priority","text":"Change priority of a skills store, this will affect order of results and have impact in the OSM-skill (coming soon) (.venv) user@hostname:~$ osm priority --help Usage: osm priority [OPTIONS] Options: --appstore [ovos|mycroft|pling|andlo] change priority of a specific appstore --priority INTEGER RANGE appstore priority, from 0 (highest) to 100 (lowest) --help Show this message and exit.","title":"Priority"},{"location":"osm/#print-config","text":"print current configuration of osm, config file can be found at ~/.config/OpenVoiceOS/OVOS-SkillsManager.json (.venv) user@hostname:~$ osm print --help Usage: osm print [OPTIONS] Options: --appstore [ovos|mycroft|pling|andlo|all|default] print config of a specific appstore --help Show this message and exit.","title":"Print config"},{"location":"osm/#search","text":"Search skills and print results, searching can be done according any number of criteria, this is useful for discovery (.venv) user@hostname:~$ osm search --help Usage: osm search [OPTIONS] Options: --query TEXT Search a skill with this query --method [all|name|url|category|author|tag|description] match this metadata field when searching --appstore [ovos|mycroft|pling|andlo|default|all] search a specific appstore, by default searches appstores enabled in config file --fuzzy / --exact exact or fuzzy matching --thresh INTEGER RANGE fuzzy matching threshold from 0 (everything is a match) to 100 (exact match) --no-ignore-case ignore upper/lower case --help Show this message and exit.","title":"Search"},{"location":"ovos_bigscreen/","text":"Plasma Bigscreen - OVOS Edition EXPERIMENTAL - experimental repository WARNING - Not actively maintained, this fork is a temporary measure since Plasma Bigscreen dropped support for OVOS and moved to QT6 Introduction A big launcher giving you easy access to any installed apps and skills. Controllable via voice or TV remote. This project is using various open-source components like Plasma Bigscreen, OpenVoiceOS and libcec. This is a fork from https://invent.kde.org/plasma/plasma-bigscreen/ Changes: moves from Mycroft to OVOS \"mycroft\" is no longer optional and it's integration is enabled by default Remove MycroftSkillInstaller (not OVOS compliant) Remove \"Recent\" section Remove generic \"Applications\" section Add \"Media\" section Add \"Network\" section Add \"Graphics\" section Voice Control Bigscreen supports OpenVoiceOS, a free and open-source voice assistant that can be run completely decentralized on your own server. Download new apps (aka skills) for your Bigscreen or add your own ones for others to enjoy. Remote control your TV via CEC CEC (Consumer Electronics Control) is a standard to control devices over HDMI. Use your normal TV remote control, or a RC with built-in microphone for voice control and optional mouse simulation. Application Launcher Bigscreen replaces your DE, to stop an application from showing up you can edit /etc/xdg/applications-blacklistrc Adding new applications only requires a .desktop file see plasma-bigscreen/bigscreen-image-settings for more settings you might want to tweak in a Bigscreen image Installing from source mkdir build cd build cmake .. -DCMAKE_INSTALL_PREFIX=/usr -DCMAKE_BUILD_TYPE=Release -DKDE_INSTALL_LIBDIR=lib -DKDE_INSTALL_USE_QT_SYS_PATHS=ON -DCMAKE_CXX_COMPILER=clazy make sudo make install Running To start the Bigscreen homescreen in a window, run: QT_QPA_PLATFORM=wayland dbus-run-session kwin_wayland \"plasmashell -p org.kde.plasma.mycroft.bigscreen\" you can also select plasma-bigscreen-x11 on your login screen as DE Related repositories Image Settings for Bigscreen https://invent.kde.org/plasma-bigscreen/bigscreen-image-settings Plasma Remote Controllers https://invent.kde.org/plasma-bigscreen/plasma-remotecontrollers ovos-gui-app - https://github.com/OpenVoiceOS/mycroft-gui-qt5 bigscreen gui extension https://github.com/OpenVoiceOS/ovos-gui-plugin-bigscreen","title":"Plasma Bigscreen"},{"location":"ovos_bigscreen/#plasma-bigscreen-ovos-edition","text":"EXPERIMENTAL - experimental repository WARNING - Not actively maintained, this fork is a temporary measure since Plasma Bigscreen dropped support for OVOS and moved to QT6","title":"Plasma Bigscreen - OVOS Edition"},{"location":"ovos_bigscreen/#introduction","text":"A big launcher giving you easy access to any installed apps and skills. Controllable via voice or TV remote. This project is using various open-source components like Plasma Bigscreen, OpenVoiceOS and libcec. This is a fork from https://invent.kde.org/plasma/plasma-bigscreen/ Changes: moves from Mycroft to OVOS \"mycroft\" is no longer optional and it's integration is enabled by default Remove MycroftSkillInstaller (not OVOS compliant) Remove \"Recent\" section Remove generic \"Applications\" section Add \"Media\" section Add \"Network\" section Add \"Graphics\" section","title":"Introduction"},{"location":"ovos_bigscreen/#voice-control","text":"Bigscreen supports OpenVoiceOS, a free and open-source voice assistant that can be run completely decentralized on your own server. Download new apps (aka skills) for your Bigscreen or add your own ones for others to enjoy.","title":"Voice Control"},{"location":"ovos_bigscreen/#remote-control-your-tv-via-cec","text":"CEC (Consumer Electronics Control) is a standard to control devices over HDMI. Use your normal TV remote control, or a RC with built-in microphone for voice control and optional mouse simulation.","title":"Remote control your TV via CEC"},{"location":"ovos_bigscreen/#application-launcher","text":"Bigscreen replaces your DE, to stop an application from showing up you can edit /etc/xdg/applications-blacklistrc Adding new applications only requires a .desktop file see plasma-bigscreen/bigscreen-image-settings for more settings you might want to tweak in a Bigscreen image","title":"Application Launcher"},{"location":"ovos_bigscreen/#installing-from-source","text":"mkdir build cd build cmake .. -DCMAKE_INSTALL_PREFIX=/usr -DCMAKE_BUILD_TYPE=Release -DKDE_INSTALL_LIBDIR=lib -DKDE_INSTALL_USE_QT_SYS_PATHS=ON -DCMAKE_CXX_COMPILER=clazy make sudo make install","title":"Installing from source"},{"location":"ovos_bigscreen/#running","text":"To start the Bigscreen homescreen in a window, run: QT_QPA_PLATFORM=wayland dbus-run-session kwin_wayland \"plasmashell -p org.kde.plasma.mycroft.bigscreen\" you can also select plasma-bigscreen-x11 on your login screen as DE","title":"Running"},{"location":"ovos_bigscreen/#related-repositories","text":"Image Settings for Bigscreen https://invent.kde.org/plasma-bigscreen/bigscreen-image-settings Plasma Remote Controllers https://invent.kde.org/plasma-bigscreen/plasma-remotecontrollers ovos-gui-app - https://github.com/OpenVoiceOS/mycroft-gui-qt5 bigscreen gui extension https://github.com/OpenVoiceOS/ovos-gui-plugin-bigscreen","title":"Related repositories"},{"location":"padacioso/","text":"Padacioso A lightweight, dead-simple intent parser Built on top of simplematch , inspired by Padaos Example from padacioso import IntentContainer container = IntentContainer() ## samples container.add_intent('hello', ['hello', 'hi', 'how are you', \"what's up\"]) ## \"optionally\" syntax container.add_intent('hello world', [\"hello [world]\"]) ## \"one_of\" syntax container.add_intent('greeting', [\"(hi|hey|hello)\"]) ## entity extraction container.add_intent('buy', [ 'buy {item}', 'purchase {item}', 'get {item}', 'get {item} for me' ]) container.add_intent('search', [ 'search for {query} on {engine}', 'using {engine} (search|look) for {query}', 'find {query} (with|using) {engine}' ]) container.add_entity('engine', ['abc', 'xyz']) container.calc_intent('find cats using xyz') # {'conf': 1.0, 'name': 'search', 'entities': {'query': 'cats', 'engine': 'xyz'}} ## wildcards syntax container.add_intent('say', [\"say *\"]) container.calc_intent('say something, whatever') # {'conf': 0.85, 'entities': {}, 'name': 'test'} ## typed entities syntax container.add_intent('pick_number', ['* number {number:int}']) container.calc_intent('i want number 3') # {'conf': 0.85, 'entities': {'number': 3}, 'name': 'pick_number'})","title":"Padacioso"},{"location":"padacioso/#padacioso","text":"A lightweight, dead-simple intent parser Built on top of simplematch , inspired by Padaos","title":"Padacioso"},{"location":"padacioso/#example","text":"from padacioso import IntentContainer container = IntentContainer() ## samples container.add_intent('hello', ['hello', 'hi', 'how are you', \"what's up\"]) ## \"optionally\" syntax container.add_intent('hello world', [\"hello [world]\"]) ## \"one_of\" syntax container.add_intent('greeting', [\"(hi|hey|hello)\"]) ## entity extraction container.add_intent('buy', [ 'buy {item}', 'purchase {item}', 'get {item}', 'get {item} for me' ]) container.add_intent('search', [ 'search for {query} on {engine}', 'using {engine} (search|look) for {query}', 'find {query} (with|using) {engine}' ]) container.add_entity('engine', ['abc', 'xyz']) container.calc_intent('find cats using xyz') # {'conf': 1.0, 'name': 'search', 'entities': {'query': 'cats', 'engine': 'xyz'}} ## wildcards syntax container.add_intent('say', [\"say *\"]) container.calc_intent('say something, whatever') # {'conf': 0.85, 'entities': {}, 'name': 'test'} ## typed entities syntax container.add_intent('pick_number', ['* number {number:int}']) container.calc_intent('i want number 3') # {'conf': 0.85, 'entities': {'number': 3}, 'name': 'pick_number'})","title":"Example"},{"location":"persona_server/","text":"Persona Server EXPERIMENTAL - NEW ovos-core version 0.0.8 The Persona initiative is a work in progress, it can be integrated via persona-server and ChatGPT skill as a developer preview Personas give personality to OVOS, they can be anything from classic chatbots to state of the art language models! Persona server follows the standard OpenAI API, also allowing you to connect personas to existing ChatGPT applications Public Servers Member Solvers / LLM model Native Language URL mistral-7b-openorca.Q4_K_M.gguf english https://llama.smartgic.io Companion plugin Use with ovos-solver-plugin-openai-persona Personas A Persona is defined by a combination of question solver plugins , the solvers are tried sequentially until one succeeds in answering the question similarly to fallback skills some repos and skills also provide solvers, such as ovos-classifiers (wordnet), skill-ddg, skill-wikipedia and skill-wolfie { \"name\": \"OldSchoolBot\", \"solvers\": [ \"ovos-solver-wikipedia-plugin\", \"ovos-solver-ddg-plugin\", \"ovos-solver-plugin-wolfram-alpha\", \"ovos-solver-wordnet-plugin\", \"ovos-solver-rivescript-plugin\", \"ovos-solver-failure-plugin\" ], \"ovos-solver-plugin-wolfram-alpha\": {\"appid\": \"Y7353-9HQAAL8KKA\"} } this persona would search ddg api / wikipedia for \"what is\"/\"tell me about\" questions, falling back to wordnet when offline for dictionary look up, and finally rivescript for general chitchat, we also add the failure solver to be sure the persona always says something wolfram alpha illustrates how to pass solver configs, it has a requirement for an API key search/knowledge base solvers can be used together with chatbot solvers to ensure factual answers and act as a tool/internet access layer, for classic chatbots the example above serves as a rudimentary implementation of tool use Chatbots are a nice toy and for chit-chat work well as a fallback if internet is not available, but you most likely want to connect a LLM to persona, be it remote or local LLMs A basic example connecting to ChatGPT or compatible LLM (or a remote persona!) \"name\": \"ChatGPT\", \"solvers\": [ \"ovos-solver-openai-persona-plugin\" ], \"ovos-solver-openai-persona-plugin\": { \"api_url\": \"<OpenAI_or_compatible_server_url>\", \"key\": \"<your_OpenAI_key>\", \"persona\": \"helpful, creative, clever, and very friendly.\" } NOTE : In the config above \"persona\" sets the system prompt, this is specific to the solver plugin Wrapping your LLM in a solver plugin provides you Bidirectional translation , even if your LLM is language specific you are able to use it in any language RAG and tool use for LLMs is better implemented as part of a dedicated solver plugin, LLM solvers may also use other solvers internally as agents/tools for LLMs, Running $ ovos-persona-server --persona rivescript_bot.json OpenAI API compatible OpenAI compatible API, can be used with hundreds of projects that support it import openai openai.api_key = \"\" openai.api_base = \"http://localhost:8337\" # NOTE - most solvers don't support a chat history, # only last message in messages list is considered chat_completion = openai.ChatCompletion.create( model=\"\", # individual personas might support this, passed under context messages=[{\"role\": \"user\", \"content\": \"tell me a joke\"}], stream=False, ) if isinstance(chat_completion, dict): # not stream print(chat_completion.choices[0].message.content) else: # stream for token in chat_completion: content = token[\"choices\"][0][\"delta\"].get(\"content\") if content != None: print(content, end=\"\", flush=True) Compatible Projects The OpenAI API ended up becoming a sort of standard, ovos-solver-plugin-openai-persona can not only connect to ChatGPT, but is also compatible with public servers for several projects oobabooga/text-generation-webui llama.cpp LocalAI Xinference RayLLM OpenLLM FastChat gatogpt litellm nitro modelz-llm llama-api tabbyAPI basaran ... and many more","title":"Persona Server"},{"location":"persona_server/#persona-server","text":"EXPERIMENTAL - NEW ovos-core version 0.0.8 The Persona initiative is a work in progress, it can be integrated via persona-server and ChatGPT skill as a developer preview Personas give personality to OVOS, they can be anything from classic chatbots to state of the art language models! Persona server follows the standard OpenAI API, also allowing you to connect personas to existing ChatGPT applications","title":"Persona Server"},{"location":"persona_server/#public-servers","text":"Member Solvers / LLM model Native Language URL mistral-7b-openorca.Q4_K_M.gguf english https://llama.smartgic.io","title":"Public Servers"},{"location":"persona_server/#companion-plugin","text":"Use with ovos-solver-plugin-openai-persona","title":"Companion plugin"},{"location":"persona_server/#personas","text":"A Persona is defined by a combination of question solver plugins , the solvers are tried sequentially until one succeeds in answering the question similarly to fallback skills some repos and skills also provide solvers, such as ovos-classifiers (wordnet), skill-ddg, skill-wikipedia and skill-wolfie { \"name\": \"OldSchoolBot\", \"solvers\": [ \"ovos-solver-wikipedia-plugin\", \"ovos-solver-ddg-plugin\", \"ovos-solver-plugin-wolfram-alpha\", \"ovos-solver-wordnet-plugin\", \"ovos-solver-rivescript-plugin\", \"ovos-solver-failure-plugin\" ], \"ovos-solver-plugin-wolfram-alpha\": {\"appid\": \"Y7353-9HQAAL8KKA\"} } this persona would search ddg api / wikipedia for \"what is\"/\"tell me about\" questions, falling back to wordnet when offline for dictionary look up, and finally rivescript for general chitchat, we also add the failure solver to be sure the persona always says something wolfram alpha illustrates how to pass solver configs, it has a requirement for an API key search/knowledge base solvers can be used together with chatbot solvers to ensure factual answers and act as a tool/internet access layer, for classic chatbots the example above serves as a rudimentary implementation of tool use Chatbots are a nice toy and for chit-chat work well as a fallback if internet is not available, but you most likely want to connect a LLM to persona, be it remote or local","title":"Personas"},{"location":"persona_server/#llms","text":"A basic example connecting to ChatGPT or compatible LLM (or a remote persona!) \"name\": \"ChatGPT\", \"solvers\": [ \"ovos-solver-openai-persona-plugin\" ], \"ovos-solver-openai-persona-plugin\": { \"api_url\": \"<OpenAI_or_compatible_server_url>\", \"key\": \"<your_OpenAI_key>\", \"persona\": \"helpful, creative, clever, and very friendly.\" } NOTE : In the config above \"persona\" sets the system prompt, this is specific to the solver plugin Wrapping your LLM in a solver plugin provides you Bidirectional translation , even if your LLM is language specific you are able to use it in any language RAG and tool use for LLMs is better implemented as part of a dedicated solver plugin, LLM solvers may also use other solvers internally as agents/tools for LLMs,","title":"LLMs"},{"location":"persona_server/#running","text":"$ ovos-persona-server --persona rivescript_bot.json","title":"Running"},{"location":"persona_server/#openai-api-compatible","text":"OpenAI compatible API, can be used with hundreds of projects that support it import openai openai.api_key = \"\" openai.api_base = \"http://localhost:8337\" # NOTE - most solvers don't support a chat history, # only last message in messages list is considered chat_completion = openai.ChatCompletion.create( model=\"\", # individual personas might support this, passed under context messages=[{\"role\": \"user\", \"content\": \"tell me a joke\"}], stream=False, ) if isinstance(chat_completion, dict): # not stream print(chat_completion.choices[0].message.content) else: # stream for token in chat_completion: content = token[\"choices\"][0][\"delta\"].get(\"content\") if content != None: print(content, end=\"\", flush=True)","title":"OpenAI API compatible"},{"location":"persona_server/#compatible-projects","text":"The OpenAI API ended up becoming a sort of standard, ovos-solver-plugin-openai-persona can not only connect to ChatGPT, but is also compatible with public servers for several projects oobabooga/text-generation-webui llama.cpp LocalAI Xinference RayLLM OpenLLM FastChat gatogpt litellm nitro modelz-llm llama-api tabbyAPI basaran ... and many more","title":"Compatible Projects"},{"location":"personal_backend/","text":"OVOS Personal Backend Personal mycroft backend alternative to mycroft.home, written in flask This repo is an alternative to the backend meant for personal usage, this allows you to run without mycroft servers there are no user accounts This is NOT meant to provision third party devices, but rather to run on the mycroft devices directly or on a private network NOTE: There is no website for pairing, devices will just activate themselves and work Public Servers Member Service Needs Auth URL Personal Backend No https://ovospublicbackend.ziggyai.online WARNING : public servers are meant for demo purposes, you are strongly encouraged to self host a backend instead! Usage from pip pip install ovos-local-backend start backend $ ovos-local-backend -h usage: ovos-local-backend [-h] [--flask-port FLASK_PORT] [--flask-host FLASK_HOST] optional arguments: -h, --help show this help message and exit --flask-port FLASK_PORT Mock backend port number --flask-host FLASK_HOST Mock backend host Classic Mycroft devices With selene gone, personal backend is the only option to keep the lights on for classic mycroft devices There are 2 main intended ways to run local backend with mycroft on same device as mycroft-core, tricking it to run without mycroft servers on a private network, to manage all your devices locally NOTE: you can not fully run mycroft-core offline, it refuses to launch without internet connection, you can only replace the calls to use this backend instead of mycroft.home We recommend you use ovos-core instead update your mycroft config to use this backend, delete identity2.json and restart mycroft { \"server\": { \"url\": \"http://0.0.0.0:6712\", \"version\": \"v1\", \"update\": true, \"metrics\": true }, \"listener\": { \"wake_word_upload\": { \"url\": \"http://0.0.0.0:6712/precise/upload\" } } } Backend Manager a simple UI for ovos-personal-backend , utility to manage all your devices pip install ovos-backend-manager ovos-backend-manager will be available in the command line after installing Docker There is also a docker container you can use docker run -p 8086:6712 -d --restart always --name local_backend ghcr.io/openvoiceos/local-backend:dev a docker-compose.yml could look like this version: '3.6' services: # ... ovosbackend: container_name: ovos_backend image: ghcr.io/openvoiceos/local-backend:dev # or build from local source (relative to docker-compose.yml) # build: ../ovos/ovos-personal-backend/. restart: unless-stopped ports: - \"6712:6712\" # default port backend API - \"36535:36535\" # default port backend-manager volumes: # <host>:<guest>:<SELinux flag> - ./ovos/backend/config:/root/.config/json_database:z # shared config directory - ./ovos/backend/data:/root/.local/share/ovos_backend:Z # shared data directory # set `data_path` to `/root/.local/share/ovos_backend` about selinux flags (omit if you don't deal with selinux) How it works Configuration configure backend by editing/creating ~/.config/ovos_local_backend/ovos_backend.json { \"stt\": { \"module\": \"ovos-stt-plugin-server\", \"ovos-stt-plugin-server\": { \"url\": \"https://stt.openvoiceos.com/stt\" } }, \"backend_port\": 6712, \"geolocate\": true, \"override_location\": false, \"api_version\": \"v1\", \"data_path\": \"~\", \"record_utterances\": false, \"record_wakewords\": false, \"wolfram_key\": \"$KEY\", \"owm_key\": \"$KEY\", \"lang\": \"en-us\", \"date_format\": \"DMY\", \"system_unit\": \"metric\", \"time_format\": \"full\", \"default_location\": { \"city\": { \"...\": \"...\" }, \"coordinate\": { \"...\": \"...\" }, \"timezone\": { \"...\": \"...\" } } } stt config follows the same format of mycroft.conf and uses ovos-plugin-manager set wolfram alpha key for wolfram alpha proxy expected by official mycroft skill set open weather map key for weather proxy expected by official mycroft skill if record_wakewords is set, recordings can be found at DATA_PATH/wakewords if record_utterances is set, recordings can be found at DATA_PATH/utterances Device Settings Each paired device has a few settings that control behaviour backend side name - default \"Device-{uuid}\" , friendly device name for display opt_in - default True , flag to control if metrics and speech from this device will be saved device_location - default \"unknown\" , friendly name for indoor location email - default from backend config, email to send notifications to isolated_skills - default False , flag to control if skill settings are shared across devices (ovos only) In selene this info would be populated during pairing process, in local backend it needs to be updated manually you can change these settings per device via the admin api you can also change these settings per device by manually editing paired devices database Location Device location can be updated via the backend, mycroft-core will request this info on its own from time to time default values comes from the local backend config file { \"geolocate\": true, \"override_location\": false, \"default_location\": { \"city\": { \"...\": \"...\" }, \"coordinate\": { \"...\": \"...\" }, \"timezone\": { \"...\": \"...\" } } } if override location is True, then location will be set to configured default value if geolocate is True then location will be set from your ip address you can set a default location per device via the admin api you can also set a default location per device by manually editing paired devices database Device Preferences Some settings can be updated via the backend, mycroft-core will request this info on its own from time to time default values comes from the local backend config file { \"lang\": \"en-us\", \"date_format\": \"DMY\", \"system_unit\": \"metric\", \"time_format\": \"full\" } these settings are also used for wolfram alpha / weather default values you can set these values per device via the admin api you can also set these values per device by manually editing paired devices database Skill settings in selene all device share skill settings, with local backend you can control this per device via isolated_skills flag \"old selene\" supported a single endpoint for both skill settings and settings meta, this allowed devices both to download and upload settings \"new selene\" split this into two endpoints, settingsMeta (upload only) and settings (download only), this disabled two-way sync across devices you can set isolated_skills per device via the admin api you can also set isolated_skills per device by manually editing paired devices database both endpoints are available, but mycroft-core by default will use the new endpoints and does not support two-way sync you can edit settings by using the \"old selene\" endpoint you can also edit settings by manually editing settings database Email Mycroft skills can request the backend to send an email to the account used for pairing the device Email will be sent to a pre-defined recipient email since there are no user accounts you can set a recipient email per device via the admin api you can set a recipient email per device by manually editing paired devices database with the local backend you need to configure your own SMTP server and recipient email, add the following section to your .conf { \"email\": { \"smtp\": { \"username\": \"sender@gmail.com\", \"password\": \"123456\", \"host\": \"\", \"port\": 465 }, \"recipient\": \"receiver@gmail.com\" } } If using gmail you will need to enable less secure apps Project Timeline Jan 2018 - initial release of reverse engineered mycroft backend - by JarbasAI July 2018 - Personal backend added to Mycroft Roadmap October 2018 - Community involved in discussion Jan 2019 - JarbasAI implementation adopted by Mycroft NOTE: this should have been a fork or repository transferred, but was a bare clone Original repository was archived October 2019 - Official mycroft backend open sourced under a viral license Jun 2020 - original project repurposed to be a mock backend instead of a full alternative, skill-mock-backend released Jan 2021 - mock-backend adopted by OpenVoiceOS, original repo unarchived and ownership transferred","title":"Personal Backend"},{"location":"personal_backend/#ovos-personal-backend","text":"Personal mycroft backend alternative to mycroft.home, written in flask This repo is an alternative to the backend meant for personal usage, this allows you to run without mycroft servers there are no user accounts This is NOT meant to provision third party devices, but rather to run on the mycroft devices directly or on a private network NOTE: There is no website for pairing, devices will just activate themselves and work","title":"OVOS Personal Backend"},{"location":"personal_backend/#public-servers","text":"Member Service Needs Auth URL Personal Backend No https://ovospublicbackend.ziggyai.online WARNING : public servers are meant for demo purposes, you are strongly encouraged to self host a backend instead!","title":"Public Servers"},{"location":"personal_backend/#usage","text":"from pip pip install ovos-local-backend start backend $ ovos-local-backend -h usage: ovos-local-backend [-h] [--flask-port FLASK_PORT] [--flask-host FLASK_HOST] optional arguments: -h, --help show this help message and exit --flask-port FLASK_PORT Mock backend port number --flask-host FLASK_HOST Mock backend host","title":"Usage"},{"location":"personal_backend/#classic-mycroft-devices","text":"With selene gone, personal backend is the only option to keep the lights on for classic mycroft devices There are 2 main intended ways to run local backend with mycroft on same device as mycroft-core, tricking it to run without mycroft servers on a private network, to manage all your devices locally NOTE: you can not fully run mycroft-core offline, it refuses to launch without internet connection, you can only replace the calls to use this backend instead of mycroft.home We recommend you use ovos-core instead update your mycroft config to use this backend, delete identity2.json and restart mycroft { \"server\": { \"url\": \"http://0.0.0.0:6712\", \"version\": \"v1\", \"update\": true, \"metrics\": true }, \"listener\": { \"wake_word_upload\": { \"url\": \"http://0.0.0.0:6712/precise/upload\" } } }","title":"Classic Mycroft devices"},{"location":"personal_backend/#backend-manager","text":"a simple UI for ovos-personal-backend , utility to manage all your devices pip install ovos-backend-manager ovos-backend-manager will be available in the command line after installing","title":"Backend Manager"},{"location":"personal_backend/#docker","text":"There is also a docker container you can use docker run -p 8086:6712 -d --restart always --name local_backend ghcr.io/openvoiceos/local-backend:dev a docker-compose.yml could look like this version: '3.6' services: # ... ovosbackend: container_name: ovos_backend image: ghcr.io/openvoiceos/local-backend:dev # or build from local source (relative to docker-compose.yml) # build: ../ovos/ovos-personal-backend/. restart: unless-stopped ports: - \"6712:6712\" # default port backend API - \"36535:36535\" # default port backend-manager volumes: # <host>:<guest>:<SELinux flag> - ./ovos/backend/config:/root/.config/json_database:z # shared config directory - ./ovos/backend/data:/root/.local/share/ovos_backend:Z # shared data directory # set `data_path` to `/root/.local/share/ovos_backend` about selinux flags (omit if you don't deal with selinux)","title":"Docker"},{"location":"personal_backend/#how-it-works","text":"","title":"How it works"},{"location":"personal_backend/#configuration","text":"configure backend by editing/creating ~/.config/ovos_local_backend/ovos_backend.json { \"stt\": { \"module\": \"ovos-stt-plugin-server\", \"ovos-stt-plugin-server\": { \"url\": \"https://stt.openvoiceos.com/stt\" } }, \"backend_port\": 6712, \"geolocate\": true, \"override_location\": false, \"api_version\": \"v1\", \"data_path\": \"~\", \"record_utterances\": false, \"record_wakewords\": false, \"wolfram_key\": \"$KEY\", \"owm_key\": \"$KEY\", \"lang\": \"en-us\", \"date_format\": \"DMY\", \"system_unit\": \"metric\", \"time_format\": \"full\", \"default_location\": { \"city\": { \"...\": \"...\" }, \"coordinate\": { \"...\": \"...\" }, \"timezone\": { \"...\": \"...\" } } } stt config follows the same format of mycroft.conf and uses ovos-plugin-manager set wolfram alpha key for wolfram alpha proxy expected by official mycroft skill set open weather map key for weather proxy expected by official mycroft skill if record_wakewords is set, recordings can be found at DATA_PATH/wakewords if record_utterances is set, recordings can be found at DATA_PATH/utterances","title":"Configuration"},{"location":"personal_backend/#device-settings","text":"Each paired device has a few settings that control behaviour backend side name - default \"Device-{uuid}\" , friendly device name for display opt_in - default True , flag to control if metrics and speech from this device will be saved device_location - default \"unknown\" , friendly name for indoor location email - default from backend config, email to send notifications to isolated_skills - default False , flag to control if skill settings are shared across devices (ovos only) In selene this info would be populated during pairing process, in local backend it needs to be updated manually you can change these settings per device via the admin api you can also change these settings per device by manually editing paired devices database","title":"Device Settings"},{"location":"personal_backend/#location","text":"Device location can be updated via the backend, mycroft-core will request this info on its own from time to time default values comes from the local backend config file { \"geolocate\": true, \"override_location\": false, \"default_location\": { \"city\": { \"...\": \"...\" }, \"coordinate\": { \"...\": \"...\" }, \"timezone\": { \"...\": \"...\" } } } if override location is True, then location will be set to configured default value if geolocate is True then location will be set from your ip address you can set a default location per device via the admin api you can also set a default location per device by manually editing paired devices database","title":"Location"},{"location":"personal_backend/#device-preferences","text":"Some settings can be updated via the backend, mycroft-core will request this info on its own from time to time default values comes from the local backend config file { \"lang\": \"en-us\", \"date_format\": \"DMY\", \"system_unit\": \"metric\", \"time_format\": \"full\" } these settings are also used for wolfram alpha / weather default values you can set these values per device via the admin api you can also set these values per device by manually editing paired devices database","title":"Device Preferences"},{"location":"personal_backend/#skill-settings","text":"in selene all device share skill settings, with local backend you can control this per device via isolated_skills flag \"old selene\" supported a single endpoint for both skill settings and settings meta, this allowed devices both to download and upload settings \"new selene\" split this into two endpoints, settingsMeta (upload only) and settings (download only), this disabled two-way sync across devices you can set isolated_skills per device via the admin api you can also set isolated_skills per device by manually editing paired devices database both endpoints are available, but mycroft-core by default will use the new endpoints and does not support two-way sync you can edit settings by using the \"old selene\" endpoint you can also edit settings by manually editing settings database","title":"Skill settings"},{"location":"personal_backend/#email","text":"Mycroft skills can request the backend to send an email to the account used for pairing the device Email will be sent to a pre-defined recipient email since there are no user accounts you can set a recipient email per device via the admin api you can set a recipient email per device by manually editing paired devices database with the local backend you need to configure your own SMTP server and recipient email, add the following section to your .conf { \"email\": { \"smtp\": { \"username\": \"sender@gmail.com\", \"password\": \"123456\", \"host\": \"\", \"port\": 465 }, \"recipient\": \"receiver@gmail.com\" } } If using gmail you will need to enable less secure apps","title":"Email"},{"location":"personal_backend/#project-timeline","text":"Jan 2018 - initial release of reverse engineered mycroft backend - by JarbasAI July 2018 - Personal backend added to Mycroft Roadmap October 2018 - Community involved in discussion Jan 2019 - JarbasAI implementation adopted by Mycroft NOTE: this should have been a fork or repository transferred, but was a bare clone Original repository was archived October 2019 - Official mycroft backend open sourced under a viral license Jun 2020 - original project repurposed to be a mock backend instead of a full alternative, skill-mock-backend released Jan 2021 - mock-backend adopted by OpenVoiceOS, original repo unarchived and ownership transferred","title":"Project Timeline"},{"location":"playing_music/","text":"Playing music For playing music (and video as discussed within the next chapter), OpenVoiceOS uses OCP (OpenVoiceOS Common Play) and is basically a full fledge multimedia player on its own designed around open standards like MPRIS and with the vision of being fully integrated within the OpenVoiceOS software stack. Skills designed for OCP provide search results for OCP (think about them as media providers/catalogs/scrapers), OCP will play the best search result for you. OpenVoiceOS comes with a few OCP skills pre-installed, however more can be installed just like any other OVOS skill. You can find more OCP skills in the awesome-ocp-skills list Youtube Music A voiceassistant with smartspeaker functionality should be able to play music straight out of the box. For that reason the buildroot edition of OpenVoiceOS comes with the Youtube Music OCP Skill pre-installed. Just ask it to play something will start playback from Youtube assuming the asked sonmg is present on Youtube ofcourse. Hey Mycroft, play disturbed sound of silence This should just start playing utilizing OCP as shown below. More information about the full functionality of OCP can be found at its own chapter. Play the news Nothing more relaxing after you woke up, cancelling your alarm set on you OpenVoiceOS device than listening to your favorite news station while drinking some coffee (No OpenVoiceOS can not make you that coffee yet). Hey Mycroft, play the BBC news Some more features that come out of the box The whole OCP framework has some benefits and features that are not skill specific, such as \"Playlists\" and a view of the search results. You can access those by swiping to the right when something is playing. Homescreen widget The homescreen skill that comes pre-installed with OpenVoiceOS also comes with a widget for the OCP framework.","title":"Playing music"},{"location":"playing_music/#playing-music","text":"For playing music (and video as discussed within the next chapter), OpenVoiceOS uses OCP (OpenVoiceOS Common Play) and is basically a full fledge multimedia player on its own designed around open standards like MPRIS and with the vision of being fully integrated within the OpenVoiceOS software stack. Skills designed for OCP provide search results for OCP (think about them as media providers/catalogs/scrapers), OCP will play the best search result for you. OpenVoiceOS comes with a few OCP skills pre-installed, however more can be installed just like any other OVOS skill. You can find more OCP skills in the awesome-ocp-skills list","title":"Playing music"},{"location":"playing_music/#youtube-music","text":"A voiceassistant with smartspeaker functionality should be able to play music straight out of the box. For that reason the buildroot edition of OpenVoiceOS comes with the Youtube Music OCP Skill pre-installed. Just ask it to play something will start playback from Youtube assuming the asked sonmg is present on Youtube ofcourse. Hey Mycroft, play disturbed sound of silence This should just start playing utilizing OCP as shown below. More information about the full functionality of OCP can be found at its own chapter.","title":"Youtube Music"},{"location":"playing_music/#play-the-news","text":"Nothing more relaxing after you woke up, cancelling your alarm set on you OpenVoiceOS device than listening to your favorite news station while drinking some coffee (No OpenVoiceOS can not make you that coffee yet). Hey Mycroft, play the BBC news","title":"Play the news"},{"location":"playing_music/#some-more-features-that-come-out-of-the-box","text":"The whole OCP framework has some benefits and features that are not skill specific, such as \"Playlists\" and a view of the search results. You can access those by swiping to the right when something is playing.","title":"Some more features that come out of the box"},{"location":"playing_music/#homescreen-widget","text":"The homescreen skill that comes pre-installed with OpenVoiceOS also comes with a widget for the OCP framework.","title":"Homescreen widget"},{"location":"playing_video/","text":"Playing video Although the screen used on your OpenVoiceOS device might be small, the whole OCP mediaplaying frame does support video playback. You can find video OCP skills in the same awesome-ocp-skills list. The fourth column, \"playback type\" shows which type of payer is used for that specific skill. If you use a skill that utilizes the \"video player\" the below will be shown on your OpenVoiceOS it's screen at playback.","title":"Playing video"},{"location":"playing_video/#playing-video","text":"Although the screen used on your OpenVoiceOS device might be small, the whole OCP mediaplaying frame does support video playback. You can find video OCP skills in the same awesome-ocp-skills list. The fourth column, \"playback type\" shows which type of payer is used for that specific skill. If you use a skill that utilizes the \"video player\" the below will be shown on your OpenVoiceOS it's screen at playback.","title":"Playing video"},{"location":"prompts/","text":"Prompts Here we look at how to implement the most common types of prompts. For more information on conversation design see the Voice User Interface Design Guidelines . Open Ended Questions Sometimes after a Skill finishes speaking, we want to activate the microphone and let the utterance be handled as usual It is possible to speak some dialog, and activate the listener, directing the response back to the standard intent parsing engine. We may do this to let the user trigger another Skill, or because we want to make use of our own intents to handle the response. To do this, we use the expect_response parameter of self.speak and self.speak_dialog methods expose a keyword argument to trigger listening once TTS has finished playback from ovos_workshop.skills import OVOSSkill from ovos_workshop.decorators import intent_handler class AskMeSkill(OVOSSkill): @intent_handler('ask_me_something.intent') def handle_set_favorite(self): question = random.choice(self.question_list) self.speak(question, expect_response=True) Request Extra Information Any Skill can request extra information from the user before continuing - making a statement or asking a question before the microphone is activated to record the User's response. The base implementation of this is the get_response() method To see it in action, let's create a simple Skill that asks the User what their favorite flavor of ice cream is. from ovos_workshop.skills import OVOSSkill from ovos_workshop.decorators import intent_handler class IceCreamSkill(OVOSSkill): @intent_handler('set.favorite.intent') def handle_set_favorite(self): favorite_flavor = self.get_response('what.is.your.favorite.flavor') self.speak_dialog('confirm.favorite.flavor', {'flavor': favorite_flavor}) In this Skill we have used get_response() and passed it the name of our dialog file 'what.is.your.favorite.flavor.dialog' . This is the simplest form of this method. It will speak dialog from the given file, then activate the microphone for 3-10 seconds allowing the User to respond. The transcript of their response will then be assigned to our variable favorite_flavor . To confirm that we have heard the User correctly we then speak a confirmation dialog passing the value of favorite_flavor to be spoken as part of that dialog. Optional Arguments The get_response() method also takes the following optional arguments: data (dict) - used to populate the dialog file, just like speak_dialog() validator (function) - returns a boolean to define whether the response meets some criteria for success on_fail (function) - returns a string that will be spoken if the validator returns False num_retries (int) - number of times the system should repeat the question to get a successful result Yes / No Questions ask_yesno() checks if the response contains \"yes\" or \"no\" like phrases. A longer phrase containing the required vocab is considered successful e.g. both \"yes\" and \"yeah that would be great thanks\" would be considered a successful \"yes\". If \"yes\" or \"no\" responses are detected, then the method will return the string \"yes\" or \"no\". If the response does not contain \"yes\" or \"no\" vocabulary then the entire utterance will be returned. If no speech was detected indicating the User did not respond, or said something unexpected, then the method will return None . None indicates an invalid answer, this tells the skill it should handle the error, such as by asking the question again Let's add a new intent to our IceCreamSkill to see how this works. from ovos_workshop.skills import OVOSSkill from ovos_workshop.decorators import intent_handler class IceCreamSkill(OVOSSkill): @intent_handler('do.you.like.intent') def handle_do_you_like(self): likes_ice_cream = self.ask_yesno('do.you.like.ice.cream') if likes_ice_cream == 'yes': self.speak_dialog('does.like') elif likes_ice_cream == 'no': self.speak_dialog('does.not.like') else: self.speak_dialog('could.not.understand') In this example we have asked the User if they like ice cream. We then speak different dialog whether they respond yes or no. We also speak some error dialog if neither yes nor no are returned. A dedicated parser for each language is provided by ovos-lingua-franca , ensuring we can understand nuances such as double negation Some example sentences that would be considered either \"yes\" or \"no\". Test Utterance Expected Result \"beans\" None \"yes\" \"yes\" \"no\" \"no\" \"don't think so\" \"no\" \"i think not\" \"no\" \"that's affirmative\" \"yes\" \"no, but actually, yes\" \"yes\" \"yes, but actually, no\" \"no\" \"yes, yes, yes, but actually, no\" \"no\" \"please\" \"yes\" \"please don't\" \"no\" \"no! please! I beg you\" \"no\" \"yes, i don't want it for sure\" \"no\" \"please! I beg you\" \"yes\" \"i want it for sure\" \"yes\" \"obviously\" \"yes\" \"indeed\" \"yes\" \"no, I obviously hate it\" \"no\" \"that's certainly undesirable\" \"no\" \"yes, it's a lie\" \"yes\" \"no, it's a lie\" \"no\" \"he is lying\" \"no\" \"correct, he is lying\" \"yes\" \"it's a lie\" \"no\" \"you are mistaken\" \"no\" \"that's a mistake\" \"no\" \"wrong answer\" \"no\" \"it's not a lie\" \"yes\" \"he is not lying\" \"yes\" \"you are not mistaken\" \"yes\" \"tou are not wrong\" \"yes\" Providing a list of options ask_selection() provides a list of options to the User for them to select from. The User can respond with either the name of one of these options or select with a numbered ordinal eg \"the third\". This method automatically manages fuzzy matching the users response against the list of options provided. It also understands \"numeric\" responses, such as \"the first option\", \"the third one\", \"option 4\", \"the last one\" Let's jump back into our IceCreamSkill to give the User a list of options to choose from. from ovos_workshop.skills import OVOSSkill from ovos_workshop.decorators import intent_handler class IceCreamSkill(OVOSSkill): def initialize(self): self.flavors = ['vanilla', 'chocolate', 'mint'] @intent_handler('request.icecream.intent') def handle_request_icecream(self): self.speak_dialog('welcome') selection = self.ask_selection(self.flavors, 'what.flavor') self.speak_dialog('coming.right.up', {'flavor': selection}) In this example we first speak some welcome.dialog . The list of flavors is then spoken, followed by the what.flavor.dialog . Finally, we confirm the Users selection by speaking coming.right.up.dialog Optional arguments There are two optional arguments for this method. min_conf (float) defines the minimum confidence level for fuzzy matching the Users response against the list of options. numeric (bool) if set to True will speak the options as a numbered list eg \"One, vanilla. Two, chocolate. Or three, mint\"","title":"Prompts"},{"location":"prompts/#prompts","text":"Here we look at how to implement the most common types of prompts. For more information on conversation design see the Voice User Interface Design Guidelines .","title":"Prompts"},{"location":"prompts/#open-ended-questions","text":"Sometimes after a Skill finishes speaking, we want to activate the microphone and let the utterance be handled as usual It is possible to speak some dialog, and activate the listener, directing the response back to the standard intent parsing engine. We may do this to let the user trigger another Skill, or because we want to make use of our own intents to handle the response. To do this, we use the expect_response parameter of self.speak and self.speak_dialog methods expose a keyword argument to trigger listening once TTS has finished playback from ovos_workshop.skills import OVOSSkill from ovos_workshop.decorators import intent_handler class AskMeSkill(OVOSSkill): @intent_handler('ask_me_something.intent') def handle_set_favorite(self): question = random.choice(self.question_list) self.speak(question, expect_response=True)","title":"Open Ended Questions"},{"location":"prompts/#request-extra-information","text":"Any Skill can request extra information from the user before continuing - making a statement or asking a question before the microphone is activated to record the User's response. The base implementation of this is the get_response() method To see it in action, let's create a simple Skill that asks the User what their favorite flavor of ice cream is. from ovos_workshop.skills import OVOSSkill from ovos_workshop.decorators import intent_handler class IceCreamSkill(OVOSSkill): @intent_handler('set.favorite.intent') def handle_set_favorite(self): favorite_flavor = self.get_response('what.is.your.favorite.flavor') self.speak_dialog('confirm.favorite.flavor', {'flavor': favorite_flavor}) In this Skill we have used get_response() and passed it the name of our dialog file 'what.is.your.favorite.flavor.dialog' . This is the simplest form of this method. It will speak dialog from the given file, then activate the microphone for 3-10 seconds allowing the User to respond. The transcript of their response will then be assigned to our variable favorite_flavor . To confirm that we have heard the User correctly we then speak a confirmation dialog passing the value of favorite_flavor to be spoken as part of that dialog.","title":"Request Extra Information"},{"location":"prompts/#optional-arguments","text":"The get_response() method also takes the following optional arguments: data (dict) - used to populate the dialog file, just like speak_dialog() validator (function) - returns a boolean to define whether the response meets some criteria for success on_fail (function) - returns a string that will be spoken if the validator returns False num_retries (int) - number of times the system should repeat the question to get a successful result","title":"Optional Arguments"},{"location":"prompts/#yes-no-questions","text":"ask_yesno() checks if the response contains \"yes\" or \"no\" like phrases. A longer phrase containing the required vocab is considered successful e.g. both \"yes\" and \"yeah that would be great thanks\" would be considered a successful \"yes\". If \"yes\" or \"no\" responses are detected, then the method will return the string \"yes\" or \"no\". If the response does not contain \"yes\" or \"no\" vocabulary then the entire utterance will be returned. If no speech was detected indicating the User did not respond, or said something unexpected, then the method will return None . None indicates an invalid answer, this tells the skill it should handle the error, such as by asking the question again Let's add a new intent to our IceCreamSkill to see how this works. from ovos_workshop.skills import OVOSSkill from ovos_workshop.decorators import intent_handler class IceCreamSkill(OVOSSkill): @intent_handler('do.you.like.intent') def handle_do_you_like(self): likes_ice_cream = self.ask_yesno('do.you.like.ice.cream') if likes_ice_cream == 'yes': self.speak_dialog('does.like') elif likes_ice_cream == 'no': self.speak_dialog('does.not.like') else: self.speak_dialog('could.not.understand') In this example we have asked the User if they like ice cream. We then speak different dialog whether they respond yes or no. We also speak some error dialog if neither yes nor no are returned. A dedicated parser for each language is provided by ovos-lingua-franca , ensuring we can understand nuances such as double negation Some example sentences that would be considered either \"yes\" or \"no\". Test Utterance Expected Result \"beans\" None \"yes\" \"yes\" \"no\" \"no\" \"don't think so\" \"no\" \"i think not\" \"no\" \"that's affirmative\" \"yes\" \"no, but actually, yes\" \"yes\" \"yes, but actually, no\" \"no\" \"yes, yes, yes, but actually, no\" \"no\" \"please\" \"yes\" \"please don't\" \"no\" \"no! please! I beg you\" \"no\" \"yes, i don't want it for sure\" \"no\" \"please! I beg you\" \"yes\" \"i want it for sure\" \"yes\" \"obviously\" \"yes\" \"indeed\" \"yes\" \"no, I obviously hate it\" \"no\" \"that's certainly undesirable\" \"no\" \"yes, it's a lie\" \"yes\" \"no, it's a lie\" \"no\" \"he is lying\" \"no\" \"correct, he is lying\" \"yes\" \"it's a lie\" \"no\" \"you are mistaken\" \"no\" \"that's a mistake\" \"no\" \"wrong answer\" \"no\" \"it's not a lie\" \"yes\" \"he is not lying\" \"yes\" \"you are not mistaken\" \"yes\" \"tou are not wrong\" \"yes\"","title":"Yes / No Questions"},{"location":"prompts/#providing-a-list-of-options","text":"ask_selection() provides a list of options to the User for them to select from. The User can respond with either the name of one of these options or select with a numbered ordinal eg \"the third\". This method automatically manages fuzzy matching the users response against the list of options provided. It also understands \"numeric\" responses, such as \"the first option\", \"the third one\", \"option 4\", \"the last one\" Let's jump back into our IceCreamSkill to give the User a list of options to choose from. from ovos_workshop.skills import OVOSSkill from ovos_workshop.decorators import intent_handler class IceCreamSkill(OVOSSkill): def initialize(self): self.flavors = ['vanilla', 'chocolate', 'mint'] @intent_handler('request.icecream.intent') def handle_request_icecream(self): self.speak_dialog('welcome') selection = self.ask_selection(self.flavors, 'what.flavor') self.speak_dialog('coming.right.up', {'flavor': selection}) In this example we first speak some welcome.dialog . The list of flavors is then spoken, followed by the what.flavor.dialog . Finally, we confirm the Users selection by speaking coming.right.up.dialog","title":"Providing a list of options"},{"location":"prompts/#optional-arguments_1","text":"There are two optional arguments for this method. min_conf (float) defines the minimum confidence level for fuzzy matching the Users response against the list of options. numeric (bool) if set to True will speak the options as a numbered list eg \"One, vanilla. Two, chocolate. Or three, mint\"","title":"Optional arguments"},{"location":"qt5/","text":"Mycroft-GUI QT5 NOTE - Currently only a QT5 gui-client is available, help wanted to migrate to QT6 ! Introduction to QML The reference GUI client implementation is based on the QML user interface markup language that gives you complete freedom to create in-depth innovative interactions without boundaries or provide you with simple templates within the GUI framework that allow minimalistic display of text and images based on your skill development specifics and preferences. QML user interface markup language is a declarative language built on top of Qt's existing strengths designed to describe the user interface of a program: both what it looks like, and how it behaves. QML provides modules that consist of sophisticated set of graphical and behavioral building elements. Before Getting Started A collection of resources to familiarize you with QML and Kirigami Framework. Introduction to QML Introduction to Kirigami Importing Modules A QML module provides versioned types and JavaScript resources in a type namespace which may be used by clients who import the module. Modules make use of the QML versioning system which allows modules to be independently updated. More in-depth information about QML modules can be found here Qt QML Modules Documentation In the code snippet example below we will look at importing some of the common modules that provide the components required to get started with our Visual User Interface. import QtQuick 2.4 import QtQuick.Controls 2.2 import QtQuick.Layouts 1.4 import org.kde.kirigami 2.4 as Kirigami import Mycroft 1.0 as Mycroft import org.kde.lottie 1.0 QTQuick Module: Qt Quick module is the standard library for writing QML applications, the module provides a visual canvas and includes types for creating and animating visual components, receiving user input, creating data models and views and delayed object instantiation. In-depth information about QtQuick can be found at Qt Quick Documentation QTQuick.Controls Module: The QtQuick Controls module provides a set of controls that can be used to build complete interfaces in Qt Quick. Some of the controls provided are button controls, container controls, delegate controls, indicator controls, input controls, navigation controls and more, for a complete list of controls and components provided by QtQuick Controls you can refer to QtQuick Controls 2 Guidelines QtQuick.Layouts Module: QtQuick Layouts are a set of QML types used to arrange items in a user interface. Some of the layouts provided by QtQuick Layouts are Column Layout, Grid Layout, Row Layout and more, for a complete list of layouts you can refer to QtQuick Layouts Documentation Kirigami Module: Kirigami is a set of QtQuick components for mobile and convergent applications. Kirigami is a set of high level components to make the creation of applications that look and feel great on mobile as well as desktop devices and follow the Kirigami Human Interface Guidelines Mycroft Module: Mycroft GUI frameworks provides a set of high level components and events system for aiding in the development of Mycroft visual skills. One of the controls provided by Mycroft GUI frameworks are Mycroft-GUI Framework Base Delegates Mycroft-GUI Framework Base Delegates Documentation QML Lottie Module: This provides a QML Item to render Adobe\u00ae After Effects\u2122 animations exported as JSON with Bodymovin using the Lottie Web library. For list of all properties supported refer Lottie QML Mycroft-GUI Framework Base Delegates When you design your skill with QML, Mycroft-GUI frameworks provides you with some base delegates you should use when designing your GUI skill. The base delegates provide you with a basic presentation layer for your skill with some property assignments that can help you setup background images, background dim, timeout and grace time properties to give you the control you need for rendering an experience. In your GUI Skill you can use: Mycroft.Delegate: A basic and simple page based on Kirigami.Page Simple display Image and Text Example using Mycroft.Delegate import Mycroft 1.0 as Mycroft Mycroft.Delegate { skillBackgroundSource: sessionData.exampleImage ColumnLayout { anchors.fill: parent Image { id: imageId Layout.fillWidth: true Layout.preferredHeight: Kirigami.Units.gridUnit * 2 source: \"https://source.unsplash.com/1920x1080/?+autumn\" } Label { id: labelId Layout.fillWidth: true Layout.preferredHeight: Kirigami.Units.gridUnit * 4 text: \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\" } } } Mycroft.ScrollableDelegate: A delegate that displays skill visuals in a scroll enabled Kirigami Page. Example of using Mycroft.ScrollableDelegate import QtQuick 2.4 import QtQuick.Controls 2.2 import QtQuick.Layouts 1.4 import org.kde.kirigami 2.4 as Kirigami import Mycroft 1.0 as Mycroft Mycroft.ScrollableDelegate{ id: root skillBackgroundSource: sessionData.background property var sampleModel: sessionData.sampleBlob Kirigami.CardsListView { id: exampleListView Layout.fillWidth: true Layout.fillHeight: true model: sampleModel.lorem delegate: Kirigami.AbstractCard { id: rootCard implicitHeight: delegateItem.implicitHeight + Kirigami.Units.largeSpacing contentItem: Item { implicitWidth: parent.implicitWidth implicitHeight: parent.implicitHeight ColumnLayout { id: delegateItem anchors.left: parent.left anchors.right: parent.right anchors.top: parent.top spacing: Kirigami.Units.largeSpacing Kirigami.Heading { id: restaurantNameLabel Layout.fillWidth: true text: modelData.text level: 2 wrapMode: Text.WordWrap } Kirigami.Separator { Layout.fillWidth: true } Image { id: placeImage source: modelData.image Layout.fillWidth: true Layout.preferredHeight: Kirigami.Units.gridUnit * 3 fillMode: Image.PreserveAspectCrop } Item { Layout.fillWidth: true Layout.preferredHeight: Kirigami.Units.gridUnit * 1 } } } } } } QML Design Guidelines Before we dive deeper into the Design Guidelines, lets look at some concepts that a GUI developer should learn about: Units & Theming Units: Mycroft.Units.GridUnit is the fundamental unit of space that should be used for all sizing inside the QML UI, expressed in pixels. Each GridUnit is predefined as 16 pixels // Usage in QML Components example width: Mycroft.Units.gridUnit * 2 // 32px Wide height: Mycroft.Units.gridUnit // 16px Tall Theming: OVOS Shell uses a custom Kirigami Platform Theme plugin to provide global theming to all our skills and user interfaces, which also allows our GUI's to be fully compatible with the system themes on platforms that are not running the OVOS Shell. Kirigami Theme and Color Scheme guide is extensive and can be found here OVOS GUI's developed to follow the color scheme depend on only a subset of available colors, mainly: Kirigami.Theme.backgroundColor = Primary Color (Background Color: This will always be a dark palette or light palette depending on the dark or light chosen color scheme) Kirigami.Theme.highlightColor = Secondary Color (Accent Color: This will always be a standout palette that defines the themes dominating color and can be used for buttons, cards, borders, highlighted text etc.) Kirigami.Theme.textColor = Text Color (This will always be an opposite palette to the selected primary color) QML Delegate Design Best Practise Let's look at this image and qml example below, this is a representation of the Mycroft Delegate: When designing your first QML file, it is important to note the red triangles in the above image, these triangles represent the margin from the screen edge the GUI needs to be designed within, these margins ensure your GUI content does not overlap with features like edge lighting and menus in the platforms that support it like OVOS-Shell The content items and components all utilize the selected color scheme, where black is the primary background color, red is our accent color and white is our contrasting text color Let's look at this in QML: import ... import Mycroft 1.0 as Mycroft Mycroft.Delegate { skillBackgroundSource: sessionData.exampleImage leftPadding: 0 rightPadding: 0 topPadding: 0 bottomPadding: 0 Rectangle { anchors.fill: parent // Setting margins that need to be left for the screen edges anchors.margins: Mycroft.Units.gridUnit * 2 //Setting a background dim using our primary theme / background color on top of our skillBackgroundSource image for better readability and contrast color: Qt.rgba(Kirigami.Theme.backgroundColor.r, Kirigami.Theme.backgroundColor.g, Kirigami.Theme.backgroundColor.b, 0.3) Kirigami.Heading { level: 2 text: \"An Example Pie Chart\" anchors.top: parent.top anchors.left: parent.left anchors.right: parent.right height: Mycroft.Units.gridUnit * 3 // Setting the text color to always follow the color scheme for this item displayed on the screen color: Kirigami.Theme.textColor } PieChart { anchors.centerIn: parent pieColorMinor: Kirigami.Theme.backgroundColor // As in the image above the minor area of the pie chart uses our primary color pieColorMid: Kirigami.Theme.highlightColor // As in the image above the middle area is assigned the highlight or our accent color pieColorMajor: Kirigami.Theme.textColor // As in the image above the major area is assigned the text color } } } QML Delegate Multi Platform and Screen Guidelines OVOS Skill GUIs are designed to be multi-platform and screen friendly, to support this we always try to support both Horizontal and Vertical display's. Let's look at an example and a general approach to writing multi resolution friendly UI's Let's look at these images below that represent a Delegate as seen in a Horizontal screen: Let's look at these images below that represent a Delegate as seen in a Vertical screen: When designing for different screens it is preferred to utilize Grids, GridLayouts and GridViews this allows easier content placement as one can control the number of columns and rows displayed on the screen It is also recommended to use Flickables when you believe your content is going to not fit on the screen, this allows for content to always be scrollable. To make it easier to design scrollable content, Mycroft GUI provides you with a ready to use Mycroft.ScrollableDelegate. It is also preferred to use the width vs height comparison on the root delegate item to know when the screen should be using a vertical layout vs horizontal layout Let's look at this in QML: import ... import Mycroft 1.0 as Mycroft Mycroft.Delegate { id: root skillBackgroundSource: sessionData.exampleImage leftPadding: 0 rightPadding: 0 topPadding: 0 bottomPadding: 0 property bool horizontalMode: width >= height ? 1 : 0 // Using a ternary operator to detect if width of the delegate is greater than the height, which provides if the delegate is in horizontalMode Rectangle { anchors.fill: parent // Setting margins that need to be left for the screen edges anchors.margins: Mycroft.Units.gridUnit * 2 //Setting a background dim using our primary theme / background color on top of our skillBackgroundSource image for better readability and contrast color: Qt.rgba(Kirigami.Theme.backgroundColor.r, Kirigami.Theme.backgroundColor.g, Kirigami.Theme.backgroundColor.b, 0.3) Kirigami.Heading { level: 2 text: \"An Example Pie Chart\" // Setting the text color to always follow the color scheme color: Kirigami.Theme.textColor } GridLayout { id: examplesGridView // Checking if we are in horizontal mode, we should display two columns to display the items in the image above, or if we are in vertical mode, we should display a single column only columns: root.horizontalMode ? 2 : 1 Repeater { model: examplesModel delegates: ExamplesDelegate { ... } } } } } Advanced skill displays using QML Display Lottie Animations : You can use the LottieAnimation item just like any other QtQuick element, such as an Image and place it in your scene any way you please. QML Example import QtQuick 2.4 import QtQuick.Controls 2.2 import QtQuick.Layouts 1.4 import org.kde.kirigami 2.4 as Kirigami import Mycroft 1.0 as Mycroft import org.kde.lottie 1.0 Mycroft.Delegate { LottieAnimation { id: fancyAnimation anchors.fill: parent source: Qt.resolvedUrl(\"animations/fancy_animation.json\") loops: Animation.Infinite fillMode: Image.PreserveAspectFit running: true } } Display Sliding Images Contains an image that will slowly scroll in order to be shown completely QML Example import QtQuick 2.4 import QtQuick.Controls 2.2 import QtQuick.Layouts 1.4 import org.kde.kirigami 2.4 as Kirigami import Mycroft 1.0 as Mycroft Mycroft.Delegate { background: Mycroft.SlidingImage { source: \"foo.jpg\" running: bool //If true the sliding animation is active speed: 1 //Animation speed in Kirigami.Units.gridUnit / second } } Display Paginated Text Takes a long text and breaks it down into pages that can be horizontally swiped QML Example import QtQuick 2.4 import QtQuick.Controls 2.2 import QtQuick.Layouts 1.4 import org.kde.kirigami 2.4 as Kirigami import Mycroft 1.0 as Mycroft Mycroft.Delegate { Mycroft.PaginatedText { text: string //The text that should be displayed currentIndex: 0 //The currently visible page number (starting from 0) } } Display A Vertical ListView With Information Cards Kirigami CardsListView is a ListView which can have AbstractCard as its delegate: it will automatically assign the proper spacing and margins around the cards adhering to the design guidelines. Python Skill Example ... def handle_food_places(self, message): ... self.gui[\"foodPlacesBlob\"] = results.json self.gui.show_page(\"foodplaces.qml\") ... QML Example import QtQuick 2.4 import QtQuick.Controls 2.2 import QtQuick.Layouts 1.4 import org.kde.kirigami 2.4 as Kirigami import Mycroft 1.0 as Mycroft Mycroft.Delegate{ id: root property var foodPlacesModel: sessionData.foodPlacesBlob Kirigami.CardsListView { id: restaurantsListView Layout.fillWidth: true Layout.fillHeight: true model: foodPlacesModel delegate: Kirigami.AbstractCard { id: rootCard implicitHeight: delegateItem.implicitHeight + Kirigami.Units.largeSpacing contentItem: Item { implicitWidth: parent.implicitWidth implicitHeight: parent.implicitHeight ColumnLayout { id: delegateItem anchors.left: parent.left anchors.right: parent.right anchors.top: parent.top spacing: Kirigami.Units.smallSpacing Kirigami.Heading { id: restaurantNameLabel Layout.fillWidth: true text: modelData.name level: 3 wrapMode: Text.WordWrap } Kirigami.Separator { Layout.fillWidth: true } RowLayout { Layout.fillWidth: true Layout.preferredHeight: form.implicitHeight Image { id: placeImage source: modelData.image Layout.fillHeight: true Layout.preferredWidth: placeImage.implicitHeight + Kirigami.Units.gridUnit * 2 fillMode: Image.PreserveAspectFit } Kirigami.Separator { Layout.fillHeight: true } Kirigami.FormLayout { id: form Layout.fillWidth: true Layout.minimumWidth: aCard.implicitWidth Layout.alignment: Qt.AlignLeft | Qt.AlignBottom Label { Kirigami.FormData.label: \"Description:\" Layout.fillWidth: true wrapMode: Text.WordWrap elide: Text.ElideRight text: modelData.restaurantDescription } Label { Kirigami.FormData.label: \"Phone:\" Layout.fillWidth: true wrapMode: Text.WordWrap elide: Text.ElideRight text: modelData.phone } } } } } } } } Using Proportional Delegate For Simple Display Skills & Auto Layout ProportionalDelegate is a delegate which has proportional padding and a columnlayout as mainItem. The delegate supports a proportionalGridUnit which is based upon its size and the contents are supposed to be scaled proportionally to the delegate size either directly or using the proportionalGridUnit. AutoFitLabel is a label that will always scale its text size according to the item size rather than the other way around QML Example import QtQuick 2.4 import QtQuick.Controls 2.2 import QtQuick.Layouts 1.4 import org.kde.kirigami 2.4 as Kirigami import Mycroft 1.0 as Mycroft Mycroft.ProportionalDelegate { id: root Mycroft.AutoFitLabel { id: monthLabel font.weight: Font.Bold Layout.fillWidth: true Layout.preferredHeight: proportionalGridUnit * 40 text: sessionData.month } Mycroft.AutoFitLabel { id: dayLabel font.weight: Font.Bold Layout.fillWidth: true Layout.preferredHeight: proportionalGridUnit * 40 text: sessionData.day } } Using Slideshow Component To Show Cards Slideshow Slideshow component lets you insert a slideshow with your custom delegate in any skill display which can be tuned to autoplay and loop and also scrolled or flicked manually by the user. QML Example import QtQuick 2.4 import QtQuick.Controls 2.2 import QtQuick.Layouts 1.4 import org.kde.kirigami 2.4 as Kirigami import Mycroft 1.0 as Mycroft Mycroft.Delegate { id: root Mycroft.SlideShow { id: simpleSlideShow model: sessionData.exampleModel // model with slideshow data anchors.fill: parent interval: 5000 // time to switch between slides running: true // can be set to false if one wants to swipe manually loop: true // can be set to play through continously or just once delegate: Kirigami.AbstractCard { width: rootItem.width height: rootItem.height contentItem: ColumnLayout { anchors.fill: parent Kirigami.Heading { Layout.fillWidth: true wrapMode: Text.WordWrap level: 3 text: modelData.Title } Kirigami.Separator { Layout.fillWidth: true Layout.preferredHeight: 1 } Image { Layout.fillWidth: true Layout.preferredHeight: rootItem.height / 4 source: modelData.Image fillMode: Image.PreserveAspectCrop } } } } } Event Handling Mycroft GUI API provides an Event Handling Protocol between the skill and QML display which allow Skill Authors to forward events in either direction to an event consumer. Skill Authors have the ability to create any amount of custom events. Event names that start with \"system.\" are available to all skills, like previous/next/pick. Simple Event Trigger Example From QML Display To Skill Python Skill Example def initialize(self): # Initialize... self.gui.register_handler('skill.foo.event', self.handle_foo_event) ... def handle_foo_event(self, message): self.speak(message.data[\"string\"]) ... ... QML Example import QtQuick 2.4 import QtQuick.Controls 2.2 import QtQuick.Layouts 1.4 import org.kde.kirigami 2.4 as Kirigami import Mycroft 1.0 as Mycroft Mycroft.Delegate { id: root Button { anchors.fill: parent text: \"Click Me\" onClicked: { triggerGuiEvent(\"skill.foo.event\", {\"string\": \"Lorem ipsum dolor sit amet\"}) } } } Simple Event Trigger Example From Skill To QML Display Python Skill Example ... def handle_foo_intent(self, message): self.gui['foobar'] = message.data.get(\"utterance\") self.gui['color'] = \"blue\" self.gui.show_page(\"foo\") ... ... QML Example import QtQuick 2.4 import QtQuick.Controls 2.2 import QtQuick.Layouts 1.4 import org.kde.kirigami 2.4 as Kirigami import Mycroft 1.0 as Mycroft Mycroft.Delegate { id: root property var fooString: sessionData.foobar onFooStringChanged: { fooRect.color = sessionData.color } Rectangle { id: fooRect anchors.fill: parent color: \"#fff\" } } Resting Faces The resting face API provides skill authors the ability to extend their skills to supply their own customized IDLE screens that will be displayed when there is no activity on the screen. Simple Idle Screen Example Python Skill Example from ovos_workshop.decorators import resting_screen_handler ... @resting_screen_handler('NameOfIdleScreen') def handle_idle(self, message): self.gui.clear() self.log.info('Activating foo/bar resting page') self.gui[\"exampleText\"] = \"This Is A Idle Screen\" self.gui.show_page('idle.qml') QML Example import QtQuick 2.4 import QtQuick.Controls 2.2 import QtQuick.Layouts 1.4 import org.kde.kirigami 2.4 as Kirigami import Mycroft 1.0 as Mycroft Mycroft.Delegate { id: root property var fooString: sessionData.exampleText Kirigami.Heading { id: headerExample anchors.centerIn: parent text: fooString } }","title":"QT5 mycroft-gui"},{"location":"qt5/#mycroft-gui-qt5","text":"NOTE - Currently only a QT5 gui-client is available, help wanted to migrate to QT6 !","title":"Mycroft-GUI QT5"},{"location":"qt5/#introduction-to-qml","text":"The reference GUI client implementation is based on the QML user interface markup language that gives you complete freedom to create in-depth innovative interactions without boundaries or provide you with simple templates within the GUI framework that allow minimalistic display of text and images based on your skill development specifics and preferences. QML user interface markup language is a declarative language built on top of Qt's existing strengths designed to describe the user interface of a program: both what it looks like, and how it behaves. QML provides modules that consist of sophisticated set of graphical and behavioral building elements.","title":"Introduction to QML"},{"location":"qt5/#before-getting-started","text":"A collection of resources to familiarize you with QML and Kirigami Framework. Introduction to QML Introduction to Kirigami","title":"Before Getting Started"},{"location":"qt5/#importing-modules","text":"A QML module provides versioned types and JavaScript resources in a type namespace which may be used by clients who import the module. Modules make use of the QML versioning system which allows modules to be independently updated. More in-depth information about QML modules can be found here Qt QML Modules Documentation In the code snippet example below we will look at importing some of the common modules that provide the components required to get started with our Visual User Interface. import QtQuick 2.4 import QtQuick.Controls 2.2 import QtQuick.Layouts 1.4 import org.kde.kirigami 2.4 as Kirigami import Mycroft 1.0 as Mycroft import org.kde.lottie 1.0 QTQuick Module: Qt Quick module is the standard library for writing QML applications, the module provides a visual canvas and includes types for creating and animating visual components, receiving user input, creating data models and views and delayed object instantiation. In-depth information about QtQuick can be found at Qt Quick Documentation QTQuick.Controls Module: The QtQuick Controls module provides a set of controls that can be used to build complete interfaces in Qt Quick. Some of the controls provided are button controls, container controls, delegate controls, indicator controls, input controls, navigation controls and more, for a complete list of controls and components provided by QtQuick Controls you can refer to QtQuick Controls 2 Guidelines QtQuick.Layouts Module: QtQuick Layouts are a set of QML types used to arrange items in a user interface. Some of the layouts provided by QtQuick Layouts are Column Layout, Grid Layout, Row Layout and more, for a complete list of layouts you can refer to QtQuick Layouts Documentation Kirigami Module: Kirigami is a set of QtQuick components for mobile and convergent applications. Kirigami is a set of high level components to make the creation of applications that look and feel great on mobile as well as desktop devices and follow the Kirigami Human Interface Guidelines Mycroft Module: Mycroft GUI frameworks provides a set of high level components and events system for aiding in the development of Mycroft visual skills. One of the controls provided by Mycroft GUI frameworks are Mycroft-GUI Framework Base Delegates Mycroft-GUI Framework Base Delegates Documentation QML Lottie Module: This provides a QML Item to render Adobe\u00ae After Effects\u2122 animations exported as JSON with Bodymovin using the Lottie Web library. For list of all properties supported refer Lottie QML","title":"Importing Modules"},{"location":"qt5/#mycroft-gui-framework-base-delegates","text":"When you design your skill with QML, Mycroft-GUI frameworks provides you with some base delegates you should use when designing your GUI skill. The base delegates provide you with a basic presentation layer for your skill with some property assignments that can help you setup background images, background dim, timeout and grace time properties to give you the control you need for rendering an experience. In your GUI Skill you can use: Mycroft.Delegate: A basic and simple page based on Kirigami.Page Simple display Image and Text Example using Mycroft.Delegate import Mycroft 1.0 as Mycroft Mycroft.Delegate { skillBackgroundSource: sessionData.exampleImage ColumnLayout { anchors.fill: parent Image { id: imageId Layout.fillWidth: true Layout.preferredHeight: Kirigami.Units.gridUnit * 2 source: \"https://source.unsplash.com/1920x1080/?+autumn\" } Label { id: labelId Layout.fillWidth: true Layout.preferredHeight: Kirigami.Units.gridUnit * 4 text: \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\" } } } Mycroft.ScrollableDelegate: A delegate that displays skill visuals in a scroll enabled Kirigami Page. Example of using Mycroft.ScrollableDelegate import QtQuick 2.4 import QtQuick.Controls 2.2 import QtQuick.Layouts 1.4 import org.kde.kirigami 2.4 as Kirigami import Mycroft 1.0 as Mycroft Mycroft.ScrollableDelegate{ id: root skillBackgroundSource: sessionData.background property var sampleModel: sessionData.sampleBlob Kirigami.CardsListView { id: exampleListView Layout.fillWidth: true Layout.fillHeight: true model: sampleModel.lorem delegate: Kirigami.AbstractCard { id: rootCard implicitHeight: delegateItem.implicitHeight + Kirigami.Units.largeSpacing contentItem: Item { implicitWidth: parent.implicitWidth implicitHeight: parent.implicitHeight ColumnLayout { id: delegateItem anchors.left: parent.left anchors.right: parent.right anchors.top: parent.top spacing: Kirigami.Units.largeSpacing Kirigami.Heading { id: restaurantNameLabel Layout.fillWidth: true text: modelData.text level: 2 wrapMode: Text.WordWrap } Kirigami.Separator { Layout.fillWidth: true } Image { id: placeImage source: modelData.image Layout.fillWidth: true Layout.preferredHeight: Kirigami.Units.gridUnit * 3 fillMode: Image.PreserveAspectCrop } Item { Layout.fillWidth: true Layout.preferredHeight: Kirigami.Units.gridUnit * 1 } } } } } }","title":"Mycroft-GUI Framework Base Delegates"},{"location":"qt5/#qml-design-guidelines","text":"Before we dive deeper into the Design Guidelines, lets look at some concepts that a GUI developer should learn about:","title":"QML Design Guidelines"},{"location":"qt5/#units-theming","text":"","title":"Units &amp; Theming"},{"location":"qt5/#units","text":"Mycroft.Units.GridUnit is the fundamental unit of space that should be used for all sizing inside the QML UI, expressed in pixels. Each GridUnit is predefined as 16 pixels // Usage in QML Components example width: Mycroft.Units.gridUnit * 2 // 32px Wide height: Mycroft.Units.gridUnit // 16px Tall","title":"Units:"},{"location":"qt5/#theming","text":"OVOS Shell uses a custom Kirigami Platform Theme plugin to provide global theming to all our skills and user interfaces, which also allows our GUI's to be fully compatible with the system themes on platforms that are not running the OVOS Shell. Kirigami Theme and Color Scheme guide is extensive and can be found here OVOS GUI's developed to follow the color scheme depend on only a subset of available colors, mainly: Kirigami.Theme.backgroundColor = Primary Color (Background Color: This will always be a dark palette or light palette depending on the dark or light chosen color scheme) Kirigami.Theme.highlightColor = Secondary Color (Accent Color: This will always be a standout palette that defines the themes dominating color and can be used for buttons, cards, borders, highlighted text etc.) Kirigami.Theme.textColor = Text Color (This will always be an opposite palette to the selected primary color)","title":"Theming:"},{"location":"qt5/#qml-delegate-design-best-practise","text":"Let's look at this image and qml example below, this is a representation of the Mycroft Delegate: When designing your first QML file, it is important to note the red triangles in the above image, these triangles represent the margin from the screen edge the GUI needs to be designed within, these margins ensure your GUI content does not overlap with features like edge lighting and menus in the platforms that support it like OVOS-Shell The content items and components all utilize the selected color scheme, where black is the primary background color, red is our accent color and white is our contrasting text color Let's look at this in QML: import ... import Mycroft 1.0 as Mycroft Mycroft.Delegate { skillBackgroundSource: sessionData.exampleImage leftPadding: 0 rightPadding: 0 topPadding: 0 bottomPadding: 0 Rectangle { anchors.fill: parent // Setting margins that need to be left for the screen edges anchors.margins: Mycroft.Units.gridUnit * 2 //Setting a background dim using our primary theme / background color on top of our skillBackgroundSource image for better readability and contrast color: Qt.rgba(Kirigami.Theme.backgroundColor.r, Kirigami.Theme.backgroundColor.g, Kirigami.Theme.backgroundColor.b, 0.3) Kirigami.Heading { level: 2 text: \"An Example Pie Chart\" anchors.top: parent.top anchors.left: parent.left anchors.right: parent.right height: Mycroft.Units.gridUnit * 3 // Setting the text color to always follow the color scheme for this item displayed on the screen color: Kirigami.Theme.textColor } PieChart { anchors.centerIn: parent pieColorMinor: Kirigami.Theme.backgroundColor // As in the image above the minor area of the pie chart uses our primary color pieColorMid: Kirigami.Theme.highlightColor // As in the image above the middle area is assigned the highlight or our accent color pieColorMajor: Kirigami.Theme.textColor // As in the image above the major area is assigned the text color } } }","title":"QML Delegate Design Best Practise"},{"location":"qt5/#qml-delegate-multi-platform-and-screen-guidelines","text":"OVOS Skill GUIs are designed to be multi-platform and screen friendly, to support this we always try to support both Horizontal and Vertical display's. Let's look at an example and a general approach to writing multi resolution friendly UI's Let's look at these images below that represent a Delegate as seen in a Horizontal screen: Let's look at these images below that represent a Delegate as seen in a Vertical screen: When designing for different screens it is preferred to utilize Grids, GridLayouts and GridViews this allows easier content placement as one can control the number of columns and rows displayed on the screen It is also recommended to use Flickables when you believe your content is going to not fit on the screen, this allows for content to always be scrollable. To make it easier to design scrollable content, Mycroft GUI provides you with a ready to use Mycroft.ScrollableDelegate. It is also preferred to use the width vs height comparison on the root delegate item to know when the screen should be using a vertical layout vs horizontal layout Let's look at this in QML: import ... import Mycroft 1.0 as Mycroft Mycroft.Delegate { id: root skillBackgroundSource: sessionData.exampleImage leftPadding: 0 rightPadding: 0 topPadding: 0 bottomPadding: 0 property bool horizontalMode: width >= height ? 1 : 0 // Using a ternary operator to detect if width of the delegate is greater than the height, which provides if the delegate is in horizontalMode Rectangle { anchors.fill: parent // Setting margins that need to be left for the screen edges anchors.margins: Mycroft.Units.gridUnit * 2 //Setting a background dim using our primary theme / background color on top of our skillBackgroundSource image for better readability and contrast color: Qt.rgba(Kirigami.Theme.backgroundColor.r, Kirigami.Theme.backgroundColor.g, Kirigami.Theme.backgroundColor.b, 0.3) Kirigami.Heading { level: 2 text: \"An Example Pie Chart\" // Setting the text color to always follow the color scheme color: Kirigami.Theme.textColor } GridLayout { id: examplesGridView // Checking if we are in horizontal mode, we should display two columns to display the items in the image above, or if we are in vertical mode, we should display a single column only columns: root.horizontalMode ? 2 : 1 Repeater { model: examplesModel delegates: ExamplesDelegate { ... } } } } }","title":"QML Delegate Multi Platform and Screen Guidelines"},{"location":"qt5/#advanced-skill-displays-using-qml","text":"Display Lottie Animations : You can use the LottieAnimation item just like any other QtQuick element, such as an Image and place it in your scene any way you please. QML Example import QtQuick 2.4 import QtQuick.Controls 2.2 import QtQuick.Layouts 1.4 import org.kde.kirigami 2.4 as Kirigami import Mycroft 1.0 as Mycroft import org.kde.lottie 1.0 Mycroft.Delegate { LottieAnimation { id: fancyAnimation anchors.fill: parent source: Qt.resolvedUrl(\"animations/fancy_animation.json\") loops: Animation.Infinite fillMode: Image.PreserveAspectFit running: true } } Display Sliding Images Contains an image that will slowly scroll in order to be shown completely QML Example import QtQuick 2.4 import QtQuick.Controls 2.2 import QtQuick.Layouts 1.4 import org.kde.kirigami 2.4 as Kirigami import Mycroft 1.0 as Mycroft Mycroft.Delegate { background: Mycroft.SlidingImage { source: \"foo.jpg\" running: bool //If true the sliding animation is active speed: 1 //Animation speed in Kirigami.Units.gridUnit / second } } Display Paginated Text Takes a long text and breaks it down into pages that can be horizontally swiped QML Example import QtQuick 2.4 import QtQuick.Controls 2.2 import QtQuick.Layouts 1.4 import org.kde.kirigami 2.4 as Kirigami import Mycroft 1.0 as Mycroft Mycroft.Delegate { Mycroft.PaginatedText { text: string //The text that should be displayed currentIndex: 0 //The currently visible page number (starting from 0) } } Display A Vertical ListView With Information Cards Kirigami CardsListView is a ListView which can have AbstractCard as its delegate: it will automatically assign the proper spacing and margins around the cards adhering to the design guidelines. Python Skill Example ... def handle_food_places(self, message): ... self.gui[\"foodPlacesBlob\"] = results.json self.gui.show_page(\"foodplaces.qml\") ... QML Example import QtQuick 2.4 import QtQuick.Controls 2.2 import QtQuick.Layouts 1.4 import org.kde.kirigami 2.4 as Kirigami import Mycroft 1.0 as Mycroft Mycroft.Delegate{ id: root property var foodPlacesModel: sessionData.foodPlacesBlob Kirigami.CardsListView { id: restaurantsListView Layout.fillWidth: true Layout.fillHeight: true model: foodPlacesModel delegate: Kirigami.AbstractCard { id: rootCard implicitHeight: delegateItem.implicitHeight + Kirigami.Units.largeSpacing contentItem: Item { implicitWidth: parent.implicitWidth implicitHeight: parent.implicitHeight ColumnLayout { id: delegateItem anchors.left: parent.left anchors.right: parent.right anchors.top: parent.top spacing: Kirigami.Units.smallSpacing Kirigami.Heading { id: restaurantNameLabel Layout.fillWidth: true text: modelData.name level: 3 wrapMode: Text.WordWrap } Kirigami.Separator { Layout.fillWidth: true } RowLayout { Layout.fillWidth: true Layout.preferredHeight: form.implicitHeight Image { id: placeImage source: modelData.image Layout.fillHeight: true Layout.preferredWidth: placeImage.implicitHeight + Kirigami.Units.gridUnit * 2 fillMode: Image.PreserveAspectFit } Kirigami.Separator { Layout.fillHeight: true } Kirigami.FormLayout { id: form Layout.fillWidth: true Layout.minimumWidth: aCard.implicitWidth Layout.alignment: Qt.AlignLeft | Qt.AlignBottom Label { Kirigami.FormData.label: \"Description:\" Layout.fillWidth: true wrapMode: Text.WordWrap elide: Text.ElideRight text: modelData.restaurantDescription } Label { Kirigami.FormData.label: \"Phone:\" Layout.fillWidth: true wrapMode: Text.WordWrap elide: Text.ElideRight text: modelData.phone } } } } } } } } Using Proportional Delegate For Simple Display Skills & Auto Layout ProportionalDelegate is a delegate which has proportional padding and a columnlayout as mainItem. The delegate supports a proportionalGridUnit which is based upon its size and the contents are supposed to be scaled proportionally to the delegate size either directly or using the proportionalGridUnit. AutoFitLabel is a label that will always scale its text size according to the item size rather than the other way around QML Example import QtQuick 2.4 import QtQuick.Controls 2.2 import QtQuick.Layouts 1.4 import org.kde.kirigami 2.4 as Kirigami import Mycroft 1.0 as Mycroft Mycroft.ProportionalDelegate { id: root Mycroft.AutoFitLabel { id: monthLabel font.weight: Font.Bold Layout.fillWidth: true Layout.preferredHeight: proportionalGridUnit * 40 text: sessionData.month } Mycroft.AutoFitLabel { id: dayLabel font.weight: Font.Bold Layout.fillWidth: true Layout.preferredHeight: proportionalGridUnit * 40 text: sessionData.day } } Using Slideshow Component To Show Cards Slideshow Slideshow component lets you insert a slideshow with your custom delegate in any skill display which can be tuned to autoplay and loop and also scrolled or flicked manually by the user. QML Example import QtQuick 2.4 import QtQuick.Controls 2.2 import QtQuick.Layouts 1.4 import org.kde.kirigami 2.4 as Kirigami import Mycroft 1.0 as Mycroft Mycroft.Delegate { id: root Mycroft.SlideShow { id: simpleSlideShow model: sessionData.exampleModel // model with slideshow data anchors.fill: parent interval: 5000 // time to switch between slides running: true // can be set to false if one wants to swipe manually loop: true // can be set to play through continously or just once delegate: Kirigami.AbstractCard { width: rootItem.width height: rootItem.height contentItem: ColumnLayout { anchors.fill: parent Kirigami.Heading { Layout.fillWidth: true wrapMode: Text.WordWrap level: 3 text: modelData.Title } Kirigami.Separator { Layout.fillWidth: true Layout.preferredHeight: 1 } Image { Layout.fillWidth: true Layout.preferredHeight: rootItem.height / 4 source: modelData.Image fillMode: Image.PreserveAspectCrop } } } } }","title":"Advanced skill displays using QML"},{"location":"qt5/#event-handling","text":"Mycroft GUI API provides an Event Handling Protocol between the skill and QML display which allow Skill Authors to forward events in either direction to an event consumer. Skill Authors have the ability to create any amount of custom events. Event names that start with \"system.\" are available to all skills, like previous/next/pick. Simple Event Trigger Example From QML Display To Skill Python Skill Example def initialize(self): # Initialize... self.gui.register_handler('skill.foo.event', self.handle_foo_event) ... def handle_foo_event(self, message): self.speak(message.data[\"string\"]) ... ... QML Example import QtQuick 2.4 import QtQuick.Controls 2.2 import QtQuick.Layouts 1.4 import org.kde.kirigami 2.4 as Kirigami import Mycroft 1.0 as Mycroft Mycroft.Delegate { id: root Button { anchors.fill: parent text: \"Click Me\" onClicked: { triggerGuiEvent(\"skill.foo.event\", {\"string\": \"Lorem ipsum dolor sit amet\"}) } } } Simple Event Trigger Example From Skill To QML Display Python Skill Example ... def handle_foo_intent(self, message): self.gui['foobar'] = message.data.get(\"utterance\") self.gui['color'] = \"blue\" self.gui.show_page(\"foo\") ... ... QML Example import QtQuick 2.4 import QtQuick.Controls 2.2 import QtQuick.Layouts 1.4 import org.kde.kirigami 2.4 as Kirigami import Mycroft 1.0 as Mycroft Mycroft.Delegate { id: root property var fooString: sessionData.foobar onFooStringChanged: { fooRect.color = sessionData.color } Rectangle { id: fooRect anchors.fill: parent color: \"#fff\" } }","title":"Event Handling"},{"location":"qt5/#resting-faces","text":"The resting face API provides skill authors the ability to extend their skills to supply their own customized IDLE screens that will be displayed when there is no activity on the screen. Simple Idle Screen Example Python Skill Example from ovos_workshop.decorators import resting_screen_handler ... @resting_screen_handler('NameOfIdleScreen') def handle_idle(self, message): self.gui.clear() self.log.info('Activating foo/bar resting page') self.gui[\"exampleText\"] = \"This Is A Idle Screen\" self.gui.show_page('idle.qml') QML Example import QtQuick 2.4 import QtQuick.Controls 2.2 import QtQuick.Layouts 1.4 import org.kde.kirigami 2.4 as Kirigami import Mycroft 1.0 as Mycroft Mycroft.Delegate { id: root property var fooString: sessionData.exampleText Kirigami.Heading { id: headerExample anchors.centerIn: parent text: fooString } }","title":"Resting Faces"},{"location":"qt_apps/","text":"QT Applications NOTE : Only QT5 is supported, help wanted to migrate to QT6! You can build full standalone QT Voice Applications using QML with ovos-gui either via skills or OVOSAbstractApp Desktop Files GUI clients are allowed to filter a namespace , providing a GUI skill in it's own dedicated window. This is what powers Plasma Bigscreen Voice Apps via .desktop files Desktop files are also parsed to populate the skills in the homescreen app drawer NOTE : be sure to have mycroft-gui-qt5 installed [Desktop Entry] X-DBUS-StartupType=None X-KDE-StartupNotify=false Version=1.0 Terminal=false Type=Application Name=OCP Exec=ovos-gui-app --hideTextInput --skill=ovos.common_play Icon=OCP Categories=VoiceApp StartupNotify=false Learn more about the Desktop Entry Specification","title":"QT Voice Apps"},{"location":"qt_apps/#qt-applications","text":"NOTE : Only QT5 is supported, help wanted to migrate to QT6! You can build full standalone QT Voice Applications using QML with ovos-gui either via skills or OVOSAbstractApp","title":"QT Applications"},{"location":"qt_apps/#desktop-files","text":"GUI clients are allowed to filter a namespace , providing a GUI skill in it's own dedicated window. This is what powers Plasma Bigscreen Voice Apps via .desktop files Desktop files are also parsed to populate the skills in the homescreen app drawer NOTE : be sure to have mycroft-gui-qt5 installed [Desktop Entry] X-DBUS-StartupType=None X-KDE-StartupNotify=false Version=1.0 Terminal=false Type=Application Name=OCP Exec=ovos-gui-app --hideTextInput --skill=ovos.common_play Icon=OCP Categories=VoiceApp StartupNotify=false Learn more about the Desktop Entry Specification","title":"Desktop Files"},{"location":"runtime/","text":"Runtime Requirements ready_settings The ready_settings configuration is part of the SkillManager and determines when the 'mycroft.ready' message is emitted, indicating that OVOS is ready to operate. 'skills': Waits for the all offline skills to be loaded 'network_skills': Waits for the 'mycroft.network.connected' message 'internet_skills': Waits for the 'mycroft.internet.connected' message 'setup': Waits for an external setup skill to handle actions such as pairing and configuration. 'audio': Waits for the audio stack to be initialized 'speech': Waits for the speech engine (STT) to be initialized { \"ready_settings\": [ \"skills\", \"network_skills\", \"internet_skills\", \"audio\", \"speech\" ] } WARNING The default behavior is to wait only for offline skills at the start. This is a departure from mycroft-core, which did not have dynamic skills. It may impact skills listening for the mycroft.ready event. In this example, the ready_settings are configured to wait for network and internet connectivity before emitting the 'mycroft.ready' message. Each setup can costumize these settings based on their needs, a offline install won't want internet skills, a server wont want a audio stack etc. Dynamic Loading and Unloading NEW in ovos-core version 0.0.8 Optimizing Resource Usage OVOS introduces dynamic loading and unloading of skills, streamlining resource management. Skill developers no longer need to perform connectivity checks during the skill loading process. Skills are loaded only when their specific requirements are met. This optimization prevents unnecessary loading, conserving system resources and ensuring a more efficient skill environment. Reducing Unintended Activations Dynamic unloading of skills based on specific conditions significantly reduces the chance of unintentional activations. In scenarios where required conditions are not met, skills are gracefully unloaded, enhancing the user experience by avoiding unintended skill triggers. This approach aligns with resource-conscious design, providing a more responsive and reliable voice assistant environment. Developers can focus on skill functionality, knowing that OVOS efficiently manages skill loading and unloading based on runtime requirements. The RuntimeRequirements @classproperty NEW in ovos-core version 0.0.8 WARNING this functionality deprecates \"priority_skills\" config option The RuntimeRequirements property is a class property that skill developers can utilize to define the conditions and resource requirements of their skill during initialization and runtime. This property is a part of the OVOSSkill class and is used to guide the loading and unloading of skills based on specified criteria. To use RuntimeRequirements , skill developers need to override the property within their skill class. It's important to note that RuntimeRequirements is optional. If not defined, OVOS will assume internet is required but GUI is optional. This default behavior reflects the original behavior of mycroft-core and might change in the future . NOTE: The @classproperty decorator allows SkillManager to read this without instantiating the skill. These examples showcase how RuntimeRequirements can be used to tailor the loading and runtime conditions of a skill based on specific requirements and dependencies. Fully Offline Skill In this example, a fully offline skill is defined. The skill does not require internet or network connectivity during loading or runtime. If the network or internet is unavailable, the skill can still operate. Defining this will ensure your skill loads as soon as possible; otherwise, the SkillManager will wait for internet before loading the skill. from ovos_utils import classproperty from ovos_workshop.skills import OVOSSkill class MyOfflineSkill(OVOSSkill): @classproperty def runtime_requirements(self): return RuntimeRequirements(internet_before_load=False, network_before_load=False, requires_internet=False, requires_network=False, no_internet_fallback=True, no_network_fallback=True) Internet-Dependent Skill In this example, an online search skill with a local cache is defined. The skill requires internet connectivity during both loading and runtime. If the internet is not available, the skill won't load. Once loaded, the skill continues to require internet connectivity. However, our skill keeps a cache of previous results; therefore, it declares it can handle internet outages and will not be unloaded when the internet goes down. from ovos_utils import classproperty from ovos_workshop.skills import OVOSSkill class MyInternetSkill(OVOSSkill): @classproperty def runtime_requirements(self): # our skill can answer cached results when the internet goes down return RuntimeRequirements( internet_before_load=True, # only load once we have internet requires_internet=True, # indicate we need internet to work no_internet_fallback=True # do NOT unload if internet goes down ) def initialize(self): ... # do something that requires internet connectivity IOT Skill Controlling Devices via LAN Consider a skill that should only load once we have a network connection. By specifying that requirement, we can ensure that the skill is only loaded once the requirements are met, and it is safe to utilize network resources on initialization. In this example, an IOT skill controlling devices via LAN is defined. The skill requires network connectivity during loading, and if the network is not available, it won't load. Once loaded, the skill continues to require network connectivity and will unload if the network is lost. from ovos_utils import classproperty from ovos_workshop.skills import OVOSSkill class MyIOTSkill(OVOSSkill): @classproperty def runtime_requirements(self): return RuntimeRequirements( network_before_load=True, # only load once network available requires_network=True, # we need network to work no_network_fallback=False # unload if network goes down ) def initialize(self): ... # do something that needs LAN connectivity GUI and Internet-Dependent Skill with Fallback Consider a skill with both graphical user interface (GUI) and internet dependencies is defined. The skill requires both GUI availability and internet connectivity during loading. If either the GUI or the internet is not available, the skill won't load. Once loaded, the skill continues to require both GUI availability, but internet connectivity is optional. If the user asks \"show me the picture of the day\" and we have both internet and a GUI, our skill will match the intent. If we do not have internet but have a GUI, the skill will still operate, using a cached picture. If no GUI is available then the skill will unload regardless of internet status from ovos_utils import classproperty from ovos_workshop.skills import OVOSSkill class MyGUIAndInternetSkill(OVOSSkill): @classproperty def runtime_requirements(self) -> RuntimeRequirements: return RuntimeRequirements( gui_before_load=True, # only load if GUI is available requires_gui=True, # continue requiring GUI once loaded internet_before_load=True, # only load if internet is available requires_internet=True, # continue requiring internet once loaded no_gui_fallback=False, # unload if GUI becomes unavailable no_internet_fallback=True # do NOT unload if internet becomes unavailable, use cached picture ) def initialize(self): ... # do something that requires both GUI and internet connectivity","title":"Runtime Requirements"},{"location":"runtime/#runtime-requirements","text":"","title":"Runtime Requirements"},{"location":"runtime/#ready_settings","text":"The ready_settings configuration is part of the SkillManager and determines when the 'mycroft.ready' message is emitted, indicating that OVOS is ready to operate. 'skills': Waits for the all offline skills to be loaded 'network_skills': Waits for the 'mycroft.network.connected' message 'internet_skills': Waits for the 'mycroft.internet.connected' message 'setup': Waits for an external setup skill to handle actions such as pairing and configuration. 'audio': Waits for the audio stack to be initialized 'speech': Waits for the speech engine (STT) to be initialized { \"ready_settings\": [ \"skills\", \"network_skills\", \"internet_skills\", \"audio\", \"speech\" ] } WARNING The default behavior is to wait only for offline skills at the start. This is a departure from mycroft-core, which did not have dynamic skills. It may impact skills listening for the mycroft.ready event. In this example, the ready_settings are configured to wait for network and internet connectivity before emitting the 'mycroft.ready' message. Each setup can costumize these settings based on their needs, a offline install won't want internet skills, a server wont want a audio stack etc.","title":"ready_settings"},{"location":"runtime/#dynamic-loading-and-unloading","text":"NEW in ovos-core version 0.0.8 Optimizing Resource Usage OVOS introduces dynamic loading and unloading of skills, streamlining resource management. Skill developers no longer need to perform connectivity checks during the skill loading process. Skills are loaded only when their specific requirements are met. This optimization prevents unnecessary loading, conserving system resources and ensuring a more efficient skill environment. Reducing Unintended Activations Dynamic unloading of skills based on specific conditions significantly reduces the chance of unintentional activations. In scenarios where required conditions are not met, skills are gracefully unloaded, enhancing the user experience by avoiding unintended skill triggers. This approach aligns with resource-conscious design, providing a more responsive and reliable voice assistant environment. Developers can focus on skill functionality, knowing that OVOS efficiently manages skill loading and unloading based on runtime requirements.","title":"Dynamic Loading and Unloading"},{"location":"runtime/#the-runtimerequirements-classproperty","text":"NEW in ovos-core version 0.0.8 WARNING this functionality deprecates \"priority_skills\" config option The RuntimeRequirements property is a class property that skill developers can utilize to define the conditions and resource requirements of their skill during initialization and runtime. This property is a part of the OVOSSkill class and is used to guide the loading and unloading of skills based on specified criteria. To use RuntimeRequirements , skill developers need to override the property within their skill class. It's important to note that RuntimeRequirements is optional. If not defined, OVOS will assume internet is required but GUI is optional. This default behavior reflects the original behavior of mycroft-core and might change in the future . NOTE: The @classproperty decorator allows SkillManager to read this without instantiating the skill. These examples showcase how RuntimeRequirements can be used to tailor the loading and runtime conditions of a skill based on specific requirements and dependencies.","title":"The RuntimeRequirements @classproperty"},{"location":"runtime/#fully-offline-skill","text":"In this example, a fully offline skill is defined. The skill does not require internet or network connectivity during loading or runtime. If the network or internet is unavailable, the skill can still operate. Defining this will ensure your skill loads as soon as possible; otherwise, the SkillManager will wait for internet before loading the skill. from ovos_utils import classproperty from ovos_workshop.skills import OVOSSkill class MyOfflineSkill(OVOSSkill): @classproperty def runtime_requirements(self): return RuntimeRequirements(internet_before_load=False, network_before_load=False, requires_internet=False, requires_network=False, no_internet_fallback=True, no_network_fallback=True)","title":"Fully Offline Skill"},{"location":"runtime/#internet-dependent-skill","text":"In this example, an online search skill with a local cache is defined. The skill requires internet connectivity during both loading and runtime. If the internet is not available, the skill won't load. Once loaded, the skill continues to require internet connectivity. However, our skill keeps a cache of previous results; therefore, it declares it can handle internet outages and will not be unloaded when the internet goes down. from ovos_utils import classproperty from ovos_workshop.skills import OVOSSkill class MyInternetSkill(OVOSSkill): @classproperty def runtime_requirements(self): # our skill can answer cached results when the internet goes down return RuntimeRequirements( internet_before_load=True, # only load once we have internet requires_internet=True, # indicate we need internet to work no_internet_fallback=True # do NOT unload if internet goes down ) def initialize(self): ... # do something that requires internet connectivity","title":"Internet-Dependent Skill"},{"location":"runtime/#iot-skill-controlling-devices-via-lan","text":"Consider a skill that should only load once we have a network connection. By specifying that requirement, we can ensure that the skill is only loaded once the requirements are met, and it is safe to utilize network resources on initialization. In this example, an IOT skill controlling devices via LAN is defined. The skill requires network connectivity during loading, and if the network is not available, it won't load. Once loaded, the skill continues to require network connectivity and will unload if the network is lost. from ovos_utils import classproperty from ovos_workshop.skills import OVOSSkill class MyIOTSkill(OVOSSkill): @classproperty def runtime_requirements(self): return RuntimeRequirements( network_before_load=True, # only load once network available requires_network=True, # we need network to work no_network_fallback=False # unload if network goes down ) def initialize(self): ... # do something that needs LAN connectivity","title":"IOT Skill Controlling Devices via LAN"},{"location":"runtime/#gui-and-internet-dependent-skill-with-fallback","text":"Consider a skill with both graphical user interface (GUI) and internet dependencies is defined. The skill requires both GUI availability and internet connectivity during loading. If either the GUI or the internet is not available, the skill won't load. Once loaded, the skill continues to require both GUI availability, but internet connectivity is optional. If the user asks \"show me the picture of the day\" and we have both internet and a GUI, our skill will match the intent. If we do not have internet but have a GUI, the skill will still operate, using a cached picture. If no GUI is available then the skill will unload regardless of internet status from ovos_utils import classproperty from ovos_workshop.skills import OVOSSkill class MyGUIAndInternetSkill(OVOSSkill): @classproperty def runtime_requirements(self) -> RuntimeRequirements: return RuntimeRequirements( gui_before_load=True, # only load if GUI is available requires_gui=True, # continue requiring GUI once loaded internet_before_load=True, # only load if internet is available requires_internet=True, # continue requiring internet once loaded no_gui_fallback=False, # unload if GUI becomes unavailable no_internet_fallback=True # do NOT unload if internet becomes unavailable, use cached picture ) def initialize(self): ... # do something that requires both GUI and internet connectivity","title":"GUI and Internet-Dependent Skill with Fallback"},{"location":"session_skills/","text":"Session Aware Skills NEW ovos-core version 0.0.8 If you want your skills to handle simultaneous users you need to make them Session aware Each remote client, usually a voice satellite , will send a Session with the Message Your skill should keep track of any Session specific state separately, eg, a chat history WARNING : Stateful Skills need to be Session Aware to play well with HiveMind SessionManager You can access the Session in a Message object via the SessionManager class from ovos_bus_client.session import SessionManager, Session class MySkill(OVOSSkill): def on_something(self, message): sess = SessionManager.get(message) print(sess.session_id) If the message originated in the device itself, the session_id is always equal to \"default\" , if it comes from an external client then it will be a unique uuid Magic Properties Skills have some \"magic properties\", these will always reflect the value in the current Session # magic properties -> depend on message.context / Session @property def lang(self) -> str: \"\"\" Get the current language as a BCP-47 language code. This will consider current session data if available, else Configuration. \"\"\" @property def location(self) -> dict: \"\"\" Get the JSON data struction holding location information. This info can come from Session \"\"\" @property def location_pretty(self) -> Optional[str]: \"\"\" Get a speakable city from the location config if available This info can come from Session \"\"\" @property def location_timezone(self) -> Optional[str]: \"\"\" Get the timezone code, such as 'America/Los_Angeles' This info can come from Session \"\"\" @property def dialog_renderer(self) -> Optional[MustacheDialogRenderer]: \"\"\" Get a dialog renderer for this skill. Language will be determined by message context to match the language associated with the current session or else from Configuration. \"\"\" @property def resources(self) -> SkillResources: \"\"\" Get a SkillResources object for the current language. Objects are initialized for the current Session language as needed. \"\"\" Per User Interactions Let's consider a skill that keeps track of a chat history, how would such a skill keep track of Sessions ? from ovos_bus_client.session import SessionManager, Session from ovos_workshop.decorators import intent_handler from ovos_workshop.skills import OVOSSkill class UtteranceRepeaterSkill(OVOSSkill): def initialize(self): self.chat_sessions = {} self.add_event('recognizer_loop:utterance', self.on_utterance) # keep chat history per session def on_utterance(self, message): utt = message.data['utterances'][0] sess = SessionManager.get(message) if sess.session_id not in self.chat_sessions: self.chat_sessions[sess.session_id] = {\"current_stt\": \"\"} self.chat_sessions[sess.session_id][\"prev_stt\"] = self.chat_sessions[sess.session_id][\"current_stt\"] self.chat_sessions[sess.session_id][\"current_stt\"] = utt # retrieve previous STT per session @intent_handler('repeat.stt.intent') def handle_repeat_stt(self, message): sess = SessionManager.get(message) if sess.session_id not in self.chat_sessions: utt = self.translate('nothing') else: utt = self.chat_sessions[sess.session_id][\"prev_stt\"] self.speak_dialog('repeat.stt', {\"stt\": utt}) # session specific stop event # if this method returns True then self.stop will NOT be called def stop_session(self, session: Session): if session.session_id in self.chat_sessions: self.chat_sessions.pop(session.session_id) return True return False A full example can be found in the parrot skill","title":"Session Aware Skills"},{"location":"session_skills/#session-aware-skills","text":"NEW ovos-core version 0.0.8 If you want your skills to handle simultaneous users you need to make them Session aware Each remote client, usually a voice satellite , will send a Session with the Message Your skill should keep track of any Session specific state separately, eg, a chat history WARNING : Stateful Skills need to be Session Aware to play well with HiveMind","title":"Session Aware Skills"},{"location":"session_skills/#sessionmanager","text":"You can access the Session in a Message object via the SessionManager class from ovos_bus_client.session import SessionManager, Session class MySkill(OVOSSkill): def on_something(self, message): sess = SessionManager.get(message) print(sess.session_id) If the message originated in the device itself, the session_id is always equal to \"default\" , if it comes from an external client then it will be a unique uuid","title":"SessionManager"},{"location":"session_skills/#magic-properties","text":"Skills have some \"magic properties\", these will always reflect the value in the current Session # magic properties -> depend on message.context / Session @property def lang(self) -> str: \"\"\" Get the current language as a BCP-47 language code. This will consider current session data if available, else Configuration. \"\"\" @property def location(self) -> dict: \"\"\" Get the JSON data struction holding location information. This info can come from Session \"\"\" @property def location_pretty(self) -> Optional[str]: \"\"\" Get a speakable city from the location config if available This info can come from Session \"\"\" @property def location_timezone(self) -> Optional[str]: \"\"\" Get the timezone code, such as 'America/Los_Angeles' This info can come from Session \"\"\" @property def dialog_renderer(self) -> Optional[MustacheDialogRenderer]: \"\"\" Get a dialog renderer for this skill. Language will be determined by message context to match the language associated with the current session or else from Configuration. \"\"\" @property def resources(self) -> SkillResources: \"\"\" Get a SkillResources object for the current language. Objects are initialized for the current Session language as needed. \"\"\"","title":"Magic Properties"},{"location":"session_skills/#per-user-interactions","text":"Let's consider a skill that keeps track of a chat history, how would such a skill keep track of Sessions ? from ovos_bus_client.session import SessionManager, Session from ovos_workshop.decorators import intent_handler from ovos_workshop.skills import OVOSSkill class UtteranceRepeaterSkill(OVOSSkill): def initialize(self): self.chat_sessions = {} self.add_event('recognizer_loop:utterance', self.on_utterance) # keep chat history per session def on_utterance(self, message): utt = message.data['utterances'][0] sess = SessionManager.get(message) if sess.session_id not in self.chat_sessions: self.chat_sessions[sess.session_id] = {\"current_stt\": \"\"} self.chat_sessions[sess.session_id][\"prev_stt\"] = self.chat_sessions[sess.session_id][\"current_stt\"] self.chat_sessions[sess.session_id][\"current_stt\"] = utt # retrieve previous STT per session @intent_handler('repeat.stt.intent') def handle_repeat_stt(self, message): sess = SessionManager.get(message) if sess.session_id not in self.chat_sessions: utt = self.translate('nothing') else: utt = self.chat_sessions[sess.session_id][\"prev_stt\"] self.speak_dialog('repeat.stt', {\"stt\": utt}) # session specific stop event # if this method returns True then self.stop will NOT be called def stop_session(self, session: Session): if session.session_id in self.chat_sessions: self.chat_sessions.pop(session.session_id) return True return False A full example can be found in the parrot skill","title":"Per User Interactions"},{"location":"shell/","text":"OVOS Shell OVOS-shell is the OpenVoiceOS client implementation of the mycroft-gui-qt5 library used in our embedded device images Design Principles The OpenVoiceOS Shell was designed with some simple principles in mind. The visual interface is always secondary to the voice interface. Our goal is to make all interactions Voice First, meaning that the user could accomplish their tasks with just voice interaction. Touchscreen menus should be kept to a minimum, this reinforces using the primary mode of interaction, voice. However, many important controls need to be implemented as multimodal such as the ability to return to the home screen, change the volume, change the brightness of the screen, control media playback, and other system settings. OpenVoiceOS images ship with ovos-homescreen and ovos-shell , built on top of QT5, these components ensure the viability of the GUI in embedded devices running ovos-shell via EGLFS, without requiring a traditional desktop environment Companion Plugins To unlock full functionality you also need to configure ovos-gui-plugin-shell-companion in mycroft.conf This plugin integrates with ovos-gui to provide: color scheme manager notifications widgets configuration provider (settings UI) brightness control (night mode etc) { \"gui\": { // Extensions provide additional GUI platform support for specific devices \"extension\": \"ovos-gui-plugin-shell-companion\", // homescreen skill to use \"idle_display_skill\": \"skill-ovos-homescreen.openvoiceos\" } } OVOS-shell is tightly coupled to PHAL , the following companion plugins should also be installed ovos-PHAL-plugin-network-manager ovos-PHAL-plugin-gui-network-client ovos-PHAL-plugin-wifi-setup ovos-PHAL-plugin-alsa ovos-PHAL-plugin-system Configuration The Shell can be configured in a few ways. GUI Display settings Color Theme editor Shell Options ~/.config/OpenvoiceOS/OvosShell.conf can be edited to change shell options that may also be changed via UI. An example config would look like: [General] fakeBrightness=1 menuLabels=true Themes Shell themes can be included in /usr/share/OVOS/ColorSchemes/ or ~/.local/share/OVOS/ColorSchemes/ in json format. Note that colors should include an alpha value (usually FF ). { \"name\": \"Neon Green\", \"primaryColor\": \"#FF072103\", \"secondaryColor\": \"#FF2C7909\", \"textColor\": \"#FFF1F1F1\" }","title":"Shell"},{"location":"shell/#ovos-shell","text":"OVOS-shell is the OpenVoiceOS client implementation of the mycroft-gui-qt5 library used in our embedded device images","title":"OVOS Shell"},{"location":"shell/#design-principles","text":"The OpenVoiceOS Shell was designed with some simple principles in mind. The visual interface is always secondary to the voice interface. Our goal is to make all interactions Voice First, meaning that the user could accomplish their tasks with just voice interaction. Touchscreen menus should be kept to a minimum, this reinforces using the primary mode of interaction, voice. However, many important controls need to be implemented as multimodal such as the ability to return to the home screen, change the volume, change the brightness of the screen, control media playback, and other system settings. OpenVoiceOS images ship with ovos-homescreen and ovos-shell , built on top of QT5, these components ensure the viability of the GUI in embedded devices running ovos-shell via EGLFS, without requiring a traditional desktop environment","title":"Design Principles"},{"location":"shell/#companion-plugins","text":"To unlock full functionality you also need to configure ovos-gui-plugin-shell-companion in mycroft.conf This plugin integrates with ovos-gui to provide: color scheme manager notifications widgets configuration provider (settings UI) brightness control (night mode etc) { \"gui\": { // Extensions provide additional GUI platform support for specific devices \"extension\": \"ovos-gui-plugin-shell-companion\", // homescreen skill to use \"idle_display_skill\": \"skill-ovos-homescreen.openvoiceos\" } } OVOS-shell is tightly coupled to PHAL , the following companion plugins should also be installed ovos-PHAL-plugin-network-manager ovos-PHAL-plugin-gui-network-client ovos-PHAL-plugin-wifi-setup ovos-PHAL-plugin-alsa ovos-PHAL-plugin-system","title":"Companion Plugins"},{"location":"shell/#configuration","text":"The Shell can be configured in a few ways.","title":"Configuration"},{"location":"shell/#gui","text":"Display settings Color Theme editor","title":"GUI"},{"location":"shell/#shell-options","text":"~/.config/OpenvoiceOS/OvosShell.conf can be edited to change shell options that may also be changed via UI. An example config would look like: [General] fakeBrightness=1 menuLabels=true","title":"Shell Options"},{"location":"shell/#themes","text":"Shell themes can be included in /usr/share/OVOS/ColorSchemes/ or ~/.local/share/OVOS/ColorSchemes/ in json format. Note that colors should include an alpha value (usually FF ). { \"name\": \"Neon Green\", \"primaryColor\": \"#FF072103\", \"secondaryColor\": \"#FF2C7909\", \"textColor\": \"#FFF1F1F1\" }","title":"Themes"},{"location":"skill_api/","text":"Skill API The Skill API uses the Message Bus to communicate between Skills and wraps the interaction in simple Python objects making them easy to use. Making a method available through the Skill API A method can be tagged with the skill_api_method decorator. This will handle all the basics of making the method available to other Skills over the Message Bus. @skill_api_method def my_exported_method(self, my_arg, my_other_arg): \"\"\"My skill api method documentation \"\"\" The decorator will generate everything needed for accessing the method over the Message Bus and extract the associated docstring. Limitations The Skill API works over the Message Bus. This requires that the return values are json serializable. All common Python builtin types (such as List, String, None, etc.) work well, however custom classes are not currently supported. Example from ovos_workshop.skills import OVOSSkill from ovos_workshop.decorators import intent_handler, skill_api_method class RobberSkill(OVOSSkill): @skill_api_method def robber_lang(self, sentence): \"\"\"Encode a sentence to \"R\u00f6varspr\u00e5ket\". Each consonant gets converted to consonant + \"o\" + consonant, vowels are left as is. Returns: (str) sentence in the robber language. \"\"\" wovels = \"aeiouy\u00e5\u00e4\u00f6\" tokens = [] for char in sentence.lower() and char.isalpha(): if char not in wovels: tokens.append(char + 'o' + char) else: tokens.append(char) return ' '.join(tokens) Using another Skill's API If you want to make use of exported functionality from another Skill, you must fetch that Skill's SkillApi . This will give you a small class with the target Skill's exported methods. These methods are nothing special and can be called like any other class's methods. To access the robber_lang() method we created above, we could write: from ovos_workshop.skills.api import SkillApi class NewRobberSkill(OVOSSkill): def initialize(self): self.robber = SkillApi.get('robber-skill.forslund') self.speak(self.robber.robber_lang('hello world')) When the NewRobberSkill is initialized, it will assign the API from the Skill robber-skill.forslund to self.robber . We then run the exported method robber_lang() passing the argument 'hello world' . Our NewRobberSkill will therefore speak something like \"hoh e lol lol o wow o ror lol dod\".","title":"Skill API"},{"location":"skill_api/#skill-api","text":"The Skill API uses the Message Bus to communicate between Skills and wraps the interaction in simple Python objects making them easy to use.","title":"Skill API"},{"location":"skill_api/#making-a-method-available-through-the-skill-api","text":"A method can be tagged with the skill_api_method decorator. This will handle all the basics of making the method available to other Skills over the Message Bus. @skill_api_method def my_exported_method(self, my_arg, my_other_arg): \"\"\"My skill api method documentation \"\"\" The decorator will generate everything needed for accessing the method over the Message Bus and extract the associated docstring.","title":"Making a method available through the Skill API"},{"location":"skill_api/#limitations","text":"The Skill API works over the Message Bus. This requires that the return values are json serializable. All common Python builtin types (such as List, String, None, etc.) work well, however custom classes are not currently supported.","title":"Limitations"},{"location":"skill_api/#example","text":"from ovos_workshop.skills import OVOSSkill from ovos_workshop.decorators import intent_handler, skill_api_method class RobberSkill(OVOSSkill): @skill_api_method def robber_lang(self, sentence): \"\"\"Encode a sentence to \"R\u00f6varspr\u00e5ket\". Each consonant gets converted to consonant + \"o\" + consonant, vowels are left as is. Returns: (str) sentence in the robber language. \"\"\" wovels = \"aeiouy\u00e5\u00e4\u00f6\" tokens = [] for char in sentence.lower() and char.isalpha(): if char not in wovels: tokens.append(char + 'o' + char) else: tokens.append(char) return ' '.join(tokens)","title":"Example"},{"location":"skill_api/#using-another-skills-api","text":"If you want to make use of exported functionality from another Skill, you must fetch that Skill's SkillApi . This will give you a small class with the target Skill's exported methods. These methods are nothing special and can be called like any other class's methods. To access the robber_lang() method we created above, we could write: from ovos_workshop.skills.api import SkillApi class NewRobberSkill(OVOSSkill): def initialize(self): self.robber = SkillApi.get('robber-skill.forslund') self.speak(self.robber.robber_lang('hello world')) When the NewRobberSkill is initialized, it will assign the API from the Skill robber-skill.forslund to self.robber . We then run the exported method robber_lang() passing the argument 'hello world' . Our NewRobberSkill will therefore speak something like \"hoh e lol lol o wow o ror lol dod\".","title":"Using another Skill's API"},{"location":"skill_filesystem/","text":"Filesystem access Many Skills may want access to parts of the filesystem. To account for the many different platforms that can run OVOS there are three locations that a Skill can utilize. Persistent filesystem Temporary cache Skill's own root directory Persistent Files When your Skill needs to store some data that will persist over time and cannot easily be rebuilt, there is a persistent filesystem namespaced to your Skill. Reading and writing to files This uses the standard Python open() method to read and write files. It takes two parameters: file_name (str) - a path relative to the namespace. subdirs not currently supported. mode (str) \u2013 a file handle mode [r, r+, w, w+, rb, rb+, wb+, a, ab, a+, ab+, x] Example: def write_line_to_file(self, file_name, line): \"\"\"Write a single line to a file in the Skills persistent filesystem.\"\"\" with self.file_system.open(file_name, \"w\") as my_file: my_file.write(line) def read_file(self, file_name): \"\"\"Read the contents of a file in the Skills persistent filesystem.\"\"\" with self.file_system.open(file_name, \"r\") as my_file: return my_file.read() Check if a file exists Quick method to see if some file exists in the namespaced directory. Example: file_name = \"example.txt\" with self.file_system.open(file_name, \"w\") as my_file: my_file.write(\"Hello world\") self.log.info(self.file_system.exists(file_name)) # True self.log.info(self.file_system.exists(\"new.txt\")) # False Get the path of the namespaced directory. self.file_system.path is a member value containing the root path of the namespace. However, it is recommended that you use the self.file_system.open() method to read and write files. Example: from ovos_workshop.skills import OVOSSkill class FileSystemSkill(OVOSSkill): def initialize(self): \"\"\"Log the path of this Skills persistent namespace.\"\"\" self.log.info(self.file_system.path) Create subdirectories Now that we have the path of our namespaced filesystem, we can organize our files however we like within that directory. In this example, we create a subdirectory called \"cache\", then write to a text file inside of it. from os import mkdir from os.path import join from ovos_workshop.skills import OVOSSkill class FileSystemSkill(OVOSSkill): def initialize(self): \"\"\"Create a cache subdirectory and write to a file inside it\"\"\" cache_dir = \"cache\" file_name = \"example.txt\" if not self.file_system.exists(cache_dir): mkdir(join(self.file_system.path, cache_dir)) with self.file_system.open(join(cache_dir, file_name), \"w\") as my_file: my_file.write('hello') Example Skill from ovos_workshop.skills import OVOSSkill from ovos_workshop.decorators import intent_handler class FileSystemSkill(OVOSSkill): def initialize(self): \"\"\"Perform initial setup for the Skill. For this example we do four things: 1. Log the path of this directory. 2. Write to a file in the directory. 3. Check that our file exists. 4. Read the contents of our file from disk. \"\"\" file_name = \"example.txt\" self.log.info(self.file_system.path) self.write_line_to_file(file_name, \"hello world\") self.log.info(self.file_system.exists(file_name)) self.log.info(self.read_file(file_name)) def write_line_to_file(self, file_name, line): \"\"\"Write a single line to a file in the Skills persistent filesystem.\"\"\" with self.file_system.open(file_name, \"w\") as my_file: my_file.write(line) def read_file(self, file_name): \"\"\"Read the contents of a file in the Skills persistent filesystem.\"\"\" with self.file_system.open(file_name, \"r\") as my_file: return my_file.read() Temporary Cache Skills can create a directory for caching temporary data to speed up performance. This directory will likely be part of a small RAM disk and may be cleared at any time. So code that uses these cached files must be able to fall back and regenerate the file. Example Skill from os.path import join from ovos_workshop.skills import OVOSSkill from ovos_workshop.decorators import intent_handler from ovos_utils.file_utils import get_cache_directory class CachingSkill(OVOSSkill): def initialize(self): \"\"\"Perform initial setup for the Skill. For this example we do four things: 1. Get a cache directory namespaced for our Skill. 2. Define a file path for the cache_file. 3. Write some data to the cache_file 4. Log the path of the cache_file 4. Log the contents of the cache_file. \"\"\" cache_dir = get_cache_directory('CachingSkill') self.cache_file = join(cache_dir, \"myfile.txt\") self.cache_data() self.log.info(self.cache_file) self.log.info(self.read_cached_data()) def cache_data(self): with open(self.cache_file, \"w\") as cache_file: cache_file.write(\"Some cached data\") def read_cached_data(self): with open(self.cache_file, \"r\") as cache_file: return cache_file.read() Skill Root Directory self.root_dir This member variable contains the absolute path of a Skill\u2019s root directory e.g. ~.local/share/mycroft/skills/my-skill.me/ . Generally Skills should not modify anything within this directory. Modifying anything in the Skill directory will reload the Skill. It is also not guaranteed that the Skill will have permission to write to this directory.","title":"Filesystem"},{"location":"skill_filesystem/#filesystem-access","text":"Many Skills may want access to parts of the filesystem. To account for the many different platforms that can run OVOS there are three locations that a Skill can utilize. Persistent filesystem Temporary cache Skill's own root directory","title":"Filesystem access"},{"location":"skill_filesystem/#persistent-files","text":"When your Skill needs to store some data that will persist over time and cannot easily be rebuilt, there is a persistent filesystem namespaced to your Skill.","title":"Persistent Files"},{"location":"skill_filesystem/#reading-and-writing-to-files","text":"This uses the standard Python open() method to read and write files. It takes two parameters: file_name (str) - a path relative to the namespace. subdirs not currently supported. mode (str) \u2013 a file handle mode [r, r+, w, w+, rb, rb+, wb+, a, ab, a+, ab+, x] Example: def write_line_to_file(self, file_name, line): \"\"\"Write a single line to a file in the Skills persistent filesystem.\"\"\" with self.file_system.open(file_name, \"w\") as my_file: my_file.write(line) def read_file(self, file_name): \"\"\"Read the contents of a file in the Skills persistent filesystem.\"\"\" with self.file_system.open(file_name, \"r\") as my_file: return my_file.read()","title":"Reading and writing to files"},{"location":"skill_filesystem/#check-if-a-file-exists","text":"Quick method to see if some file exists in the namespaced directory. Example: file_name = \"example.txt\" with self.file_system.open(file_name, \"w\") as my_file: my_file.write(\"Hello world\") self.log.info(self.file_system.exists(file_name)) # True self.log.info(self.file_system.exists(\"new.txt\")) # False","title":"Check if a file exists"},{"location":"skill_filesystem/#get-the-path-of-the-namespaced-directory","text":"self.file_system.path is a member value containing the root path of the namespace. However, it is recommended that you use the self.file_system.open() method to read and write files. Example: from ovos_workshop.skills import OVOSSkill class FileSystemSkill(OVOSSkill): def initialize(self): \"\"\"Log the path of this Skills persistent namespace.\"\"\" self.log.info(self.file_system.path)","title":"Get the path of the namespaced directory."},{"location":"skill_filesystem/#create-subdirectories","text":"Now that we have the path of our namespaced filesystem, we can organize our files however we like within that directory. In this example, we create a subdirectory called \"cache\", then write to a text file inside of it. from os import mkdir from os.path import join from ovos_workshop.skills import OVOSSkill class FileSystemSkill(OVOSSkill): def initialize(self): \"\"\"Create a cache subdirectory and write to a file inside it\"\"\" cache_dir = \"cache\" file_name = \"example.txt\" if not self.file_system.exists(cache_dir): mkdir(join(self.file_system.path, cache_dir)) with self.file_system.open(join(cache_dir, file_name), \"w\") as my_file: my_file.write('hello')","title":"Create subdirectories"},{"location":"skill_filesystem/#example-skill","text":"from ovos_workshop.skills import OVOSSkill from ovos_workshop.decorators import intent_handler class FileSystemSkill(OVOSSkill): def initialize(self): \"\"\"Perform initial setup for the Skill. For this example we do four things: 1. Log the path of this directory. 2. Write to a file in the directory. 3. Check that our file exists. 4. Read the contents of our file from disk. \"\"\" file_name = \"example.txt\" self.log.info(self.file_system.path) self.write_line_to_file(file_name, \"hello world\") self.log.info(self.file_system.exists(file_name)) self.log.info(self.read_file(file_name)) def write_line_to_file(self, file_name, line): \"\"\"Write a single line to a file in the Skills persistent filesystem.\"\"\" with self.file_system.open(file_name, \"w\") as my_file: my_file.write(line) def read_file(self, file_name): \"\"\"Read the contents of a file in the Skills persistent filesystem.\"\"\" with self.file_system.open(file_name, \"r\") as my_file: return my_file.read()","title":"Example Skill"},{"location":"skill_filesystem/#temporary-cache","text":"Skills can create a directory for caching temporary data to speed up performance. This directory will likely be part of a small RAM disk and may be cleared at any time. So code that uses these cached files must be able to fall back and regenerate the file.","title":"Temporary Cache"},{"location":"skill_filesystem/#example-skill_1","text":"from os.path import join from ovos_workshop.skills import OVOSSkill from ovos_workshop.decorators import intent_handler from ovos_utils.file_utils import get_cache_directory class CachingSkill(OVOSSkill): def initialize(self): \"\"\"Perform initial setup for the Skill. For this example we do four things: 1. Get a cache directory namespaced for our Skill. 2. Define a file path for the cache_file. 3. Write some data to the cache_file 4. Log the path of the cache_file 4. Log the contents of the cache_file. \"\"\" cache_dir = get_cache_directory('CachingSkill') self.cache_file = join(cache_dir, \"myfile.txt\") self.cache_data() self.log.info(self.cache_file) self.log.info(self.read_cached_data()) def cache_data(self): with open(self.cache_file, \"w\") as cache_file: cache_file.write(\"Some cached data\") def read_cached_data(self): with open(self.cache_file, \"r\") as cache_file: return cache_file.read()","title":"Example Skill"},{"location":"skill_filesystem/#skill-root-directory","text":"self.root_dir This member variable contains the absolute path of a Skill\u2019s root directory e.g. ~.local/share/mycroft/skills/my-skill.me/ . Generally Skills should not modify anything within this directory. Modifying anything in the Skill directory will reload the Skill. It is also not guaranteed that the Skill will have permission to write to this directory.","title":"Skill Root Directory"},{"location":"skill_gui/","text":"GUIInterface Any component wanting to implement a GUI for OpenVoiceOS can do so via the GUIInterface class from ovos-bus-client Sending custom pages from skills requires skill to explicitly support a client platform class GUIInterface: \"\"\" Interface to the Graphical User Interface, allows interaction with the mycroft-gui from anywhere Values set in this class are synced to the GUI, accessible within QML via the built-in sessionData mechanism. For example, in Python you can write in a skill: self.gui['temp'] = 33 self.gui.show_page('Weather') Then in the Weather.qml you'd access the temp via code such as: text: sessionData.time \"\"\" in OVOS Skills self.gui provides a GUIInterface under self.skill_id namespace Page Templates To have a unified look and feel, and to allow simple UIs to be integrated into skills without UI framework knowledge, the GUIInterface provides page templates A page template is a ui file, like QML or html, that is used by gui clients to render the info provided by ovos-gui . Skills may provide their own pages, for example for QT Voice Apps , but is their responsibility to explicitly support individual gui client apps if not using a provided template Text Display simple strings of text. self.gui.show_text(self, text, title=None, override_idle=None, override_animations=False) Arguments: text (str): Main text content. It will auto-paginate title (str): A title to display above the text content. override_idle (boolean, int): True: Takes over the resting page indefinitely (int): Delays resting page for the specified number of seconds. override_animations (boolean): True: Disables showing all platform skill animations. False: 'Default' always show animations. Static Image Display a static image such as a jpeg or png. self.gui.show_image(self, url, caption=None, title=None, fill=None, override_idle=None, override_animations=False) Arguments: url (str): Pointer to the image caption (str): A caption to show under the image title (str): A title to display above the image content fill (str): Fill type - supports: 'PreserveAspectFit', 'PreserveAspectCrop', 'Stretch' override_idle (boolean, int): True: Takes over the resting page indefinitely (int): Delays resting page for the specified number of seconds. override_animations (boolean): True: Disables showing all platform skill animations. False: 'Default' always show animations. Animated Image Display an animated image such as a gif. self.gui.show_animated_image(self, url, caption=None, title=None, fill=None, override_idle=None, override_animations=False) Arguments: url (str): Pointer to the .gif image caption (str): A caption to show under the image title (str): A title to display above the image content fill (str): Fill type - supports: 'PreserveAspectFit', 'PreserveAspectCrop', 'Stretch' override_idle (boolean, int): True: Takes over the resting page indefinitely (int): Delays resting page for the specified number of seconds. override_animations (boolean): True: Disables showing all platform skill animations. False: 'Default' always show animations. HTML Page Display a local HTML page. self.gui.show_html(self, html, resource_url=None, override_idle=None, override_animations=False) Arguments: html (str): HTML text to display resource_url (str): Pointer to HTML resources override_idle (boolean, int): True: Takes over the resting page indefinitely (int): Delays resting page for the specified number of seconds. override_animations (boolean): True: Disables showing all platform skill animations. False: 'Default' always show animations. Remote URL Display a webpage. self.gui.show_url(self, url, override_idle=None, override_animations=False) Arguments: url (str): URL to render override_idle (boolean, int): True: Takes over the resting page indefinitely (int): Delays resting page for the specified number of seconds. override_animations (boolean): True: Disables showing all platform skill animations. False: 'Default' always show animations.","title":"GUI Interface"},{"location":"skill_gui/#guiinterface","text":"Any component wanting to implement a GUI for OpenVoiceOS can do so via the GUIInterface class from ovos-bus-client Sending custom pages from skills requires skill to explicitly support a client platform class GUIInterface: \"\"\" Interface to the Graphical User Interface, allows interaction with the mycroft-gui from anywhere Values set in this class are synced to the GUI, accessible within QML via the built-in sessionData mechanism. For example, in Python you can write in a skill: self.gui['temp'] = 33 self.gui.show_page('Weather') Then in the Weather.qml you'd access the temp via code such as: text: sessionData.time \"\"\" in OVOS Skills self.gui provides a GUIInterface under self.skill_id namespace","title":"GUIInterface"},{"location":"skill_gui/#page-templates","text":"To have a unified look and feel, and to allow simple UIs to be integrated into skills without UI framework knowledge, the GUIInterface provides page templates A page template is a ui file, like QML or html, that is used by gui clients to render the info provided by ovos-gui . Skills may provide their own pages, for example for QT Voice Apps , but is their responsibility to explicitly support individual gui client apps if not using a provided template","title":"Page Templates"},{"location":"skill_gui/#text","text":"Display simple strings of text. self.gui.show_text(self, text, title=None, override_idle=None, override_animations=False) Arguments: text (str): Main text content. It will auto-paginate title (str): A title to display above the text content. override_idle (boolean, int): True: Takes over the resting page indefinitely (int): Delays resting page for the specified number of seconds. override_animations (boolean): True: Disables showing all platform skill animations. False: 'Default' always show animations.","title":"Text"},{"location":"skill_gui/#static-image","text":"Display a static image such as a jpeg or png. self.gui.show_image(self, url, caption=None, title=None, fill=None, override_idle=None, override_animations=False) Arguments: url (str): Pointer to the image caption (str): A caption to show under the image title (str): A title to display above the image content fill (str): Fill type - supports: 'PreserveAspectFit', 'PreserveAspectCrop', 'Stretch' override_idle (boolean, int): True: Takes over the resting page indefinitely (int): Delays resting page for the specified number of seconds. override_animations (boolean): True: Disables showing all platform skill animations. False: 'Default' always show animations.","title":"Static Image"},{"location":"skill_gui/#animated-image","text":"Display an animated image such as a gif. self.gui.show_animated_image(self, url, caption=None, title=None, fill=None, override_idle=None, override_animations=False) Arguments: url (str): Pointer to the .gif image caption (str): A caption to show under the image title (str): A title to display above the image content fill (str): Fill type - supports: 'PreserveAspectFit', 'PreserveAspectCrop', 'Stretch' override_idle (boolean, int): True: Takes over the resting page indefinitely (int): Delays resting page for the specified number of seconds. override_animations (boolean): True: Disables showing all platform skill animations. False: 'Default' always show animations.","title":"Animated Image"},{"location":"skill_gui/#html-page","text":"Display a local HTML page. self.gui.show_html(self, html, resource_url=None, override_idle=None, override_animations=False) Arguments: html (str): HTML text to display resource_url (str): Pointer to HTML resources override_idle (boolean, int): True: Takes over the resting page indefinitely (int): Delays resting page for the specified number of seconds. override_animations (boolean): True: Disables showing all platform skill animations. False: 'Default' always show animations.","title":"HTML Page"},{"location":"skill_gui/#remote-url","text":"Display a webpage. self.gui.show_url(self, url, override_idle=None, override_animations=False) Arguments: url (str): URL to render override_idle (boolean, int): True: Takes over the resting page indefinitely (int): Delays resting page for the specified number of seconds. override_animations (boolean): True: Disables showing all platform skill animations. False: 'Default' always show animations.","title":"Remote URL"},{"location":"skill_json/","text":"skill.json Skills can optionally include a skill.json file in their locale folder WARNING skills in the wild might include an earlier and deprecated version of skill.json in the root of the skill directory This file contains metadata about the skill and all info about how to install and display the skill entry { \"skill_id\": \"skill-XXX.exampleauthor\", \"source\": \"https://github.com/ExampleAuthor/skill-XXX\", \"package_name\": \"ovos-skill-XXX\", \"pip_spec\": \"git+https://github.com/ExampleAuthor/skill-XXX@BRANCH\", \"license\": \"Apache-2.0\", \"author\": \"ExampleAuthor\", \"extra_plugins\": { \"core\": [\"ovos-utterance-transformer-XXX\"], \"PHAL\": [\"ovos-PHAL-XXX\"], \"listener\": [\"ovos-audio-transformer-XXX\", \"ovos-ww-plugin-XXX\", \"ovos-vad-plugin-XXX\", \"ovos-stt-plugin-XXX\"], \"audio\": [\"ovos-dialog-transformer-XXX\",\"ovos-tts-transformer-XXX\", \"ovos-tts-plugin-XXX\"], \"media\": [\"ovos-ocp-XXX\", \"ovos-media-XXX\"], \"gui\": [\"ovos-gui-extension-XXX\"] }, \"icon\": \"http://example_icon.svg\", \"images\": [\"http://example_logo.png\", \"http://example_screenshot.png\"], \"name\": \"My Skill\", \"description\": \"Does awesome skill stuff!\", \"examples\": [ \"do the thing\", \"say this to use the skill\" ], \"tags\": [\"productivity\", \"entertainment\", \"aliens\"] } EXPERIMENTAL this specification might change, it is still at the proposal stage and open for feedback Spec On the JSON file: skill_id Required unique skill identifier, this is defined in setup.py and by convention is usually repo.author (lower-cased) source Optional git url to download the source code, a skill can be installed from source with pip install git+{source} package_name Required package name of the skill, if the skill is on pypi it can be installed with pip install {package_name} pip_spec Optional PEP 508 install spec to install the skill. pip install {pip_spec} license Optional SPDX License ID author Optional string author name. Overrides any name extracted from skill_id or source URL extra_plugins Optional list of python requirements that are not direct dependencies of the skill but should be installed in a companion OVOS service name Optional human-readable name for the skill to be used in store listings, UI, etc. Overrides any name extracted from skill_id or source URL description Optional human-readable short description of the skill. This should be limited to one or two sentences max examples Optional list of example utterances that this skill should handle tags Optional list of arbitrary labels and categories, helps search results to find this skill icon Optional skill icon to display images Optional list of images to showcase the skill LANG SUPPORT : include skill.json in each lang subfolder of your locale skill directory, this signals language support and allows translation of name , description , examples and tags Installation Installation of a skill should generally follow the priority of <pip_spec> > <package_name> > git+<source> . At least one of these MUST be a valid installation option. A minimal exmple of skill installation in Python is included for reference only : import pip import json with open(\"skill.json\") as f: skill_spec = json.load(f) specs = [skill_spec.get('pip_spec')] if skill_spec.get('pip_spec') else [] specs.append(skill_spec['package_name'] if skill_spec.get('source'): specs.append(f\"git+{skill_spec['source']}\") for spec in specs: if pip.main(['install', spec) == 0: # Successful installation break Note that this example does not account for existing packages and using it on its own could result in broken dependencies.","title":"Skill Json"},{"location":"skill_json/#skilljson","text":"Skills can optionally include a skill.json file in their locale folder WARNING skills in the wild might include an earlier and deprecated version of skill.json in the root of the skill directory This file contains metadata about the skill and all info about how to install and display the skill entry { \"skill_id\": \"skill-XXX.exampleauthor\", \"source\": \"https://github.com/ExampleAuthor/skill-XXX\", \"package_name\": \"ovos-skill-XXX\", \"pip_spec\": \"git+https://github.com/ExampleAuthor/skill-XXX@BRANCH\", \"license\": \"Apache-2.0\", \"author\": \"ExampleAuthor\", \"extra_plugins\": { \"core\": [\"ovos-utterance-transformer-XXX\"], \"PHAL\": [\"ovos-PHAL-XXX\"], \"listener\": [\"ovos-audio-transformer-XXX\", \"ovos-ww-plugin-XXX\", \"ovos-vad-plugin-XXX\", \"ovos-stt-plugin-XXX\"], \"audio\": [\"ovos-dialog-transformer-XXX\",\"ovos-tts-transformer-XXX\", \"ovos-tts-plugin-XXX\"], \"media\": [\"ovos-ocp-XXX\", \"ovos-media-XXX\"], \"gui\": [\"ovos-gui-extension-XXX\"] }, \"icon\": \"http://example_icon.svg\", \"images\": [\"http://example_logo.png\", \"http://example_screenshot.png\"], \"name\": \"My Skill\", \"description\": \"Does awesome skill stuff!\", \"examples\": [ \"do the thing\", \"say this to use the skill\" ], \"tags\": [\"productivity\", \"entertainment\", \"aliens\"] } EXPERIMENTAL this specification might change, it is still at the proposal stage and open for feedback","title":"skill.json"},{"location":"skill_json/#spec","text":"On the JSON file: skill_id Required unique skill identifier, this is defined in setup.py and by convention is usually repo.author (lower-cased) source Optional git url to download the source code, a skill can be installed from source with pip install git+{source} package_name Required package name of the skill, if the skill is on pypi it can be installed with pip install {package_name} pip_spec Optional PEP 508 install spec to install the skill. pip install {pip_spec} license Optional SPDX License ID author Optional string author name. Overrides any name extracted from skill_id or source URL extra_plugins Optional list of python requirements that are not direct dependencies of the skill but should be installed in a companion OVOS service name Optional human-readable name for the skill to be used in store listings, UI, etc. Overrides any name extracted from skill_id or source URL description Optional human-readable short description of the skill. This should be limited to one or two sentences max examples Optional list of example utterances that this skill should handle tags Optional list of arbitrary labels and categories, helps search results to find this skill icon Optional skill icon to display images Optional list of images to showcase the skill LANG SUPPORT : include skill.json in each lang subfolder of your locale skill directory, this signals language support and allows translation of name , description , examples and tags","title":"Spec"},{"location":"skill_json/#installation","text":"Installation of a skill should generally follow the priority of <pip_spec> > <package_name> > git+<source> . At least one of these MUST be a valid installation option. A minimal exmple of skill installation in Python is included for reference only : import pip import json with open(\"skill.json\") as f: skill_spec = json.load(f) specs = [skill_spec.get('pip_spec')] if skill_spec.get('pip_spec') else [] specs.append(skill_spec['package_name'] if skill_spec.get('source'): specs.append(f\"git+{skill_spec['source']}\") for spec in specs: if pip.main(['install', spec) == 0: # Successful installation break Note that this example does not account for existing packages and using it on its own could result in broken dependencies.","title":"Installation"},{"location":"skill_settings/","text":"Skill Settings Skill settings provide the ability for users to configure a Skill using the command line or a web-based interface. This is often used to: Change default behaviors - such as the sound used for users alarms. Authenticate with external services - such as Spotify Enter longer data as text rather than by voice - such as the IP address of the users Home Assistant server. Skill settings are completely optional. Using settings in your Skill Settings can be managed via command line, shipped by images or managed via a backend if ovos-core us configured to use one When settings are fetched from the backend server, they are saved into a settings.json file. This file is automatically created when a Skill is loaded even if the Skill does not have any settings. Your Skill then accesses the settings from this file. Nowadays, the file is located in the Skill's XDG_CONFIG_DIR (usually ~/config/mycroft/skills/<skillname> ), however if a settings.json file already exists in the Skill's root directory (the deprecated location) that location is used for compatibility. Reading settings Skill settings are available on the OVOSSkill class and inherit from a Python Dict. This means that you can use it just like you would any other Python dictionary. To access the show_time variable from our example above we would use the Dict.get method: self.settings.get('show_time') If the setting we are trying to access is not available, the get method will return None . Instead of assigning this to a variable and then testing for None , we can provide a default value as the second argument to the get method. self.settings.get('show_time', False) In this example, if the settings have not been received, or the show_time setting has not been assigned, it will return the default value False . A few warnings We recommend using the Dict.get method above rather than accessing the setting directly with: self.settings['show_time'] Directly referencing the value may throw a KeyError if the setting has not yet been fetched from the server. It is also important to note that the settings dictionary will not be available in your Skills __init__ method as this is setting up your Skills Class. You should instead use an initialize method which is called after the Skill is fully constructed and registered with the system. Handling settings changes OVOS will check for updates to a users settings regularly, both locally and on the configured backend. To perform some action when settings are updated, you can register a callback function in your Skill. def initialize(self): self.settings_change_callback = self.on_settings_changed self.on_settings_changed() def on_settings_changed(self): show_time = self.settings.get('show_time', False) self.trigger_time_display(show_time) In the example above, we have registered the on_settings_changed method to be our callback function. We have then immediately called the method to perform the relevant actions when the Skill is being initialized even though the Skills settings have not changed. In the on_settings_changed method we have assigned the value of the show_time setting to a local variable. Then we have passed it as an argument to another method in our Skill that will trigger the display of the time based on its value. Writing to settings Your Skill can reassign a setting locally. To do this we assign a value like you would with any other dictionary key. self.settings['show_time'] = True The new value for the show_time setting will persist until a new setting is assigned locally by the Skill, or remotely if the user configured a backend Define settings UI for a Skill To define our Skills settings UI we use a settingsmeta.json or settingsmeta.yaml file. This file must be in the root directory of the Skill and must follow a specific structure. Once settings have been defined using a settingsmeta file, they will be presented to the user in the configured backend or helper application Example settingsmeta file To see it in action, lets look at a simple example from the Date-Time Skill . First using the JSON syntax as a settingsmeta.json file: { \"skillMetadata\": { \"sections\": [ { \"name\": \"Display\", \"fields\": [ { \"name\": \"show_time\", \"type\": \"checkbox\", \"label\": \"Show digital clock when idle\", \"value\": \"true\" } ] } ] } } Now, here is the same settings, as it would be defined with YAML in a settingsmeta.yaml file: skillMetadata: sections: - name: Display fields: - name: show_time type: checkbox label: Show digital clock when idle value: \"true\" Notice that the value of false is surrounded by \"quotation marks\". This is because OVOS expects a string of \"true\" or \"false\" rather than a Boolean. Both of these files would result in the same settings block. It is up to your personal preference which syntax you choose. Structure of the settingsmeta file Whilst the syntax differs, the structure of these two filetypes is the same. This starts at the top level of the file by defining a skillMetadata object. This object must contain one or more sections elements. Sections Each section represents a group of settings that logically sit together. This enables us to display the settings more clearly in the web interface for users. In the simple example above we have just one section. However, the Spotify Skill settings contains two sections. The first is for Spotify Account authentication, and the second section contains settings to define your default playback device. Each section must contain a name attribute that is used as the heading for that section, and an Array of fields . Fields Each section has one or more fields . Each field is a setting available to the user. Each field takes four properties: name (String) The name of the field is used by the Skill to get and set the value of the field . It will not usually be displayed to the user, unless the label property has not been set. * type (Enum) The data type of this field. The supported types are: text : any kind of text email : text validated as an email address checkbox : boolean, True or False number : text validated as a number password : text hidden from view by default select : a drop-down menu of options label : special field to display text for information purposes only. No name or value is required for a label field. label (String) The text to be displayed above the setting field. * value (String) The initial value of the field. Examples for each type of field are provided in JSON and YAML at the end of this page. SettingsMeta Examples Label Field skillMetadata: sections: - name: Label Field Example fields: - type: label label: This is descriptive text. Text Field skillMetadata: sections: - name: Text Field Example fields: - name: my_string type: text label: Enter any text value: Email skillMetadata: sections: - name: Email Field Example fields: - name: my_email_address type: email label: Enter your email address value: Checkbox skillMetadata: sections: - name: Checkbox Field Example fields: - name: my_boolean type: checkbox label: This is an example checkbox. It creates a Boolean value. value: \"false\" Number skillMetadata: sections: - name: Number Field Example fields: - name: my_number type: number label: Enter any number value: 7 Password skillMetadata: sections: - name: Password Field Example fields: - name: my_password type: password label: Enter your password value: Select skillMetadata: sections: - name: Select Field Example fields: - name: my_selected_option type: select label: Select an option options: Option 1|option_one;Option 2|option_two;Option 3|option_three value: option_one","title":"Skill Settings"},{"location":"skill_settings/#skill-settings","text":"Skill settings provide the ability for users to configure a Skill using the command line or a web-based interface. This is often used to: Change default behaviors - such as the sound used for users alarms. Authenticate with external services - such as Spotify Enter longer data as text rather than by voice - such as the IP address of the users Home Assistant server. Skill settings are completely optional.","title":"Skill Settings"},{"location":"skill_settings/#using-settings-in-your-skill","text":"Settings can be managed via command line, shipped by images or managed via a backend if ovos-core us configured to use one When settings are fetched from the backend server, they are saved into a settings.json file. This file is automatically created when a Skill is loaded even if the Skill does not have any settings. Your Skill then accesses the settings from this file. Nowadays, the file is located in the Skill's XDG_CONFIG_DIR (usually ~/config/mycroft/skills/<skillname> ), however if a settings.json file already exists in the Skill's root directory (the deprecated location) that location is used for compatibility.","title":"Using settings in your Skill"},{"location":"skill_settings/#reading-settings","text":"Skill settings are available on the OVOSSkill class and inherit from a Python Dict. This means that you can use it just like you would any other Python dictionary. To access the show_time variable from our example above we would use the Dict.get method: self.settings.get('show_time') If the setting we are trying to access is not available, the get method will return None . Instead of assigning this to a variable and then testing for None , we can provide a default value as the second argument to the get method. self.settings.get('show_time', False) In this example, if the settings have not been received, or the show_time setting has not been assigned, it will return the default value False . A few warnings We recommend using the Dict.get method above rather than accessing the setting directly with: self.settings['show_time'] Directly referencing the value may throw a KeyError if the setting has not yet been fetched from the server. It is also important to note that the settings dictionary will not be available in your Skills __init__ method as this is setting up your Skills Class. You should instead use an initialize method which is called after the Skill is fully constructed and registered with the system.","title":"Reading settings"},{"location":"skill_settings/#handling-settings-changes","text":"OVOS will check for updates to a users settings regularly, both locally and on the configured backend. To perform some action when settings are updated, you can register a callback function in your Skill. def initialize(self): self.settings_change_callback = self.on_settings_changed self.on_settings_changed() def on_settings_changed(self): show_time = self.settings.get('show_time', False) self.trigger_time_display(show_time) In the example above, we have registered the on_settings_changed method to be our callback function. We have then immediately called the method to perform the relevant actions when the Skill is being initialized even though the Skills settings have not changed. In the on_settings_changed method we have assigned the value of the show_time setting to a local variable. Then we have passed it as an argument to another method in our Skill that will trigger the display of the time based on its value.","title":"Handling settings changes"},{"location":"skill_settings/#writing-to-settings","text":"Your Skill can reassign a setting locally. To do this we assign a value like you would with any other dictionary key. self.settings['show_time'] = True The new value for the show_time setting will persist until a new setting is assigned locally by the Skill, or remotely if the user configured a backend","title":"Writing to settings"},{"location":"skill_settings/#define-settings-ui-for-a-skill","text":"To define our Skills settings UI we use a settingsmeta.json or settingsmeta.yaml file. This file must be in the root directory of the Skill and must follow a specific structure. Once settings have been defined using a settingsmeta file, they will be presented to the user in the configured backend or helper application","title":"Define settings UI for a Skill"},{"location":"skill_settings/#example-settingsmeta-file","text":"To see it in action, lets look at a simple example from the Date-Time Skill . First using the JSON syntax as a settingsmeta.json file: { \"skillMetadata\": { \"sections\": [ { \"name\": \"Display\", \"fields\": [ { \"name\": \"show_time\", \"type\": \"checkbox\", \"label\": \"Show digital clock when idle\", \"value\": \"true\" } ] } ] } } Now, here is the same settings, as it would be defined with YAML in a settingsmeta.yaml file: skillMetadata: sections: - name: Display fields: - name: show_time type: checkbox label: Show digital clock when idle value: \"true\" Notice that the value of false is surrounded by \"quotation marks\". This is because OVOS expects a string of \"true\" or \"false\" rather than a Boolean. Both of these files would result in the same settings block. It is up to your personal preference which syntax you choose.","title":"Example settingsmeta file"},{"location":"skill_settings/#structure-of-the-settingsmeta-file","text":"Whilst the syntax differs, the structure of these two filetypes is the same. This starts at the top level of the file by defining a skillMetadata object. This object must contain one or more sections elements.","title":"Structure of the settingsmeta file"},{"location":"skill_settings/#sections","text":"Each section represents a group of settings that logically sit together. This enables us to display the settings more clearly in the web interface for users. In the simple example above we have just one section. However, the Spotify Skill settings contains two sections. The first is for Spotify Account authentication, and the second section contains settings to define your default playback device. Each section must contain a name attribute that is used as the heading for that section, and an Array of fields .","title":"Sections"},{"location":"skill_settings/#fields","text":"Each section has one or more fields . Each field is a setting available to the user. Each field takes four properties: name (String) The name of the field is used by the Skill to get and set the value of the field . It will not usually be displayed to the user, unless the label property has not been set. * type (Enum) The data type of this field. The supported types are: text : any kind of text email : text validated as an email address checkbox : boolean, True or False number : text validated as a number password : text hidden from view by default select : a drop-down menu of options label : special field to display text for information purposes only. No name or value is required for a label field. label (String) The text to be displayed above the setting field. * value (String) The initial value of the field. Examples for each type of field are provided in JSON and YAML at the end of this page.","title":"Fields"},{"location":"skill_settings/#settingsmeta-examples","text":"","title":"SettingsMeta Examples"},{"location":"skill_settings/#label-field","text":"skillMetadata: sections: - name: Label Field Example fields: - type: label label: This is descriptive text.","title":"Label Field"},{"location":"skill_settings/#text-field","text":"skillMetadata: sections: - name: Text Field Example fields: - name: my_string type: text label: Enter any text value:","title":"Text Field"},{"location":"skill_settings/#email","text":"skillMetadata: sections: - name: Email Field Example fields: - name: my_email_address type: email label: Enter your email address value:","title":"Email"},{"location":"skill_settings/#checkbox","text":"skillMetadata: sections: - name: Checkbox Field Example fields: - name: my_boolean type: checkbox label: This is an example checkbox. It creates a Boolean value. value: \"false\"","title":"Checkbox"},{"location":"skill_settings/#number","text":"skillMetadata: sections: - name: Number Field Example fields: - name: my_number type: number label: Enter any number value: 7","title":"Number"},{"location":"skill_settings/#password","text":"skillMetadata: sections: - name: Password Field Example fields: - name: my_password type: password label: Enter your password value:","title":"Password"},{"location":"skill_settings/#select","text":"skillMetadata: sections: - name: Select Field Example fields: - name: my_selected_option type: select label: Select an option options: Option 1|option_one;Option 2|option_two;Option 3|option_three value: option_one","title":"Select"},{"location":"skill_structure/","text":"Anatomy of a Skill vocab , dialog , and locale directories The dialog , vocab , and locale directories contain subdirectories for each spoken language the skill supports. The subdirectories are named using the IETF language tag for the language. For example, Brazilian Portuguese is 'pt-br', German is 'de-de', and Australian English is 'en-au'. dialog and vocab have been deprecated , they are still supported, but we strongly recommend you use locale for new skills inside the locale folder you will find subfolders for each language (e.g. en-us ), often all you need to do in order to translate a skill is adding a new folder for your language here each language folder can have the structure it wants, you may see files grouped by type in subfolder or all in the base folder You will find several unfamiliar file extensions in this folder, but these are simple text files .dialog files used for defining speech responses .intent files used for defining Padatious Intents .voc files define keywords primarily used in Adapt Intents .entity files define a named entity primarily used in Padatious Intents init .py The __init__.py file is where most of the Skill is defined using Python code. Importing libraries from ovos_workshop.intents import IntentBuilder from ovos_workshop.decorators import intent_handler from ovos_workshop.skills import OVOSSkill This section of code imports the required libraries . Some libraries will be required on every Skill, and your skill may need to import additional libraries. Class definition The class definition extends the OVOSSkill class: class HelloWorldSkill(OVOSSkill): The class should be named logically, for example \"TimeSkill\", \"WeatherSkill\", \"NewsSkill\", \"IPaddressSkill\". If you would like guidance on what to call your Skill, please join the skills Channel on OVOS Chat . Inside the class, methods are then defined. init () This method is the constructor . It is called when the Skill is first constructed. It is often used to declare state variables or perform setup actions, however it cannot fully utilise OVOSSkill methods as the skill is not fully initialized yet at this point. You usually don't have to include the constructor. An example __init__ method might be: def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) self.already_said_hello = False self.be_friendly = True __init__ method must accept at least skill_id and bus kwargs and pass them to super() , we recommend passing *args, **kwargs like in example above instead NOTE : self.skill_id , self.filesystem , self.settings , self.bus are only available after the call to super() , if you need them consider using initialize instead initialize() This method is called during __init__ , if you implemented __init__ in your skill it will be called during super() Perform any final setup needed for the skill here. This function is invoked after the skill is fully constructed and registered with the system. Intents will be registered and Skill settings will be available. If you need to access self.skill_id , self.bus , self.settings or self.filesystem you must do it here instead of __init__ def initialize(self): my_setting = self.settings.get('my_setting') @intent_handler We can use the initialize function to manually register intents, however the @intent_handler decorator is a cleaner way to achieve this. We will learn all about the different Intents shortly. In skills we can see two different intent styles. An Adapt handler, triggered by a keyword defined in a ThankYouKeyword.voc file. @intent_handler(IntentBuilder('ThankYouIntent').require('ThankYouKeyword')) def handle_thank_you_intent(self, message): self.speak_dialog(\"welcome\") A Padatious intent handler, triggered using a list of sample phrases. @intent_handler('HowAreYou.intent') def handle_how_are_you_intent(self, message): self.speak_dialog(\"how.are.you\") In both cases, the function receives two parameters : self - a reference to the HelloWorldSkill object itself message - an incoming message from the messagebus . Both intents call the self.speak_dialog() method, passing the name of a dialog file to it. In this case welcome.dialog and how.are.you.dialog . stop() You will usually also have a stop() method. The stop method is called anytime a User says \"Stop\" or a similar command. It is useful for stopping any output or process that a User might want to end without needing to issue a Skill specific utterance such as media playback or an expired alarm notification. In the following example, we call a method stop_beeping to end a notification that our Skill has created. If the skill \"consumed\" the stop signal it should return True, else return False. def stop(self): if self.beeping: self.stop_beeping() return True return False If a Skill has any active functionality, the stop() method should terminate the functionality, leaving the Skill in a known good state. When the skill returns True no other skill will be stopped, when it returns False the next active skill will attempt to stop and so on until something consumes the stop signal shutdown() The shutdown method is called during the Skill process termination. It is used to perform any final actions to ensure all processes and operations in execution are stopped safely. This might be particularly useful for Skills that have scheduled future events, may be writing to a file or database, or that have initiated new processes. In the following example we cancel a scheduled event and call a method in our Skill to stop a subprocess we initiated. def shutdown(self): self.cancel_scheduled_event('my_event') self.stop_my_subprocess() settingsmeta.yaml This file defines the settings UI that will be available to a User through a backend or companion app Jump to Skill Settings for more information on this file and handling of Skill settings. setup.py This file allows a skill to be installed just like any other python package. This means you can publish your skill on pypi or favorite package manager and use it as a dependency A typical setup.py file looks like this #!/usr/bin/env python3 from setuptools import setup import os from os import walk, path # update this! URL = \"https://github.com/OpenVoiceOS/skill-awesome-stuff\" SKILL_CLAZZ = \"AwesomeSkill\" # needs to match __init__.py class name PYPI_NAME = \"skill-awesome-stuff\" # pip install PYPI_NAME # below derived from github url to ensure standard skill_id SKILL_AUTHOR, SKILL_NAME = URL.split(\".com/\")[-1].split(\"/\") SKILL_PKG = SKILL_NAME.lower().replace('-', '_') PLUGIN_ENTRY_POINT = f'{SKILL_NAME.lower()}.{SKILL_AUTHOR.lower()}={SKILL_PKG}:{SKILL_CLAZZ}' # skill_id=package_name:SkillClass def get_requirements(requirements_filename: str): requirements_file = path.join(path.abspath(path.dirname(__file__)), requirements_filename) with open(requirements_file, 'r', encoding='utf-8') as r: requirements = r.readlines() requirements = [r.strip() for r in requirements if r.strip() and not r.strip().startswith(\"#\")] if 'MYCROFT_LOOSE_REQUIREMENTS' in os.environ: print('USING LOOSE REQUIREMENTS!') requirements = [r.replace('==', '>=').replace('~=', '>=') for r in requirements] return requirements def find_resource_files(): # add any folder with files your skill uses here! resource_base_dirs = (\"locale\", \"ui\", \"vocab\", \"dialog\", \"regex\", \"skill\") base_dir = path.dirname(__file__) package_data = [\"*.json\"] for res in resource_base_dirs: if path.isdir(path.join(base_dir, res)): for (directory, _, files) in walk(path.join(base_dir, res)): if files: package_data.append( path.join(directory.replace(base_dir, \"\").lstrip('/'), '*')) return package_data # TODO - add description, author, email, license, etc setup( # this is the package name that goes on pip name=PYPI_NAME, version=\"0.0.0\", url=URL, license='Apache-2.0', package_dir={SKILL_PKG: \"\"}, package_data={SKILL_PKG: find_resource_files()}, packages=[SKILL_PKG], include_package_data=True, install_requires=get_requirements(\"requirements.txt\"), keywords='ovos skill plugin', entry_points={'ovos.plugin.skill': PLUGIN_ENTRY_POINT} )","title":"Anatomy"},{"location":"skill_structure/#anatomy-of-a-skill","text":"","title":"Anatomy of a Skill"},{"location":"skill_structure/#vocab-dialog-and-locale-directories","text":"The dialog , vocab , and locale directories contain subdirectories for each spoken language the skill supports. The subdirectories are named using the IETF language tag for the language. For example, Brazilian Portuguese is 'pt-br', German is 'de-de', and Australian English is 'en-au'. dialog and vocab have been deprecated , they are still supported, but we strongly recommend you use locale for new skills inside the locale folder you will find subfolders for each language (e.g. en-us ), often all you need to do in order to translate a skill is adding a new folder for your language here each language folder can have the structure it wants, you may see files grouped by type in subfolder or all in the base folder You will find several unfamiliar file extensions in this folder, but these are simple text files .dialog files used for defining speech responses .intent files used for defining Padatious Intents .voc files define keywords primarily used in Adapt Intents .entity files define a named entity primarily used in Padatious Intents","title":"vocab, dialog, and locale directories"},{"location":"skill_structure/#initpy","text":"The __init__.py file is where most of the Skill is defined using Python code.","title":"init.py"},{"location":"skill_structure/#importing-libraries","text":"from ovos_workshop.intents import IntentBuilder from ovos_workshop.decorators import intent_handler from ovos_workshop.skills import OVOSSkill This section of code imports the required libraries . Some libraries will be required on every Skill, and your skill may need to import additional libraries.","title":"Importing libraries"},{"location":"skill_structure/#class-definition","text":"The class definition extends the OVOSSkill class: class HelloWorldSkill(OVOSSkill): The class should be named logically, for example \"TimeSkill\", \"WeatherSkill\", \"NewsSkill\", \"IPaddressSkill\". If you would like guidance on what to call your Skill, please join the skills Channel on OVOS Chat . Inside the class, methods are then defined.","title":"Class definition"},{"location":"skill_structure/#init","text":"This method is the constructor . It is called when the Skill is first constructed. It is often used to declare state variables or perform setup actions, however it cannot fully utilise OVOSSkill methods as the skill is not fully initialized yet at this point. You usually don't have to include the constructor. An example __init__ method might be: def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) self.already_said_hello = False self.be_friendly = True __init__ method must accept at least skill_id and bus kwargs and pass them to super() , we recommend passing *args, **kwargs like in example above instead NOTE : self.skill_id , self.filesystem , self.settings , self.bus are only available after the call to super() , if you need them consider using initialize instead","title":"init()"},{"location":"skill_structure/#initialize","text":"This method is called during __init__ , if you implemented __init__ in your skill it will be called during super() Perform any final setup needed for the skill here. This function is invoked after the skill is fully constructed and registered with the system. Intents will be registered and Skill settings will be available. If you need to access self.skill_id , self.bus , self.settings or self.filesystem you must do it here instead of __init__ def initialize(self): my_setting = self.settings.get('my_setting')","title":"initialize()"},{"location":"skill_structure/#intent_handler","text":"We can use the initialize function to manually register intents, however the @intent_handler decorator is a cleaner way to achieve this. We will learn all about the different Intents shortly. In skills we can see two different intent styles. An Adapt handler, triggered by a keyword defined in a ThankYouKeyword.voc file. @intent_handler(IntentBuilder('ThankYouIntent').require('ThankYouKeyword')) def handle_thank_you_intent(self, message): self.speak_dialog(\"welcome\") A Padatious intent handler, triggered using a list of sample phrases. @intent_handler('HowAreYou.intent') def handle_how_are_you_intent(self, message): self.speak_dialog(\"how.are.you\") In both cases, the function receives two parameters : self - a reference to the HelloWorldSkill object itself message - an incoming message from the messagebus . Both intents call the self.speak_dialog() method, passing the name of a dialog file to it. In this case welcome.dialog and how.are.you.dialog .","title":"@intent_handler"},{"location":"skill_structure/#stop","text":"You will usually also have a stop() method. The stop method is called anytime a User says \"Stop\" or a similar command. It is useful for stopping any output or process that a User might want to end without needing to issue a Skill specific utterance such as media playback or an expired alarm notification. In the following example, we call a method stop_beeping to end a notification that our Skill has created. If the skill \"consumed\" the stop signal it should return True, else return False. def stop(self): if self.beeping: self.stop_beeping() return True return False If a Skill has any active functionality, the stop() method should terminate the functionality, leaving the Skill in a known good state. When the skill returns True no other skill will be stopped, when it returns False the next active skill will attempt to stop and so on until something consumes the stop signal","title":"stop()"},{"location":"skill_structure/#shutdown","text":"The shutdown method is called during the Skill process termination. It is used to perform any final actions to ensure all processes and operations in execution are stopped safely. This might be particularly useful for Skills that have scheduled future events, may be writing to a file or database, or that have initiated new processes. In the following example we cancel a scheduled event and call a method in our Skill to stop a subprocess we initiated. def shutdown(self): self.cancel_scheduled_event('my_event') self.stop_my_subprocess()","title":"shutdown()"},{"location":"skill_structure/#settingsmetayaml","text":"This file defines the settings UI that will be available to a User through a backend or companion app Jump to Skill Settings for more information on this file and handling of Skill settings.","title":"settingsmeta.yaml"},{"location":"skill_structure/#setuppy","text":"This file allows a skill to be installed just like any other python package. This means you can publish your skill on pypi or favorite package manager and use it as a dependency A typical setup.py file looks like this #!/usr/bin/env python3 from setuptools import setup import os from os import walk, path # update this! URL = \"https://github.com/OpenVoiceOS/skill-awesome-stuff\" SKILL_CLAZZ = \"AwesomeSkill\" # needs to match __init__.py class name PYPI_NAME = \"skill-awesome-stuff\" # pip install PYPI_NAME # below derived from github url to ensure standard skill_id SKILL_AUTHOR, SKILL_NAME = URL.split(\".com/\")[-1].split(\"/\") SKILL_PKG = SKILL_NAME.lower().replace('-', '_') PLUGIN_ENTRY_POINT = f'{SKILL_NAME.lower()}.{SKILL_AUTHOR.lower()}={SKILL_PKG}:{SKILL_CLAZZ}' # skill_id=package_name:SkillClass def get_requirements(requirements_filename: str): requirements_file = path.join(path.abspath(path.dirname(__file__)), requirements_filename) with open(requirements_file, 'r', encoding='utf-8') as r: requirements = r.readlines() requirements = [r.strip() for r in requirements if r.strip() and not r.strip().startswith(\"#\")] if 'MYCROFT_LOOSE_REQUIREMENTS' in os.environ: print('USING LOOSE REQUIREMENTS!') requirements = [r.replace('==', '>=').replace('~=', '>=') for r in requirements] return requirements def find_resource_files(): # add any folder with files your skill uses here! resource_base_dirs = (\"locale\", \"ui\", \"vocab\", \"dialog\", \"regex\", \"skill\") base_dir = path.dirname(__file__) package_data = [\"*.json\"] for res in resource_base_dirs: if path.isdir(path.join(base_dir, res)): for (directory, _, files) in walk(path.join(base_dir, res)): if files: package_data.append( path.join(directory.replace(base_dir, \"\").lstrip('/'), '*')) return package_data # TODO - add description, author, email, license, etc setup( # this is the package name that goes on pip name=PYPI_NAME, version=\"0.0.0\", url=URL, license='Apache-2.0', package_dir={SKILL_PKG: \"\"}, package_data={SKILL_PKG: find_resource_files()}, packages=[SKILL_PKG], include_package_data=True, install_requires=get_requirements(\"requirements.txt\"), keywords='ovos skill plugin', entry_points={'ovos.plugin.skill': PLUGIN_ENTRY_POINT} )","title":"setup.py"},{"location":"skill_tests/","text":"Skill Tests Testing is an important part of skill development. Here are some recommendations for how to implement good tests for a skill. Unit Tests Like any other module, testing each method in you skill can help prevent debugging headaches when something goes wrong. These tests are specific to a skill, and it is up to the skill author to test that methods act as expected. Skill Resource Tests This test case may be used to test skill resource files and event registration. A shared GitHub Action is provided by Neon AI to easily implement automated tests. The following files should be included in the skill repository to use these tests. test/test_resources.yaml This file specifies the expected skill resources, including files, entities, and intent names. An example from the Support Helper Skill is included for reference: # Specify resources to test here. # Specify languages to be tested languages: - \"en-us\" # vocab is lowercase .voc file basenames vocab: [] # dialog is .dialog file basenames (case-sensitive) dialog: - ask_description - cancelled - complete - confirm_support - email_intro - email_signature - email_title - no_email - one_moment - support # regex entities, not necessarily filenames regex: [] intents: # Padatious intents are the `.intent` file names padatious: - contact_support.intent # Adapt intents are the name passed to the constructor adapt: [] .github/workflows/skill_tests.yml The filename isn't important here, but a workflow must specify a job using neongeckocom/.github/.github/workflows/skill_test_resources.yml@master . A minimal example would be: name: Test Skill Resources on: pull_request: workflow_dispatch: jobs: skill_resource_tests: uses: neongeckocom/.github/.github/workflows/skill_test_resources.yml@master Skill Intent Tests This test case may be used to test skill resource files and event registration. A shared GitHub Action is provided by Neon AI to easily implement automated tests. The following files should be included in the skill repository to use these tests. test/test_intents.yaml This file specifies skill intents and utterances that should match those intents. Specific vocabulary and entity matches may also be specified, An example from the IP Address Skill is included for reference: # Specify intents to test here. Valid test cases are as follows: # Basic intent match tests only: #lang: # intent_name: # - example utterance # - other example utterance # Intent tests with expected vocab/entity matches: #lang: # intent_name: # - example_utterance: # - expected vocab name # - other expected vocab name # Intent tests with specific vocab/entity extraction tests: #lang: # intent_name: # - example_utterance: # - expected_vocab_key: expected_vocab_value # - expected_entity_key: expected_entity_value en-us: IPIntent: - what is your ip address - what is my ip address: - IP - what is my i.p. address - What is your I.P. address? - what is my public IP address?: - public: public .github/workflows/skill_tests.yml The filename isn't important here, but a workflow must specify a job using neongeckocom/.github/.github/workflows/skill_test_intents.yml@master . A minimal example would be: name: Test Skill Resources on: pull_request: workflow_dispatch: jobs: skill_intent_tests: uses: neongeckocom/.github/.github/workflows/skill_test_intents.yml@master","title":"Skill Tests"},{"location":"skill_tests/#skill-tests","text":"Testing is an important part of skill development. Here are some recommendations for how to implement good tests for a skill.","title":"Skill Tests"},{"location":"skill_tests/#unit-tests","text":"Like any other module, testing each method in you skill can help prevent debugging headaches when something goes wrong. These tests are specific to a skill, and it is up to the skill author to test that methods act as expected.","title":"Unit Tests"},{"location":"skill_tests/#skill-resource-tests","text":"This test case may be used to test skill resource files and event registration. A shared GitHub Action is provided by Neon AI to easily implement automated tests. The following files should be included in the skill repository to use these tests.","title":"Skill Resource Tests"},{"location":"skill_tests/#testtest_resourcesyaml","text":"This file specifies the expected skill resources, including files, entities, and intent names. An example from the Support Helper Skill is included for reference: # Specify resources to test here. # Specify languages to be tested languages: - \"en-us\" # vocab is lowercase .voc file basenames vocab: [] # dialog is .dialog file basenames (case-sensitive) dialog: - ask_description - cancelled - complete - confirm_support - email_intro - email_signature - email_title - no_email - one_moment - support # regex entities, not necessarily filenames regex: [] intents: # Padatious intents are the `.intent` file names padatious: - contact_support.intent # Adapt intents are the name passed to the constructor adapt: []","title":"test/test_resources.yaml"},{"location":"skill_tests/#githubworkflowsskill_testsyml","text":"The filename isn't important here, but a workflow must specify a job using neongeckocom/.github/.github/workflows/skill_test_resources.yml@master . A minimal example would be: name: Test Skill Resources on: pull_request: workflow_dispatch: jobs: skill_resource_tests: uses: neongeckocom/.github/.github/workflows/skill_test_resources.yml@master","title":".github/workflows/skill_tests.yml"},{"location":"skill_tests/#skill-intent-tests","text":"This test case may be used to test skill resource files and event registration. A shared GitHub Action is provided by Neon AI to easily implement automated tests. The following files should be included in the skill repository to use these tests.","title":"Skill Intent Tests"},{"location":"skill_tests/#testtest_intentsyaml","text":"This file specifies skill intents and utterances that should match those intents. Specific vocabulary and entity matches may also be specified, An example from the IP Address Skill is included for reference: # Specify intents to test here. Valid test cases are as follows: # Basic intent match tests only: #lang: # intent_name: # - example utterance # - other example utterance # Intent tests with expected vocab/entity matches: #lang: # intent_name: # - example_utterance: # - expected vocab name # - other expected vocab name # Intent tests with specific vocab/entity extraction tests: #lang: # intent_name: # - example_utterance: # - expected_vocab_key: expected_vocab_value # - expected_entity_key: expected_entity_value en-us: IPIntent: - what is your ip address - what is my ip address: - IP - what is my i.p. address - What is your I.P. address? - what is my public IP address?: - public: public","title":"test/test_intents.yaml"},{"location":"skill_tests/#githubworkflowsskill_testsyml_1","text":"The filename isn't important here, but a workflow must specify a job using neongeckocom/.github/.github/workflows/skill_test_intents.yml@master . A minimal example would be: name: Test Skill Resources on: pull_request: workflow_dispatch: jobs: skill_intent_tests: uses: neongeckocom/.github/.github/workflows/skill_test_intents.yml@master","title":".github/workflows/skill_tests.yml"},{"location":"skills_bus/","text":"OVOSSkill Bus Interaction The base OVOSSkill API handles most of the Messagebus usage automatically. For example, the mycroft.stop message is caught by the skill framework, invoking an overridden OVOSSkills.stop() method within a Skill . Similarly, the OVOSSkill.speak() and OVOSSkill.speak_dialog() methods generate speak messages to be conveyed to the text-to-speech (TTS) and audio systems. You will really only need to know about the Mycroft Messagebus if you are developing advanced Skills . The OVOSSkill.add_event() method allows you to attach a handler which will be triggered when the message is seen on the Messagebus. Connecting Message handlers class ListenForMessageSkill(OVOSSkill): def initialize(self): self.add_event('recognizer_loop:record_begin', self.handle_listener_started) self.add_event('recognizer_loop:record_end', self.handle_listener_ended) def handle_listener_started(self, message): # code to excecute when active listening begins... def handle_listener_ended(self, message): # code to excecute when active listening begins... Generating Messages from ovos_bus_client import Message class GenerateMessageSkill(OVOSSkill): def some_method(self): self.bus.emit(Message(\"recognizer_loop:utterance\", {'utterances': [\"the injected utterance\"], 'lang': 'en-us'}))","title":"Bus Interaction"},{"location":"skills_bus/#ovosskill-bus-interaction","text":"The base OVOSSkill API handles most of the Messagebus usage automatically. For example, the mycroft.stop message is caught by the skill framework, invoking an overridden OVOSSkills.stop() method within a Skill . Similarly, the OVOSSkill.speak() and OVOSSkill.speak_dialog() methods generate speak messages to be conveyed to the text-to-speech (TTS) and audio systems. You will really only need to know about the Mycroft Messagebus if you are developing advanced Skills . The OVOSSkill.add_event() method allows you to attach a handler which will be triggered when the message is seen on the Messagebus.","title":"OVOSSkill Bus Interaction"},{"location":"skills_bus/#connecting-message-handlers","text":"class ListenForMessageSkill(OVOSSkill): def initialize(self): self.add_event('recognizer_loop:record_begin', self.handle_listener_started) self.add_event('recognizer_loop:record_end', self.handle_listener_ended) def handle_listener_started(self, message): # code to excecute when active listening begins... def handle_listener_ended(self, message): # code to excecute when active listening begins...","title":"Connecting Message handlers"},{"location":"skills_bus/#generating-messages","text":"from ovos_bus_client import Message class GenerateMessageSkill(OVOSSkill): def some_method(self): self.bus.emit(Message(\"recognizer_loop:utterance\", {'utterances': [\"the injected utterance\"], 'lang': 'en-us'}))","title":"Generating Messages"},{"location":"solvers/","text":"Solver Plugins Solver plugins solve natural language queries, they define a unified api around specific kinds of questions and provide auto translation capabilities for language support A plugin can define the language it works in, eg, wolfram alpha only accepts english input at the time of this writing Bidirectional translation will be handled behind the scenes for other languages Solvers are used by individual skills and by the Persona Framework Question Solvers NEW in ovos-core version 0.0.8 Given a free form natural language question, return an answer Originally implemented for Neon non-exhaustive reference table of question solver plugins plugin description native language ovos-solver-plugin-ddg extract keywords from query and search duck duck english ovos-solver-plugin-wikipedia extract keywords from query and search wikipedia english ovos-solver-plugin-wolfram-alpha wolfram alpha spoken answers api english ovos-question-solver-wordnet answer \"what is\" questions via wordnet english ovos-solver-plugin-aiml AIML chatbot english ovos-solver-plugin-rivescript rivescript chatbot english ovos-solver-pandorabots-plugin old school chatbots hosted around the web english ovos-solver-plugin-openai-persona OpenAI API compatible LLMs english Example Usage - DuckDuckGo plugin single answer from skill_ovos_ddg import DuckDuckGoSolver d = DuckDuckGoSolver() query = \"who is Isaac Newton\" # full answer ans = d.spoken_answer(query) print(ans) # Sir Isaac Newton was an English mathematician, physicist, astronomer, alchemist, theologian, and author widely recognised as one of the greatest mathematicians and physicists of all time and among the most influential scientists. He was a key figure in the philosophical revolution known as the Enlightenment. His book Philosophi\u00e6 Naturalis Principia Mathematica, first published in 1687, established classical mechanics. Newton also made seminal contributions to optics, and shares credit with German mathematician Gottfried Wilhelm Leibniz for developing infinitesimal calculus. In the Principia, Newton formulated the laws of motion and universal gravitation that formed the dominant scientific viewpoint until it was superseded by the theory of relativity. chunked answer, for conversational dialogs, ie \"tell me more\" from skill_ovos_ddg import DuckDuckGoSolver d = DuckDuckGoSolver() query = \"who is Isaac Newton\" # chunked answer for sentence in d.long_answer(query): print(sentence[\"title\"]) print(sentence[\"summary\"]) print(sentence.get(\"img\")) # who is Isaac Newton # Sir Isaac Newton was an English mathematician, physicist, astronomer, alchemist, theologian, and author widely recognised as one of the greatest mathematicians and physicists of all time and among the most influential scientists. # https://duckduckgo.com/i/ea7be744.jpg # who is Isaac Newton # He was a key figure in the philosophical revolution known as the Enlightenment. # https://duckduckgo.com/i/ea7be744.jpg # who is Isaac Newton # His book Philosophi\u00e6 Naturalis Principia Mathematica, first published in 1687, established classical mechanics. # https://duckduckgo.com/i/ea7be744.jpg # who is Isaac Newton # Newton also made seminal contributions to optics, and shares credit with German mathematician Gottfried Wilhelm Leibniz for developing infinitesimal calculus. # https://duckduckgo.com/i/ea7be744.jpg # who is Isaac Newton # In the Principia, Newton formulated the laws of motion and universal gravitation that formed the dominant scientific viewpoint until it was superseded by the theory of relativity. # https://duckduckgo.com/i/ea7be744.jpg Auto translation, pass user language in context from skill_ovos_ddg import DuckDuckGoSolver d = DuckDuckGoSolver() # bidirectional auto translate by passing lang context sentence = d.spoken_answer(\"Quem \u00e9 Isaac Newton\", context={\"lang\": \"pt\"}) print(sentence) # Sir Isaac Newton foi um matem\u00e1tico ingl\u00eas, f\u00edsico, astr\u00f4nomo, alquimista, te\u00f3logo e autor amplamente reconhecido como um dos maiores matem\u00e1ticos e f\u00edsicos de todos os tempos e entre os cientistas mais influentes. Ele era uma figura chave na revolu\u00e7\u00e3o filos\u00f3fica conhecida como o Iluminismo. Seu livro Philosophi\u00e6 Naturalis Principia Mathematica, publicado pela primeira vez em 1687, estabeleceu a mec\u00e2nica cl\u00e1ssica. Newton tamb\u00e9m fez contribui\u00e7\u00f5es seminais para a \u00f3ptica, e compartilha cr\u00e9dito com o matem\u00e1tico alem\u00e3o Gottfried Wilhelm Leibniz para desenvolver c\u00e1lculo infinitesimal. No Principia, Newton formulou as leis do movimento e da gravita\u00e7\u00e3o universal que formaram o ponto de vista cient\u00edfico dominante at\u00e9 ser superado pela teoria da relatividade Plugins are expected to implement the get_xxx methods and leave the user facing equivalents alone from ovos_plugin_manager.templates.solvers import QuestionSolver class MySolver(QuestionSolver): enable_tx = False # if True enables bidirectional translation priority = 100 def __init__(self, config=None): config = config or {} # set the \"internal\" language, defined by dev, not user # this plugin internally only accepts and outputs english config[\"lang\"] = \"en\" super().__init__(config) # expected solver methods to be implemented def get_data(self, query, context): \"\"\" query assured to be in self.default_lang return a dict response \"\"\" return {\"error\": \"404 answer not found\"} def get_image(self, query, context=None): \"\"\" query assured to be in self.default_lang return path/url to a single image to acompany spoken_answer \"\"\" return \"http://stock.image.jpg\" def get_spoken_answer(self, query, context=None): \"\"\" query assured to be in self.default_lang return a single sentence text response \"\"\" return \"The full answer is XXX\" def get_expanded_answer(self, query, context=None): \"\"\" query assured to be in self.default_lang return a list of ordered steps to expand the answer, eg, \"tell me more\" { \"title\": \"optional\", \"summary\": \"speak this\", \"img\": \"optional/path/or/url } :return: \"\"\" steps = [ {\"title\": \"the question\", \"summary\": \"we forgot the question\", \"image\": \"404.jpg\"}, {\"title\": \"the answer\", \"summary\": \"but the answer is 42\", \"image\": \"42.jpg\"} ] return steps Multiple Choice Solvers NEW in ovos-core version 0.0.8 given a question and multiple answers, select the best answer non-exhaustive reference table of multiple choice solver plugins plugin description native language ovos-choice-solver-bm25 using Okapi BM25 ranking function to estimate the relevance of documents to a given search query Implementation class MultipleChoiceSolver(AbstractSolver): \"\"\" select best answer from question + multiple choice handling automatic translation back and forth as needed\"\"\" # plugin methods to override @abc.abstractmethod def select_answer(self, query: str, options: List[str], context: Optional[dict] = None) -> str: \"\"\" query and options assured to be in self.default_lang return best answer from options list \"\"\" raise NotImplementedError Evidence Solver NEW in ovos-core version 0.0.8 given a document and a question about it, select the best passage that answers the question non-exhaustive reference table of evidence solver plugins plugin description native language ovos-evidence-solver-bm25 using Okapi BM25 ranking function to estimate the relevance of documents to a given search query Implementation class EvidenceSolver(AbstractSolver): \"\"\"perform NLP reading comprehension task, handling automatic translation back and forth as needed\"\"\" # plugin methods to override @abc.abstractmethod def get_best_passage(self, evidence: str, question: str, context: Optional[dict] = None) -> str: \"\"\" evidence and question assured to be in self.default_lang returns summary of provided document \"\"\" raise NotImplementedError Entailment Solver NEW in ovos-core version 0.0.8 Given a hypothesis and a premise, return True if the premise entails the hypothesis, False otherwise class EntailmentSolver(AbstractSolver): \"\"\" select best answer from question + multiple choice handling automatic translation back and forth as needed\"\"\" # plugin methods to override @abc.abstractmethod def check_entailment(self, premise: str, hypothesis: str, context: Optional[dict] = None) -> bool: \"\"\" premise and hyopithesis assured to be in self.default_lang return Bool, True if premise entails the hypothesis False otherwise \"\"\" raise NotImplementedError Summarization Solver NEW in ovos-core version 0.0.8 Given a document, return it's summary non-exhaustive reference table of multiple choice solver plugins plugin description native language ovos-summarizer-solver-wordfreq using word frequencies select the top utterances Implementation class TldrSolver(AbstractSolver): \"\"\"perform NLP summarization task, handling automatic translation back and forth as needed\"\"\" # plugin methods to override @abc.abstractmethod def get_tldr(self, document: str, context: Optional[dict] = None) -> str: \"\"\" document assured to be in self.default_lang returns summary of provided document \"\"\" raise NotImplementedError","title":"Question Solvers"},{"location":"solvers/#solver-plugins","text":"Solver plugins solve natural language queries, they define a unified api around specific kinds of questions and provide auto translation capabilities for language support A plugin can define the language it works in, eg, wolfram alpha only accepts english input at the time of this writing Bidirectional translation will be handled behind the scenes for other languages Solvers are used by individual skills and by the Persona Framework","title":"Solver Plugins"},{"location":"solvers/#question-solvers","text":"NEW in ovos-core version 0.0.8 Given a free form natural language question, return an answer Originally implemented for Neon non-exhaustive reference table of question solver plugins plugin description native language ovos-solver-plugin-ddg extract keywords from query and search duck duck english ovos-solver-plugin-wikipedia extract keywords from query and search wikipedia english ovos-solver-plugin-wolfram-alpha wolfram alpha spoken answers api english ovos-question-solver-wordnet answer \"what is\" questions via wordnet english ovos-solver-plugin-aiml AIML chatbot english ovos-solver-plugin-rivescript rivescript chatbot english ovos-solver-pandorabots-plugin old school chatbots hosted around the web english ovos-solver-plugin-openai-persona OpenAI API compatible LLMs english Example Usage - DuckDuckGo plugin single answer from skill_ovos_ddg import DuckDuckGoSolver d = DuckDuckGoSolver() query = \"who is Isaac Newton\" # full answer ans = d.spoken_answer(query) print(ans) # Sir Isaac Newton was an English mathematician, physicist, astronomer, alchemist, theologian, and author widely recognised as one of the greatest mathematicians and physicists of all time and among the most influential scientists. He was a key figure in the philosophical revolution known as the Enlightenment. His book Philosophi\u00e6 Naturalis Principia Mathematica, first published in 1687, established classical mechanics. Newton also made seminal contributions to optics, and shares credit with German mathematician Gottfried Wilhelm Leibniz for developing infinitesimal calculus. In the Principia, Newton formulated the laws of motion and universal gravitation that formed the dominant scientific viewpoint until it was superseded by the theory of relativity. chunked answer, for conversational dialogs, ie \"tell me more\" from skill_ovos_ddg import DuckDuckGoSolver d = DuckDuckGoSolver() query = \"who is Isaac Newton\" # chunked answer for sentence in d.long_answer(query): print(sentence[\"title\"]) print(sentence[\"summary\"]) print(sentence.get(\"img\")) # who is Isaac Newton # Sir Isaac Newton was an English mathematician, physicist, astronomer, alchemist, theologian, and author widely recognised as one of the greatest mathematicians and physicists of all time and among the most influential scientists. # https://duckduckgo.com/i/ea7be744.jpg # who is Isaac Newton # He was a key figure in the philosophical revolution known as the Enlightenment. # https://duckduckgo.com/i/ea7be744.jpg # who is Isaac Newton # His book Philosophi\u00e6 Naturalis Principia Mathematica, first published in 1687, established classical mechanics. # https://duckduckgo.com/i/ea7be744.jpg # who is Isaac Newton # Newton also made seminal contributions to optics, and shares credit with German mathematician Gottfried Wilhelm Leibniz for developing infinitesimal calculus. # https://duckduckgo.com/i/ea7be744.jpg # who is Isaac Newton # In the Principia, Newton formulated the laws of motion and universal gravitation that formed the dominant scientific viewpoint until it was superseded by the theory of relativity. # https://duckduckgo.com/i/ea7be744.jpg Auto translation, pass user language in context from skill_ovos_ddg import DuckDuckGoSolver d = DuckDuckGoSolver() # bidirectional auto translate by passing lang context sentence = d.spoken_answer(\"Quem \u00e9 Isaac Newton\", context={\"lang\": \"pt\"}) print(sentence) # Sir Isaac Newton foi um matem\u00e1tico ingl\u00eas, f\u00edsico, astr\u00f4nomo, alquimista, te\u00f3logo e autor amplamente reconhecido como um dos maiores matem\u00e1ticos e f\u00edsicos de todos os tempos e entre os cientistas mais influentes. Ele era uma figura chave na revolu\u00e7\u00e3o filos\u00f3fica conhecida como o Iluminismo. Seu livro Philosophi\u00e6 Naturalis Principia Mathematica, publicado pela primeira vez em 1687, estabeleceu a mec\u00e2nica cl\u00e1ssica. Newton tamb\u00e9m fez contribui\u00e7\u00f5es seminais para a \u00f3ptica, e compartilha cr\u00e9dito com o matem\u00e1tico alem\u00e3o Gottfried Wilhelm Leibniz para desenvolver c\u00e1lculo infinitesimal. No Principia, Newton formulou as leis do movimento e da gravita\u00e7\u00e3o universal que formaram o ponto de vista cient\u00edfico dominante at\u00e9 ser superado pela teoria da relatividade Plugins are expected to implement the get_xxx methods and leave the user facing equivalents alone from ovos_plugin_manager.templates.solvers import QuestionSolver class MySolver(QuestionSolver): enable_tx = False # if True enables bidirectional translation priority = 100 def __init__(self, config=None): config = config or {} # set the \"internal\" language, defined by dev, not user # this plugin internally only accepts and outputs english config[\"lang\"] = \"en\" super().__init__(config) # expected solver methods to be implemented def get_data(self, query, context): \"\"\" query assured to be in self.default_lang return a dict response \"\"\" return {\"error\": \"404 answer not found\"} def get_image(self, query, context=None): \"\"\" query assured to be in self.default_lang return path/url to a single image to acompany spoken_answer \"\"\" return \"http://stock.image.jpg\" def get_spoken_answer(self, query, context=None): \"\"\" query assured to be in self.default_lang return a single sentence text response \"\"\" return \"The full answer is XXX\" def get_expanded_answer(self, query, context=None): \"\"\" query assured to be in self.default_lang return a list of ordered steps to expand the answer, eg, \"tell me more\" { \"title\": \"optional\", \"summary\": \"speak this\", \"img\": \"optional/path/or/url } :return: \"\"\" steps = [ {\"title\": \"the question\", \"summary\": \"we forgot the question\", \"image\": \"404.jpg\"}, {\"title\": \"the answer\", \"summary\": \"but the answer is 42\", \"image\": \"42.jpg\"} ] return steps","title":"Question Solvers"},{"location":"solvers/#multiple-choice-solvers","text":"NEW in ovos-core version 0.0.8 given a question and multiple answers, select the best answer non-exhaustive reference table of multiple choice solver plugins plugin description native language ovos-choice-solver-bm25 using Okapi BM25 ranking function to estimate the relevance of documents to a given search query Implementation class MultipleChoiceSolver(AbstractSolver): \"\"\" select best answer from question + multiple choice handling automatic translation back and forth as needed\"\"\" # plugin methods to override @abc.abstractmethod def select_answer(self, query: str, options: List[str], context: Optional[dict] = None) -> str: \"\"\" query and options assured to be in self.default_lang return best answer from options list \"\"\" raise NotImplementedError","title":"Multiple Choice Solvers"},{"location":"solvers/#evidence-solver","text":"NEW in ovos-core version 0.0.8 given a document and a question about it, select the best passage that answers the question non-exhaustive reference table of evidence solver plugins plugin description native language ovos-evidence-solver-bm25 using Okapi BM25 ranking function to estimate the relevance of documents to a given search query Implementation class EvidenceSolver(AbstractSolver): \"\"\"perform NLP reading comprehension task, handling automatic translation back and forth as needed\"\"\" # plugin methods to override @abc.abstractmethod def get_best_passage(self, evidence: str, question: str, context: Optional[dict] = None) -> str: \"\"\" evidence and question assured to be in self.default_lang returns summary of provided document \"\"\" raise NotImplementedError","title":"Evidence Solver"},{"location":"solvers/#entailment-solver","text":"NEW in ovos-core version 0.0.8 Given a hypothesis and a premise, return True if the premise entails the hypothesis, False otherwise class EntailmentSolver(AbstractSolver): \"\"\" select best answer from question + multiple choice handling automatic translation back and forth as needed\"\"\" # plugin methods to override @abc.abstractmethod def check_entailment(self, premise: str, hypothesis: str, context: Optional[dict] = None) -> bool: \"\"\" premise and hyopithesis assured to be in self.default_lang return Bool, True if premise entails the hypothesis False otherwise \"\"\" raise NotImplementedError","title":"Entailment Solver"},{"location":"solvers/#summarization-solver","text":"NEW in ovos-core version 0.0.8 Given a document, return it's summary non-exhaustive reference table of multiple choice solver plugins plugin description native language ovos-summarizer-solver-wordfreq using word frequencies select the top utterances Implementation class TldrSolver(AbstractSolver): \"\"\"perform NLP summarization task, handling automatic translation back and forth as needed\"\"\" # plugin methods to override @abc.abstractmethod def get_tldr(self, document: str, context: Optional[dict] = None) -> str: \"\"\" document assured to be in self.default_lang returns summary of provided document \"\"\" raise NotImplementedError","title":"Summarization Solver"},{"location":"speech_service/","text":"Listener Service The listener service is responsible for handling audio input, it understands speech and converts it into utterances to be handled by ovos-core Different implementations of the listener service have been available during the years mycroft-classic-listener the original listener from mycroft mark1 extracted into a standalone component - archived ovos-listener - an updated version of the mycroft listener with VAD plugins and multiple hotwords support - deprecated in ovos-core version 0.0.8 ovos-dinkum-listener - a listener rewrite based on mycroft-dinkum - NEW in ovos-core version 0.0.8 Listener You can modify microphone settings and enable additional features under the listener section such as wake word / utterance recording / uploading \"listener\": { // NOTE, multiple hotwords are supported, these fields define the main wake_word, // this is equivalent to setting \"active\": true in the \"hotwords\" section \"wake_word\": \"hey_mycroft\", \"stand_up_word\": \"wake_up\", \"microphone\": {...}, \"VAD\": {...}, // Seconds of speech before voice command has begun \"speech_begin\": 0.1, // Seconds of silence before a voice command has finished \"silence_end\": 0.5, // Settings used by microphone to set recording timeout with and without speech detected \"recording_timeout\": 10.0, // Settings used by microphone to set recording timeout without speech detected. \"recording_timeout_with_silence\": 3.0, // Setting to remove all silence/noise from start and end of recorded speech (only non-streaming) \"remove_silence\": true } Microphone NEW in ovos-core version 0.0.8 Microphone plugins are responsible for feeding audio to the listener, different Operating Systems may require different plugins or otherwise have performance benefits \"listener\": { \"microphone\": { \"module\": \"ovos-microphone-plugin-alsa\" } } Hotwords By default the listener is waiting for a hotword to do something in response the most common usage of a hotword is as the assistant's name, instead of continuously transcribing audio the listener waits for a wake word, and then listens to the user speaking OVOS allows you to load any number of hot words in parallel and trigger different actions when they are detected each hotword can do one or more of the following: trigger listening, also called a wake_word play a sound emit a bus event take ovos-core out of sleep mode, also called a wakeup_word or standup_word take ovos-core out of recording mode, also called a stop_word To add a new hotword add its configuration under \"hotwords\" section. By default, all hotwords are disabled unless you set \"active\": true . Under the \"listener\" setting a main wake word and stand up word are defined, those will be automatically enabled unless you set \"active\": false . Users are expected to only change listener.wake_word if using a single wake word, setting \"active\": true is only intended for extra hotwords \"listener\": { // Default wake_word and stand_up_word will be automatically set to active // unless explicitly disabled under \"hotwords\" section \"wake_word\": \"hey mycroft\", \"stand_up_word\": \"wake up\" }, // Hotword configurations \"hotwords\": { \"hey_mycroft\": { \"module\": \"ovos-ww-plugin-precise-lite\", \"model\": \"https://github.com/OpenVoiceOS/precise-lite-models/raw/master/wakewords/en/hey_mycroft.tflite\", \"expected_duration\": 3, \"trigger_level\": 3, \"sensitivity\": 0.5, \"listen\": true }, // default wakeup word to take ovos out of SLEEPING mode, \"wake_up\": { \"module\": \"ovos-ww-plugin-pocketsphinx\", \"phonemes\": \"W EY K . AH P\", \"threshold\": 1e-20, \"lang\": \"en-us\", \"wakeup\": true, \"fallback_ww\": \"wake_up_vosk\" } } Sound Classifiers hotwords can be used as generic sound classifiers that emit bus events for other systems to detect Let's consider a model trained to recognize coughing, and a companion plugin to track how often it happens, this can be used as an indicator of disease \"hotwords\": { \"cough\": { \"module\": \"ovos-ww-plugin-precise\", \"version\": \"0.3\", \"model\": \"https://github.com/MycroftAI/precise-data/blob/models-dev/cough.tar.gz\", \"expected_duration\": 3, \"trigger_level\": 3, \"sensitivity\": 0.5, \"listen\": false, \"active\": true, // on detection emit this msg_type \"bus_event\": \"cough.detected\" } } Multilingualism In multilingual homes a wake word can be configured for each language, by giving the assistant a different name in each we can assign a language to be used by STT \"listener\": { \"wake_word\": \"hey mycroft\" }, \"hotwords\": { // default wake word, in global language \"hey_mycroft\": {...}, // extra wake word with lang assigned \"android\": { \"module\": \"...\", \"model\": \"...\", // set to active as extra wake word \"active\": true, \"listen\": true, // assign a language \"stt_lang\": \"pt-pt\" } } Fallback Wake Words NEW in ovos-core version 0.0.8 hotword definitions can also include a \"fallback_ww\" , this indicates an alternative hotword config to load in case the original failed to load for any reason \"listener\": { // Default wake_word and stand_up_word will be automatically set to active // unless explicitly disabled under \"hotwords\" section \"wake_word\": \"hey mycroft\", \"stand_up_word\": \"wake up\" }, // Hotword configurations \"hotwords\": { \"hey_mycroft\": { \"module\": \"ovos-ww-plugin-precise-lite\", \"model\": \"https://github.com/OpenVoiceOS/precise-lite-models/raw/master/wakewords/en/hey_mycroft.tflite\", \"expected_duration\": 3, \"trigger_level\": 3, \"sensitivity\": 0.5, \"listen\": true, \"fallback_ww\": \"hey_mycroft_precise\" }, // in case precise-lite is not installed, attempt to use classic precise \"hey_mycroft_precise\": { \"module\": \"ovos-ww-plugin-precise\", \"version\": \"0.3\", \"model\": \"https://github.com/MycroftAI/precise-data/raw/models-dev/hey-mycroft.tar.gz\", \"expected_duration\": 3, \"trigger_level\": 3, \"sensitivity\": 0.5, \"listen\": true, \"fallback_ww\": \"hey_mycroft_vosk\" }, // in case classic precise is not installed, attempt to use vosk \"hey_mycroft_vosk\": { \"module\": \"ovos-ww-plugin-vosk\", \"samples\": [\"hey mycroft\", \"hey microsoft\", \"hey mike roft\", \"hey minecraft\"], \"rule\": \"fuzzy\", \"listen\": true, \"fallback_ww\": \"hey_mycroft_pocketsphinx\" }, // in case vosk is not installed, attempt to use pocketsphinx \"hey_mycroft_pocketsphinx\": { \"module\": \"ovos-ww-plugin-pocketsphinx\", \"phonemes\": \"HH EY . M AY K R AO F T\", \"threshold\": 1e-90, \"lang\": \"en-us\", \"listen\": true }, // default wakeup word to take ovos out of SLEEPING mode, \"wake_up\": { \"module\": \"ovos-ww-plugin-pocketsphinx\", \"phonemes\": \"W EY K . AH P\", \"threshold\": 1e-20, \"lang\": \"en-us\", \"wakeup\": true, \"fallback_ww\": \"wake_up_vosk\" }, // in case pocketsphinx plugin is not installed, attempt to use vosk \"wake_up_vosk\": { \"module\": \"ovos-ww-plugin-vosk\", \"rule\": \"fuzzy\", \"samples\": [\"wake up\"], \"lang\": \"en-us\", // makes this a wakeup word for usage in SLEEPING mode \"wakeup\": true } } VAD Voice Activity Detection plugins have several functions under the listener service detect when user finished speaking remove silence before sending audio to STT - NEW in ovos-core version 0.0.8 detect when user is speaking during continuous mode (read below) \"listener\": { // Setting to remove all silence/noise from start and end of recorded speech (only non-streaming) \"remove_silence\": true, \"VAD\": { // recommended plugin: \"ovos-vad-plugin-silero\" \"module\": \"ovos-vad-plugin-silero\", \"ovos-vad-plugin-silero\": {\"threshold\": 0.2}, \"ovos-vad-plugin-webrtcvad\": {\"vad_mode\": 3} } } STT Two STT plugins may be loaded at once, if the primary plugin fails for some reason the second plugin will be used. This allows you to have a lower accuracy offline model as fallback to account for internet outages, this ensures your device never becomes fully unusable \"stt\": { \"module\": \"ovos-stt-plugin-server\", \"fallback_module\": \"ovos-stt-plugin-vosk\", \"ovos-stt-plugin-server\": {\"url\": \"https://stt.openvoiceos.com/stt\"} }, Audio Transformers NEW in ovos-core version 0.0.8 , originally developed for Neon Similarly to utterance transformers in ovos-core , the listener exposes audio and message. context to a set of plugins that can transform it before STT stage Audio transformer plugins can either transform the audio binary data itself (eg, denoise) or the context (eg, speaker recognition) The audio is sent sequentially to all transformer plugins, ordered by priority (developer defined), until finally it is sent to the STT stage Modes of Operation There are 3 modes to run dinkum, wakeword, hybrid, or continuous (VAD only) Additionally, there are 2 temporary modes that can be triggered via bus events / companion skills Wake Word mode Sleep mode Can be used via Naptime skill Be sure to enable a wakeup word to get out of sleep! \"listener\": { \"stand_up_word\": \"wake up\" }, \"hotwords\": { \"wake up\": { \"module\": \"ovos-ww-plugin-pocketsphinx\", \"phonemes\": \"W EY K . AH P\", \"threshold\": 1e-20, \"lang\": \"en-us\", \"wakeup\": true } } Continuous mode EXPERIMENTAL - NEW in ovos-core version 0.0.8 \"listener\": { // continuous listen is an experimental setting, it removes the need for // wake words and uses VAD only, a streaming STT is strongly recommended // NOTE: depending on hardware this may cause mycroft to hear its own TTS responses as questions \"continuous_listen\": false } Hybrid mode EXPERIMENTAL - NEW in ovos-core version 0.0.8 \"listener\": { // hybrid listen is an experimental setting, // it will not require a wake word for X seconds after a user interaction // this means you dont need to say \"hey mycroft\" for follow up questions \"hybrid_listen\": false, // number of seconds to wait for an interaction before requiring wake word again \"listen_timeout\": 45 } Recording mode EXPERIMENTAL - NEW in ovos-core version 0.0.8 Can be used via Recording skill","title":"Listener"},{"location":"speech_service/#listener-service","text":"The listener service is responsible for handling audio input, it understands speech and converts it into utterances to be handled by ovos-core Different implementations of the listener service have been available during the years mycroft-classic-listener the original listener from mycroft mark1 extracted into a standalone component - archived ovos-listener - an updated version of the mycroft listener with VAD plugins and multiple hotwords support - deprecated in ovos-core version 0.0.8 ovos-dinkum-listener - a listener rewrite based on mycroft-dinkum - NEW in ovos-core version 0.0.8","title":"Listener Service"},{"location":"speech_service/#listener","text":"You can modify microphone settings and enable additional features under the listener section such as wake word / utterance recording / uploading \"listener\": { // NOTE, multiple hotwords are supported, these fields define the main wake_word, // this is equivalent to setting \"active\": true in the \"hotwords\" section \"wake_word\": \"hey_mycroft\", \"stand_up_word\": \"wake_up\", \"microphone\": {...}, \"VAD\": {...}, // Seconds of speech before voice command has begun \"speech_begin\": 0.1, // Seconds of silence before a voice command has finished \"silence_end\": 0.5, // Settings used by microphone to set recording timeout with and without speech detected \"recording_timeout\": 10.0, // Settings used by microphone to set recording timeout without speech detected. \"recording_timeout_with_silence\": 3.0, // Setting to remove all silence/noise from start and end of recorded speech (only non-streaming) \"remove_silence\": true }","title":"Listener"},{"location":"speech_service/#microphone","text":"NEW in ovos-core version 0.0.8 Microphone plugins are responsible for feeding audio to the listener, different Operating Systems may require different plugins or otherwise have performance benefits \"listener\": { \"microphone\": { \"module\": \"ovos-microphone-plugin-alsa\" } }","title":"Microphone"},{"location":"speech_service/#hotwords","text":"By default the listener is waiting for a hotword to do something in response the most common usage of a hotword is as the assistant's name, instead of continuously transcribing audio the listener waits for a wake word, and then listens to the user speaking OVOS allows you to load any number of hot words in parallel and trigger different actions when they are detected each hotword can do one or more of the following: trigger listening, also called a wake_word play a sound emit a bus event take ovos-core out of sleep mode, also called a wakeup_word or standup_word take ovos-core out of recording mode, also called a stop_word To add a new hotword add its configuration under \"hotwords\" section. By default, all hotwords are disabled unless you set \"active\": true . Under the \"listener\" setting a main wake word and stand up word are defined, those will be automatically enabled unless you set \"active\": false . Users are expected to only change listener.wake_word if using a single wake word, setting \"active\": true is only intended for extra hotwords \"listener\": { // Default wake_word and stand_up_word will be automatically set to active // unless explicitly disabled under \"hotwords\" section \"wake_word\": \"hey mycroft\", \"stand_up_word\": \"wake up\" }, // Hotword configurations \"hotwords\": { \"hey_mycroft\": { \"module\": \"ovos-ww-plugin-precise-lite\", \"model\": \"https://github.com/OpenVoiceOS/precise-lite-models/raw/master/wakewords/en/hey_mycroft.tflite\", \"expected_duration\": 3, \"trigger_level\": 3, \"sensitivity\": 0.5, \"listen\": true }, // default wakeup word to take ovos out of SLEEPING mode, \"wake_up\": { \"module\": \"ovos-ww-plugin-pocketsphinx\", \"phonemes\": \"W EY K . AH P\", \"threshold\": 1e-20, \"lang\": \"en-us\", \"wakeup\": true, \"fallback_ww\": \"wake_up_vosk\" } }","title":"Hotwords"},{"location":"speech_service/#sound-classifiers","text":"hotwords can be used as generic sound classifiers that emit bus events for other systems to detect Let's consider a model trained to recognize coughing, and a companion plugin to track how often it happens, this can be used as an indicator of disease \"hotwords\": { \"cough\": { \"module\": \"ovos-ww-plugin-precise\", \"version\": \"0.3\", \"model\": \"https://github.com/MycroftAI/precise-data/blob/models-dev/cough.tar.gz\", \"expected_duration\": 3, \"trigger_level\": 3, \"sensitivity\": 0.5, \"listen\": false, \"active\": true, // on detection emit this msg_type \"bus_event\": \"cough.detected\" } }","title":"Sound Classifiers"},{"location":"speech_service/#multilingualism","text":"In multilingual homes a wake word can be configured for each language, by giving the assistant a different name in each we can assign a language to be used by STT \"listener\": { \"wake_word\": \"hey mycroft\" }, \"hotwords\": { // default wake word, in global language \"hey_mycroft\": {...}, // extra wake word with lang assigned \"android\": { \"module\": \"...\", \"model\": \"...\", // set to active as extra wake word \"active\": true, \"listen\": true, // assign a language \"stt_lang\": \"pt-pt\" } }","title":"Multilingualism"},{"location":"speech_service/#fallback-wake-words","text":"NEW in ovos-core version 0.0.8 hotword definitions can also include a \"fallback_ww\" , this indicates an alternative hotword config to load in case the original failed to load for any reason \"listener\": { // Default wake_word and stand_up_word will be automatically set to active // unless explicitly disabled under \"hotwords\" section \"wake_word\": \"hey mycroft\", \"stand_up_word\": \"wake up\" }, // Hotword configurations \"hotwords\": { \"hey_mycroft\": { \"module\": \"ovos-ww-plugin-precise-lite\", \"model\": \"https://github.com/OpenVoiceOS/precise-lite-models/raw/master/wakewords/en/hey_mycroft.tflite\", \"expected_duration\": 3, \"trigger_level\": 3, \"sensitivity\": 0.5, \"listen\": true, \"fallback_ww\": \"hey_mycroft_precise\" }, // in case precise-lite is not installed, attempt to use classic precise \"hey_mycroft_precise\": { \"module\": \"ovos-ww-plugin-precise\", \"version\": \"0.3\", \"model\": \"https://github.com/MycroftAI/precise-data/raw/models-dev/hey-mycroft.tar.gz\", \"expected_duration\": 3, \"trigger_level\": 3, \"sensitivity\": 0.5, \"listen\": true, \"fallback_ww\": \"hey_mycroft_vosk\" }, // in case classic precise is not installed, attempt to use vosk \"hey_mycroft_vosk\": { \"module\": \"ovos-ww-plugin-vosk\", \"samples\": [\"hey mycroft\", \"hey microsoft\", \"hey mike roft\", \"hey minecraft\"], \"rule\": \"fuzzy\", \"listen\": true, \"fallback_ww\": \"hey_mycroft_pocketsphinx\" }, // in case vosk is not installed, attempt to use pocketsphinx \"hey_mycroft_pocketsphinx\": { \"module\": \"ovos-ww-plugin-pocketsphinx\", \"phonemes\": \"HH EY . M AY K R AO F T\", \"threshold\": 1e-90, \"lang\": \"en-us\", \"listen\": true }, // default wakeup word to take ovos out of SLEEPING mode, \"wake_up\": { \"module\": \"ovos-ww-plugin-pocketsphinx\", \"phonemes\": \"W EY K . AH P\", \"threshold\": 1e-20, \"lang\": \"en-us\", \"wakeup\": true, \"fallback_ww\": \"wake_up_vosk\" }, // in case pocketsphinx plugin is not installed, attempt to use vosk \"wake_up_vosk\": { \"module\": \"ovos-ww-plugin-vosk\", \"rule\": \"fuzzy\", \"samples\": [\"wake up\"], \"lang\": \"en-us\", // makes this a wakeup word for usage in SLEEPING mode \"wakeup\": true } }","title":"Fallback Wake Words"},{"location":"speech_service/#vad","text":"Voice Activity Detection plugins have several functions under the listener service detect when user finished speaking remove silence before sending audio to STT - NEW in ovos-core version 0.0.8 detect when user is speaking during continuous mode (read below) \"listener\": { // Setting to remove all silence/noise from start and end of recorded speech (only non-streaming) \"remove_silence\": true, \"VAD\": { // recommended plugin: \"ovos-vad-plugin-silero\" \"module\": \"ovos-vad-plugin-silero\", \"ovos-vad-plugin-silero\": {\"threshold\": 0.2}, \"ovos-vad-plugin-webrtcvad\": {\"vad_mode\": 3} } }","title":"VAD"},{"location":"speech_service/#stt","text":"Two STT plugins may be loaded at once, if the primary plugin fails for some reason the second plugin will be used. This allows you to have a lower accuracy offline model as fallback to account for internet outages, this ensures your device never becomes fully unusable \"stt\": { \"module\": \"ovos-stt-plugin-server\", \"fallback_module\": \"ovos-stt-plugin-vosk\", \"ovos-stt-plugin-server\": {\"url\": \"https://stt.openvoiceos.com/stt\"} },","title":"STT"},{"location":"speech_service/#audio-transformers","text":"NEW in ovos-core version 0.0.8 , originally developed for Neon Similarly to utterance transformers in ovos-core , the listener exposes audio and message. context to a set of plugins that can transform it before STT stage Audio transformer plugins can either transform the audio binary data itself (eg, denoise) or the context (eg, speaker recognition) The audio is sent sequentially to all transformer plugins, ordered by priority (developer defined), until finally it is sent to the STT stage","title":"Audio Transformers"},{"location":"speech_service/#modes-of-operation","text":"There are 3 modes to run dinkum, wakeword, hybrid, or continuous (VAD only) Additionally, there are 2 temporary modes that can be triggered via bus events / companion skills","title":"Modes of Operation"},{"location":"speech_service/#wake-word-mode","text":"","title":"Wake Word mode"},{"location":"speech_service/#sleep-mode","text":"Can be used via Naptime skill Be sure to enable a wakeup word to get out of sleep! \"listener\": { \"stand_up_word\": \"wake up\" }, \"hotwords\": { \"wake up\": { \"module\": \"ovos-ww-plugin-pocketsphinx\", \"phonemes\": \"W EY K . AH P\", \"threshold\": 1e-20, \"lang\": \"en-us\", \"wakeup\": true } }","title":"Sleep mode"},{"location":"speech_service/#continuous-mode","text":"EXPERIMENTAL - NEW in ovos-core version 0.0.8 \"listener\": { // continuous listen is an experimental setting, it removes the need for // wake words and uses VAD only, a streaming STT is strongly recommended // NOTE: depending on hardware this may cause mycroft to hear its own TTS responses as questions \"continuous_listen\": false }","title":"Continuous mode"},{"location":"speech_service/#hybrid-mode","text":"EXPERIMENTAL - NEW in ovos-core version 0.0.8 \"listener\": { // hybrid listen is an experimental setting, // it will not require a wake word for X seconds after a user interaction // this means you dont need to say \"hey mycroft\" for follow up questions \"hybrid_listen\": false, // number of seconds to wait for an interaction before requiring wake word again \"listen_timeout\": 45 }","title":"Hybrid mode"},{"location":"speech_service/#recording-mode","text":"EXPERIMENTAL - NEW in ovos-core version 0.0.8 Can be used via Recording skill","title":"Recording mode"},{"location":"standard_skills/","text":"Standard Skills Usage Your OpenVoiceOS device comes with certain skills pre-installed for basic functionality out of the box. You can also install new skills however more about that at a later stage. Date / Time skill You can ask your device what time or date it is just in case you lost your watch. Hey Mycroft, what time is it? Hey Mycroft, what is the date? Setting an alarm Having your OpenVoiceOS device knowing and showing the time is great, but it is even better to be woken up in the morning by your device. Hey Mycroft, set an alarm for 8 AM. Setting of timers Sometimes you are just busy but want to be alerted after a certain time. For that you can use timers. Hey Mycroft, set a timer for 5 minutes. You can always set more timers and even name them, so you know which timers is for what. Hey, Mycroft, set another timer called rice cooking for 7 minutes. Asking the weather You can ask your device what the weather is or would be at any given time or place. Hey Mycroft, what is the weather like today? The weather skill actually uses multiple pages indicated by the small dots at the bottom of the screen. File Browser The file browser allows you to browse the filesystem in your device and any connected media, you can view images and play music and videos. KDEConnect integration allows you to share files with your mobile devices","title":"Standard Skills Usage"},{"location":"standard_skills/#standard-skills-usage","text":"Your OpenVoiceOS device comes with certain skills pre-installed for basic functionality out of the box. You can also install new skills however more about that at a later stage.","title":"Standard Skills Usage"},{"location":"standard_skills/#date-time-skill","text":"You can ask your device what time or date it is just in case you lost your watch. Hey Mycroft, what time is it? Hey Mycroft, what is the date?","title":"Date / Time skill"},{"location":"standard_skills/#setting-an-alarm","text":"Having your OpenVoiceOS device knowing and showing the time is great, but it is even better to be woken up in the morning by your device. Hey Mycroft, set an alarm for 8 AM.","title":"Setting an alarm"},{"location":"standard_skills/#setting-of-timers","text":"Sometimes you are just busy but want to be alerted after a certain time. For that you can use timers. Hey Mycroft, set a timer for 5 minutes. You can always set more timers and even name them, so you know which timers is for what. Hey, Mycroft, set another timer called rice cooking for 7 minutes.","title":"Setting of timers"},{"location":"standard_skills/#asking-the-weather","text":"You can ask your device what the weather is or would be at any given time or place. Hey Mycroft, what is the weather like today? The weather skill actually uses multiple pages indicated by the small dots at the bottom of the screen.","title":"Asking the weather"},{"location":"standard_skills/#file-browser","text":"The file browser allows you to browse the filesystem in your device and any connected media, you can view images and play music and videos. KDEConnect integration allows you to share files with your mobile devices","title":"File Browser"},{"location":"statements/","text":"Statements Speaking a statement One of OVOS's most important core capabilities is to convert text to speech, that is, to speak a statement. Within a Skill's Intent handler, you may pass a string of text to OVOS and OVOS will speak it. For example: self.speak('this is my statement') . That's cool and fun to experiment with, but passing strings of text to Mycroft doesn't help to make Mycroft a multilingual product. Rather than hard-coded strings of text, OVOS has a design pattern for multilingualism. Multilingualism To support multilingualism, the text that OVOS speaks must come from a file. That file is called a dialog file. The dialog file contains statements (lines of text) that a listener in a particular language would consider to be equivalent. For instance, in USA English, the statements \"I am okay\" and \"I am fine\" are equivalent, and both of these statements might appear in a dialog file used for responding to the USA English question: \"How are you?\". By convention, the dialog filename is formed by dot connected words and must end with \".dialog\". The dialog filename should be descriptive of the contents as a whole. Sometimes, the filename describes the question being answered, and other times, the filename describes the answer itself. For the example above, the dialog filename might be: how.are.you.dialog or i.am.fine.dialog . Multilingualism is accomplished by translating the dialog files into other languages, and storing them in their own directory named for the country and language. The filenames remain the same. Using the same filenames in separate language dependent directories allows the Skills to be language agnostic; no hard-coded text strings. Adjust the language setting for your Device **** and OVOS uses the corresponding set of dialog files. If the desired file does not exist in the directory for that language, Mycroft will use the file from the USA English directory. As an example of the concept, the contents of how.are.you.dialog in the directory for the French language in France (fr-fr) might include the statement: \"Je vais bien\". The Tomato Skill Revisited To demonstrate the multilingualism design pattern, we examine the usage of the speak_dialog() method in the Tomato Skill . The Tomato Skill has two Intents: one demonstrates simple, straightforward statements, and the other demonstrates the use of variables within a statement. Simple statement The first Intent within the Tomato Skill, what.is.a.tomato.intent , handles inquiries about tomatoes, and the dialog file, tomato.description.dialog , provides the statements for OVOS to speak in reply to that inquiry. Sample contents of the Intent and dialog files: what.is.a.tomato.intent what is a tomato what would you say a tomato is describe a tomato what defines a tomato tomato.description.dialog The tomato is a fruit of the nightshade family A tomato is an edible berry of the plant Solanum lycopersicum A tomato is a fruit but nutrionists consider it a vegetable Observe the statements in the tomato.description.dialog file. They are all acceptable answers to the question: \"What is a tomato?\" Providing more than one statement in a dialog file is one way to make OVOS to seem less robotic, more natural. OVOS will randomly select one of the statements. The Tomato Skill code snippet: @intent_handler('what.is.a.tomato.intent') def handle_what_is(self, message): \"\"\"Speaks a statement from the dialog file.\"\"\" self.speak_dialog('tomato.description') With the Tomato Skill installed, if the User utters **** \"Hey Mycroft, what is a tomato?\", the Intent handler method handle_what_is() will be called. Inside handle_what_is() , we find: self.speak_dialog('tomato.description') As you can probably guess, the parameter 'tomato.description' is the dialog filename without the \".dialog\" extension. Calling this method opens the dialog file, selects one of the statements, and converts that text to speech. OVOS will speak a statement from the dialog file. In this example, OVOS might say \"The tomato is a fruit of the nightshade family\". Remember, OVOS has a language setting that determines from which directory to find the dialog file. File locations The Skill Structure section describes where to place the Intent file and dialog file. Basically, there are two choices: Put both files in locale/en-us Put the dialog file in dialog/en-us , and put the Intent file in vocab/en-us Statements with variables The second Padatious Intent, do.you.like.intent , demonstrates the use of variables in the Intent file and in one of the dialog files: do.you.like.intent do you like tomatoes do you like {type} tomatoes like.tomato.type.dialog I do like {type} tomatoes {type} tomatoes are my favorite like.tomato.generic.dialog I do like tomatoes tomatoes are my favorite Compare these two dialog files. The like.tomato.generic.dialog file contains only simple statements. The statements in the like.tomato.type.dialog file include a variable named type . The variable is a placeholder in the statement specifying where text may be inserted. The speak_dialog() method accepts a dictionary as an optional parameter. If that dictionary contains an entry for a variable named in the statement, then the value from the dictionary will be inserted at the placeholder's location. Dialog file variables are formed by surrounding the variable's name with curly braces. In OVOS parlance, curly braces are known as a mustache . For multi-line dialog files, be sure to include the same variable on all lines. The Tomato Skill code snippet: @intent_handler('do.you.like.intent') def handle_do_you_like(self, message): tomato_type = message.data.get('type') if tomato_type is not None: self.speak_dialog('like.tomato.type', {'type': tomato_type}) else: self.speak_dialog('like.tomato.generic') When the User utters \"Hey Mycroft, do you like RED tomatoes?\", the second of the two Intent lines \"do you like {type} tomatoes\" is recognized by Mycroft, and the value 'RED' is returned in the message dictionary assigned to the 'type' entry when handle_do_you_like() is called. The line tomato_type = message.data.get('type') extracts the value from the dictionary for the entry 'type'. In this case, the variable tomato_type will receive the value 'RED', and speak_dialog() will be called with the 'like.tomato.type' dialog file, and a dictionary with 'RED' assigned to 'type'. The statement \"I do like {type} tomatoes\" might be randomly selected, and after insertion of the value 'RED' for the placeholder variable {type}, OVOS would say: \"I do like RED tomatoes\". Should the User utter \"Hey Mycroft, do you like tomatoes?\", the first line in the Intent file \"do you like tomatoes\" is recognized. There is no variable in this line, and when handle_do_you_like() is called, the dictionary in the message is empty. This means tomato_type is None , speak_dialog('like.tomato.generic') would be called, and Mycroft might reply with \"Yes, I do like tomatoes\". Waiting for speech By default, the speak_dialog() method is non-blocking. That is any code following the call to speak_dialog() will execute whilst OVOS is talking. This is useful to allow your Skill to perform actions while it is speaking. Rather than telling the User that we are fetching some data, then going out to fetch it, we can do the two things simultaneously providing a better experience. However, there are times when we need to wait until the statement has been spoken before doing something else. We have two options for this. Wait Parameter We can pass a wait=True parameter to our speak_dialog() method. This makes the method blocking and no other code will execute until the statement has been spoken. @intent_handler('what.is.a.tomato.intent') def handle_what_is(self, message): \"\"\"Speaks a statement from the dialog file. Waits (i.e. blocks) within speak_dialog() until the speaking has completed. \"\"\" self.speak_dialog('tomato.description', wait=True) self.log.info(\"I waited for you\") Using translatable resources There may be a situation where the dialog file and the speak_dialog() method do not give the Skill enough flexibility. For instance, there may be a need to manipulate the statement from the dialog file before having it spoken by OVOS. The OVOSSkill class provides four multilingual methods to address these needs. Each method uses a file, and multilingualism is accomplished using the country/language directory system. The translate() method returns a random string from a \".dialog\" file (modified by a data dictionary). The translate_list() method returns a list of strings from a \".list\" file (each modified by the data dictionary). Same as translate_template() just with a different file extension. The translate_namedvalue() method returns a dictionary formed from CSV entries in a \".value\" file. The translate_template() method returns a list of strings from a \".template\" file (each modified by the data dictionary). Same as translate_list() just with a different file extension.","title":"Dialog"},{"location":"statements/#statements","text":"","title":"Statements"},{"location":"statements/#speaking-a-statement","text":"One of OVOS's most important core capabilities is to convert text to speech, that is, to speak a statement. Within a Skill's Intent handler, you may pass a string of text to OVOS and OVOS will speak it. For example: self.speak('this is my statement') . That's cool and fun to experiment with, but passing strings of text to Mycroft doesn't help to make Mycroft a multilingual product. Rather than hard-coded strings of text, OVOS has a design pattern for multilingualism.","title":"Speaking a statement"},{"location":"statements/#multilingualism","text":"To support multilingualism, the text that OVOS speaks must come from a file. That file is called a dialog file. The dialog file contains statements (lines of text) that a listener in a particular language would consider to be equivalent. For instance, in USA English, the statements \"I am okay\" and \"I am fine\" are equivalent, and both of these statements might appear in a dialog file used for responding to the USA English question: \"How are you?\". By convention, the dialog filename is formed by dot connected words and must end with \".dialog\". The dialog filename should be descriptive of the contents as a whole. Sometimes, the filename describes the question being answered, and other times, the filename describes the answer itself. For the example above, the dialog filename might be: how.are.you.dialog or i.am.fine.dialog . Multilingualism is accomplished by translating the dialog files into other languages, and storing them in their own directory named for the country and language. The filenames remain the same. Using the same filenames in separate language dependent directories allows the Skills to be language agnostic; no hard-coded text strings. Adjust the language setting for your Device **** and OVOS uses the corresponding set of dialog files. If the desired file does not exist in the directory for that language, Mycroft will use the file from the USA English directory. As an example of the concept, the contents of how.are.you.dialog in the directory for the French language in France (fr-fr) might include the statement: \"Je vais bien\".","title":"Multilingualism"},{"location":"statements/#the-tomato-skill-revisited","text":"To demonstrate the multilingualism design pattern, we examine the usage of the speak_dialog() method in the Tomato Skill . The Tomato Skill has two Intents: one demonstrates simple, straightforward statements, and the other demonstrates the use of variables within a statement.","title":"The Tomato Skill Revisited"},{"location":"statements/#simple-statement","text":"The first Intent within the Tomato Skill, what.is.a.tomato.intent , handles inquiries about tomatoes, and the dialog file, tomato.description.dialog , provides the statements for OVOS to speak in reply to that inquiry. Sample contents of the Intent and dialog files: what.is.a.tomato.intent what is a tomato what would you say a tomato is describe a tomato what defines a tomato tomato.description.dialog The tomato is a fruit of the nightshade family A tomato is an edible berry of the plant Solanum lycopersicum A tomato is a fruit but nutrionists consider it a vegetable Observe the statements in the tomato.description.dialog file. They are all acceptable answers to the question: \"What is a tomato?\" Providing more than one statement in a dialog file is one way to make OVOS to seem less robotic, more natural. OVOS will randomly select one of the statements. The Tomato Skill code snippet: @intent_handler('what.is.a.tomato.intent') def handle_what_is(self, message): \"\"\"Speaks a statement from the dialog file.\"\"\" self.speak_dialog('tomato.description') With the Tomato Skill installed, if the User utters **** \"Hey Mycroft, what is a tomato?\", the Intent handler method handle_what_is() will be called. Inside handle_what_is() , we find: self.speak_dialog('tomato.description') As you can probably guess, the parameter 'tomato.description' is the dialog filename without the \".dialog\" extension. Calling this method opens the dialog file, selects one of the statements, and converts that text to speech. OVOS will speak a statement from the dialog file. In this example, OVOS might say \"The tomato is a fruit of the nightshade family\". Remember, OVOS has a language setting that determines from which directory to find the dialog file.","title":"Simple statement"},{"location":"statements/#file-locations","text":"The Skill Structure section describes where to place the Intent file and dialog file. Basically, there are two choices: Put both files in locale/en-us Put the dialog file in dialog/en-us , and put the Intent file in vocab/en-us","title":"File locations"},{"location":"statements/#statements-with-variables","text":"The second Padatious Intent, do.you.like.intent , demonstrates the use of variables in the Intent file and in one of the dialog files: do.you.like.intent do you like tomatoes do you like {type} tomatoes like.tomato.type.dialog I do like {type} tomatoes {type} tomatoes are my favorite like.tomato.generic.dialog I do like tomatoes tomatoes are my favorite Compare these two dialog files. The like.tomato.generic.dialog file contains only simple statements. The statements in the like.tomato.type.dialog file include a variable named type . The variable is a placeholder in the statement specifying where text may be inserted. The speak_dialog() method accepts a dictionary as an optional parameter. If that dictionary contains an entry for a variable named in the statement, then the value from the dictionary will be inserted at the placeholder's location. Dialog file variables are formed by surrounding the variable's name with curly braces. In OVOS parlance, curly braces are known as a mustache . For multi-line dialog files, be sure to include the same variable on all lines. The Tomato Skill code snippet: @intent_handler('do.you.like.intent') def handle_do_you_like(self, message): tomato_type = message.data.get('type') if tomato_type is not None: self.speak_dialog('like.tomato.type', {'type': tomato_type}) else: self.speak_dialog('like.tomato.generic') When the User utters \"Hey Mycroft, do you like RED tomatoes?\", the second of the two Intent lines \"do you like {type} tomatoes\" is recognized by Mycroft, and the value 'RED' is returned in the message dictionary assigned to the 'type' entry when handle_do_you_like() is called. The line tomato_type = message.data.get('type') extracts the value from the dictionary for the entry 'type'. In this case, the variable tomato_type will receive the value 'RED', and speak_dialog() will be called with the 'like.tomato.type' dialog file, and a dictionary with 'RED' assigned to 'type'. The statement \"I do like {type} tomatoes\" might be randomly selected, and after insertion of the value 'RED' for the placeholder variable {type}, OVOS would say: \"I do like RED tomatoes\". Should the User utter \"Hey Mycroft, do you like tomatoes?\", the first line in the Intent file \"do you like tomatoes\" is recognized. There is no variable in this line, and when handle_do_you_like() is called, the dictionary in the message is empty. This means tomato_type is None , speak_dialog('like.tomato.generic') would be called, and Mycroft might reply with \"Yes, I do like tomatoes\".","title":"Statements with variables"},{"location":"statements/#waiting-for-speech","text":"By default, the speak_dialog() method is non-blocking. That is any code following the call to speak_dialog() will execute whilst OVOS is talking. This is useful to allow your Skill to perform actions while it is speaking. Rather than telling the User that we are fetching some data, then going out to fetch it, we can do the two things simultaneously providing a better experience. However, there are times when we need to wait until the statement has been spoken before doing something else. We have two options for this.","title":"Waiting for speech"},{"location":"statements/#wait-parameter","text":"We can pass a wait=True parameter to our speak_dialog() method. This makes the method blocking and no other code will execute until the statement has been spoken. @intent_handler('what.is.a.tomato.intent') def handle_what_is(self, message): \"\"\"Speaks a statement from the dialog file. Waits (i.e. blocks) within speak_dialog() until the speaking has completed. \"\"\" self.speak_dialog('tomato.description', wait=True) self.log.info(\"I waited for you\")","title":"Wait Parameter"},{"location":"statements/#using-translatable-resources","text":"There may be a situation where the dialog file and the speak_dialog() method do not give the Skill enough flexibility. For instance, there may be a need to manipulate the statement from the dialog file before having it spoken by OVOS. The OVOSSkill class provides four multilingual methods to address these needs. Each method uses a file, and multilingualism is accomplished using the country/language directory system. The translate() method returns a random string from a \".dialog\" file (modified by a data dictionary). The translate_list() method returns a list of strings from a \".list\" file (each modified by the data dictionary). Same as translate_template() just with a different file extension. The translate_namedvalue() method returns a dictionary formed from CSV entries in a \".value\" file. The translate_template() method returns a list of strings from a \".template\" file (each modified by the data dictionary). Same as translate_list() just with a different file extension.","title":"Using translatable resources"},{"location":"stt_plugins/","text":"STT Plugins STT plugins are responsible for converting spoken audio into text STT The base STT, this handles the audio in \"batch mode\" taking a complete audio file, and returning the complete transcription. Each STT plugin class needs to define the execute() method taking two arguments: audio ( AudioData object) - the audio data to be transcribed. lang (str) - optional - the BCP-47 language code The bare minimum STT class will look something like from ovos_plugin_manager.templates.stt import STT class MySTT(STT): def execute(audio, language=None): # Handle audio data and return transcribed text [...] return text StreamingSTT A more advanced STT class for streaming data to the STT. This will receive chunks of audio data as they become available and they are streamed to an STT engine. The plugin author needs to implement the create_streaming_thread() method creating a thread for handling data sent through self.queue . The thread this method creates should be based on the StreamThread class . handle_audio_data() method also needs to be implemented. Entry point To make the class detectable as an STT plugin, the package needs to provide an entry point under the mycroft.plugin.stt namespace. setup([...], entry_points = {'mycroft.plugin.stt': 'example_stt = my_stt:mySTT'} ) Where example_stt is is the STT module name for the plugin, my_stt is the Python module and mySTT is the class in the module to return. List of STT plugins Plugin Offline Streaming Type ovos-stt-plugin-fasterwhisper \u2714\ufe0f \u274c FOSS ovos-stt-plugin-whispercpp \u2714\ufe0f \u274c FOSS ovos-stt-plugin-vosk \u2714\ufe0f \u274c FOSS ovos-stt-plugin-chromium \u274c \u274c API (free) ovos-stt-plugin-http-server \u274c \u274c API (self hosted) ovos-stt-plugin-pocketsphinx \u2714\ufe0f \u274c FOSS ovos-stt-azure-plugin \u274c \u274c API (key) neon-stt-plugin-google_cloud_streaming \u274c \u2714 API (key) neon-stt-plugin-nemo \u2714\ufe0f \u2714\ufe0f FOSS neon-stt-plugin-nemo-remote \u274c\ufe0f \u274c API (self hosted) Standalone Usage STT plugins can be used in your owm projects as follows from speech_recognition import Recognizer, AudioFile plug = STTPlug() # verify lang is supported lang = \"en-us\" assert lang in plug.available_languages # read file with AudioFile(\"test.wav\") as source: audio = Recognizer().record(source) # transcribe AudioData object transcript = plug.execute(audio, lang) Plugin Template from ovos_plugin_manager.templates.stt import STT # base plugin class class MySTTPlugin(STT): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) # read config settings for your plugin lm = self.config.get(\"language-model\") hmm = self.config.get(\"acoustic-model\") def execute(self, audio, language=None): # TODO - convert audio into text and return string transcript = \"You said this\" return transcript @property def available_languages(self): \"\"\"Return languages supported by this STT implementation in this state This property should be overridden by the derived class to advertise what languages that engine supports. Returns: set: supported languages \"\"\" # TODO - what langs can this STT handle? return {\"en-us\", \"es-es\"} # sample valid configurations per language # \"display_name\" and \"offline\" provide metadata for UI # \"priority\" is used to calculate position in selection dropdown # 0 - top, 100-bottom # all other keys represent an example valid config for the plugin MySTTConfig = { lang: [{\"lang\": lang, \"display_name\": f\"MySTT ({lang}\", \"priority\": 70, \"offline\": True}] for lang in [\"en-us\", \"es-es\"] }","title":"Speech To Text"},{"location":"stt_plugins/#stt-plugins","text":"STT plugins are responsible for converting spoken audio into text","title":"STT Plugins"},{"location":"stt_plugins/#stt","text":"The base STT, this handles the audio in \"batch mode\" taking a complete audio file, and returning the complete transcription. Each STT plugin class needs to define the execute() method taking two arguments: audio ( AudioData object) - the audio data to be transcribed. lang (str) - optional - the BCP-47 language code The bare minimum STT class will look something like from ovos_plugin_manager.templates.stt import STT class MySTT(STT): def execute(audio, language=None): # Handle audio data and return transcribed text [...] return text","title":"STT"},{"location":"stt_plugins/#streamingstt","text":"A more advanced STT class for streaming data to the STT. This will receive chunks of audio data as they become available and they are streamed to an STT engine. The plugin author needs to implement the create_streaming_thread() method creating a thread for handling data sent through self.queue . The thread this method creates should be based on the StreamThread class . handle_audio_data() method also needs to be implemented.","title":"StreamingSTT"},{"location":"stt_plugins/#entry-point","text":"To make the class detectable as an STT plugin, the package needs to provide an entry point under the mycroft.plugin.stt namespace. setup([...], entry_points = {'mycroft.plugin.stt': 'example_stt = my_stt:mySTT'} ) Where example_stt is is the STT module name for the plugin, my_stt is the Python module and mySTT is the class in the module to return.","title":"Entry point"},{"location":"stt_plugins/#list-of-stt-plugins","text":"Plugin Offline Streaming Type ovos-stt-plugin-fasterwhisper \u2714\ufe0f \u274c FOSS ovos-stt-plugin-whispercpp \u2714\ufe0f \u274c FOSS ovos-stt-plugin-vosk \u2714\ufe0f \u274c FOSS ovos-stt-plugin-chromium \u274c \u274c API (free) ovos-stt-plugin-http-server \u274c \u274c API (self hosted) ovos-stt-plugin-pocketsphinx \u2714\ufe0f \u274c FOSS ovos-stt-azure-plugin \u274c \u274c API (key) neon-stt-plugin-google_cloud_streaming \u274c \u2714 API (key) neon-stt-plugin-nemo \u2714\ufe0f \u2714\ufe0f FOSS neon-stt-plugin-nemo-remote \u274c\ufe0f \u274c API (self hosted)","title":"List of STT plugins"},{"location":"stt_plugins/#standalone-usage","text":"STT plugins can be used in your owm projects as follows from speech_recognition import Recognizer, AudioFile plug = STTPlug() # verify lang is supported lang = \"en-us\" assert lang in plug.available_languages # read file with AudioFile(\"test.wav\") as source: audio = Recognizer().record(source) # transcribe AudioData object transcript = plug.execute(audio, lang)","title":"Standalone Usage"},{"location":"stt_plugins/#plugin-template","text":"from ovos_plugin_manager.templates.stt import STT # base plugin class class MySTTPlugin(STT): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) # read config settings for your plugin lm = self.config.get(\"language-model\") hmm = self.config.get(\"acoustic-model\") def execute(self, audio, language=None): # TODO - convert audio into text and return string transcript = \"You said this\" return transcript @property def available_languages(self): \"\"\"Return languages supported by this STT implementation in this state This property should be overridden by the derived class to advertise what languages that engine supports. Returns: set: supported languages \"\"\" # TODO - what langs can this STT handle? return {\"en-us\", \"es-es\"} # sample valid configurations per language # \"display_name\" and \"offline\" provide metadata for UI # \"priority\" is used to calculate position in selection dropdown # 0 - top, 100-bottom # all other keys represent an example valid config for the plugin MySTTConfig = { lang: [{\"lang\": lang, \"display_name\": f\"MySTT ({lang}\", \"priority\": 70, \"offline\": True}] for lang in [\"en-us\", \"es-es\"] }","title":"Plugin Template"},{"location":"stt_server/","text":"OpenVoiceOS STT HTTP Server Turn any OVOS STT plugin into a microservice! Companion plugin Use with OpenVoiceOS companion plugin Public Servers Member Service Plugin URL STT ovos-stt-plugin-server Faster Whisper - https://fasterwhisper.ziggyai.online/stt STT ovos-stt-plugin-server Faster Whisper - https://stt.smartgic.io/fasterwhisper Chromium - https://stt.smartgic.io/chromium Nemo - https://stt.smartgic.io/nemo STT ovos-stt-server-plugin Nemo - https://nemo.neonaialpha.com Install pip install ovos-stt-http-server Usage ovos-stt-server --help usage: ovos-stt-server [-h] [--engine ENGINE] [--port PORT] [--host HOST] options: -h, --help show this help message and exit --engine ENGINE stt plugin to be used --port PORT port number --host HOST host Docker Pre built containers can be found in the ovos-docker-stt repository Template you can create easily create a docker file to serve any plugin FROM python:3.7 RUN pip3 install ovos-stt-http-server==0.0.1 RUN pip3 install {PLUGIN_HERE} ENTRYPOINT ovos-stt-http-server --engine {PLUGIN_HERE} build it docker build . -t my_ovos_stt_plugin run it docker run -p 8080:9666 my_ovos_stt_plugin Each plugin can provide its own Dockerfile in its repository using ovos-stt-http-server","title":"STT Server"},{"location":"stt_server/#openvoiceos-stt-http-server","text":"Turn any OVOS STT plugin into a microservice!","title":"OpenVoiceOS STT HTTP Server"},{"location":"stt_server/#companion-plugin","text":"Use with OpenVoiceOS companion plugin","title":"Companion plugin"},{"location":"stt_server/#public-servers","text":"Member Service Plugin URL STT ovos-stt-plugin-server Faster Whisper - https://fasterwhisper.ziggyai.online/stt STT ovos-stt-plugin-server Faster Whisper - https://stt.smartgic.io/fasterwhisper Chromium - https://stt.smartgic.io/chromium Nemo - https://stt.smartgic.io/nemo STT ovos-stt-server-plugin Nemo - https://nemo.neonaialpha.com","title":"Public Servers"},{"location":"stt_server/#install","text":"pip install ovos-stt-http-server","title":"Install"},{"location":"stt_server/#usage","text":"ovos-stt-server --help usage: ovos-stt-server [-h] [--engine ENGINE] [--port PORT] [--host HOST] options: -h, --help show this help message and exit --engine ENGINE stt plugin to be used --port PORT port number --host HOST host","title":"Usage"},{"location":"stt_server/#docker","text":"Pre built containers can be found in the ovos-docker-stt repository","title":"Docker"},{"location":"stt_server/#template","text":"you can create easily create a docker file to serve any plugin FROM python:3.7 RUN pip3 install ovos-stt-http-server==0.0.1 RUN pip3 install {PLUGIN_HERE} ENTRYPOINT ovos-stt-http-server --engine {PLUGIN_HERE} build it docker build . -t my_ovos_stt_plugin run it docker run -p 8080:9666 my_ovos_stt_plugin Each plugin can provide its own Dockerfile in its repository using ovos-stt-http-server","title":"Template"},{"location":"timeline/","text":"Family Tree mycroft-core created neon-core forked from mycroft-core chatterbox forked from mycroft-core (closed source) mycroft-lib forked from mycroft-core to become a library, it is only a properly packaged mycroft-core chatterbox rewritten to use mycroft-lib internally, no longer a hard fork neon-core rewritten to use mycroft-lib internally, no longer a hard fork mycroft-lib renamed to HolmesIV to avoid trademark issues HolmesV created to use HolmesIV internally, HolmesV gets features and HolmesIV bug fixes only chatterbox updated to HolmesV neon-core updated to HolmesV HolmesIV+HolmesV abandoned, chatterbox focus on closed source product ovos-core forked from HolmesV neon-core updated to ovos-core mycroft-dinkum forked from mycroft-core Events timeline Aug 2015 - MycroftAI kickstarter launch Feb 2016 - MycroftAI Mimic TTS released May 2016 - mycroft-core repositories made public under GPL Jun 2016 - @Aix releases MycroftAI gnome shell extension Jun 2016 - @Aix becomes a contributor on the desktop-integration team Aug 2016 - Steve Penrod becomes MycroftAI CTO ??? 2016 - NeonGecko begins working with Mycroft AI Jan 2017 - @Aix release MycrofAI plasmoid for KDE Plasma desktop Mar 2017 - @Aix incubates Mycroft plasmoid project under KDE Apr 2017 - @JarbasAI forks mycroft-core as jarbas-core May 2017 - @JarbasAI becomes a mycroft-core contributor Sep 2017 - MycroftAI Mark 1 released / mycroft kickstarter campaign fullfilled Sep 2017 - @Aix joins Blue Systems to work on MycroftAI and Plasma integration projects Sep 2017 - jarbas-core tacotron TTS integration added Aug 2017 - MycroftAI starts supporting KDE plasmoid project and installers Oct 2017 - mycroft-core relicensed as Apache 2.0 Nov 2017 - @Aix becomes a mycroft-core contributor Dec 2017 - jarbas-core abandoned ??? 2017 - Neon AI forks mycroft-core as neon-core to remove dependency on MycroftAI servers and work on other features Jan 2018 - initial release of personal-backend , reverse engineered MycroftAI backend by @JarbasAI, licensed as Apache 2.0 Jan 2018 - MycroftAI mark 2 kickstarter launch Jul 2018 - personal-backend added to MycroftAI Roadmap Aug 2018 - MycroftAI Mimic2 TTS based on tacotron released Sep 2018 - Mycroft-GUI was created by collaboration between MycroftAI, Blue Systems based on KDE frameworks, maintained by @Aix Oct 2018 - @j1nx creates \"MycroftOS\" , the first version of what will eventually become the OpenVoiceOS smart speaker Jan 2019 - @JarbasAI personal-backend implementation adopted by MycroftAI Mar 2019 - MycroftAI mark 2 completely redesigned and based on different hardware Apr 2019 - Steve Penrod and @JarbasAI create lingua-franca under MycroftAI with @JarbasAI as maintainer ??? 2019 - @JarbasAI partners with NeonAI to maintain neon-core Jul 2019 - steve penrod leaves MycroftAI Jul 2019 - Chatterbox kickstarter launched by @JarbasAI Sep 2019 - Mycroft on Plasma Automobile Demo at Akademy 2019 by @aix Oct 2019 - Official MycroftAI backend open sourced, licensed as AGPL Dec 2019 - @Chance joins lingua-franca as a maintainer Dec 2019 - Chatterbox released (closed source) / kickstarter campaign fullfilled Dec 2019 - MycroftAI starts to work on Kivy based UI ??? 2020 - MycroftAI starts neglecting community contributions Mar 2020 - MycroftAI abandons personal-backend Mar 2020 - Michael Lewis becomes MycroftAI CEO May 2020 - @JarbasAI and @AIX partner up to create a VOIP skill Sep 2020 - Community takes stand against reimplementation of GUI using Kivy Oct 2020 - Kivy has been dropped in support for QT Mycroft-GUI Oct 2020 - @JarbasAI, @AIX and @j1nx form OpenVoiceOS community project around mycroft-core Oct 2020 - ovos-utils transferred from @Jarbasai to OVOS and renamed from jarbas-utils ??? 2020 - Chatterbox forks mycroft-core as mycroft-lib (open source) ??? 2020 - mycroft-lib reorganizes mycroft imports cleanly separating each service ??? 2020 - Chatterbox (closed source) recreated on top of mycroft-lib ??? 2020 - neon-core recreated on top of mycroft-lib Oct 2020 - MycroftOS renamed to OpenVoiceOS - Mycroft Edition due to trademark issues ??? 2020 - @JarbasAI leaves lingua-franca ??? 2020 - @Chance joins OpenVoiceOS Dec 2020 - ovos-ww-plugin-pocketsphinx released Dec 2020 - ovos-ww-plugin-snowboy released Dec 2020 - ovos-ww-plugin-precise released Dec 2020 - ovos-stt-plugin-vosk released Dec 2020 - ovos-stt-plugin-chromium released Jan 2021 - ovos-plugin-manager released Jan 2021 - personal-backend adopted by OpenVoiceOS, original repo unarchived and ownership transferred Jan 2021 - Mycroft embedded shell adopted by OpenVoiceOS and renamed to ovos-shell Jan 2021 - skill-ovos-setup forked from Mycroft to replace pairing Jan 2021 - ovos-skill-manager released to support more skill Marketplaces Feb 2021 - skill-ovos-stop forked from Mycroft Mar 2021 - skill-ovos-common-play forked from Mycroft Mar 2021 - MycroftAI mark 2 dev kits start shipping Apr 2021 - OpenVoiceOS adopts mycroft-lib instead of mycroft-core ??? 202? - mycroft-lib renamed to HolmesIV to avoid trademark issues Apr 2021 - ovos-workshop released, bringing OVOS features to individual skills May 2021 - chatterbox forks lingua-franca as lingua-nostra May 2021 . OpenVoiceOS deprecates mycroft-lib and adopts HolmesIV Jun 2021 - ovos-tts-plugin-espeakNG released Jun 2021 - ovos-tts-plugin-mimic released Jun 2021 - ovos-tts-plugin-mimic2 released Jun 2021 - ovos-tts-plugin-pico released Aug 2021 - ovos-tts-plugin-google-tx released Aug 2021 - ovos-ww-plugin-vosk released Aug 2021 - precise-lite forked from Mycroft, adding tflite support Aug 2021 - ovos-ww-plugin-precise-lite released Aug 2021 - ovos-ww-plugin-nyumaya released Aug 2021 - precise-lite-models start being shared Aug 2021 - skill-ovos-volume released Sep 2021 - VocalFusionDriver released by OVOS to support the mk2 Sep 2021 - ovos-tts-plugin-SAM released Sep 2021 - backend made optional in HolmesIV Sep 2021 - msm made optional in HolmesIV Oct 2021 - \"instant_listen\" introduced in HolmesIV Oct 2021 - HolmesIV abandoned by chatterbox Oct 2021 - lingua-nostra abandoned by chatterbox Oct 2021 - OpenVoiceOS forks HolmesIV as ovos-core Oct 2021 - ovos-core becomes XDG compliant Oct 2021 - neon-core deprecates HolmesIV and adopts ovos-core Oct 2021 - skill-ovos-common-play deprecated in favor of OCP ??? 2021 - @Chance leaves lingua-franca Nov 2021 - ovos-plugin-manager released Nov 2021 - skill-ovos-timer forked from Mycroft Nov 2021 - skill-ovos-homescreen forked from Mycroft Nov 2021 - @JarbasAI leaves chatterbox Nov 2021 - ovos-core version 0.0.1 released Nov 2021 - ovos-utils adopted in ovos-core Nov 2021 - ovos-plugin-manager adopted in ovos-core Nov 2021 - multiple wake words support added Nov 2021 - installable skills (setup.py) support added Nov 2021 - ovos-PHAL released (mycroft compatible) Nov 2021 - skill-ovos-hello-world forked from Mycroft Nov 2021 - skill-ovos-naptime forked from Mycroft ??? - @NeonDaniel joins OpenVoiceOS ??? 2021 - NeonAI adopts ovos-shell Dec 2021 - ovos-PHAL-plugin-mk1 released Dec 2021 - skill-ovos-fallback-unknown forked from Mycroft Dec 2021 - skill-ovos-weather forked from Mycroft Dec 2021 - skill-ovos-common-query forked from Mycroft Dec 2021 - skill-ovos-application-launcher released Jan 2022 - OpenVoiceOS forks lingua-franca ??? 2022 - OpenVoiceOS starts development in the open via matrix chat Feb 2022 - OCP released as an audio plugin (mycroft compatible) Feb 2022 - PHAL replaces mycroft.client.enclosure in ovos-core Feb 2022 - skill-ovos-date-time forked from Mycroft Mar 2022 - Fallback STT support added Mar 2022 - VAD plugins support added Mar 2022 - ovos-vad-plugin-webrtcvad released Mar 2022 - ovos-vad-plugin-silero released Mar 2022 - OCP adopted in ovos-core as default media handler Mar 2022 - ovos-PHAL-plugin-mk2 released Mar 2022 - ovos-PHAL-plugin-respeaker-2mic released Mar 2022 - ovos-PHAL-plugin-respeaker-4mic released ??? 2022 - OpenVoiceOS starts releasing buildroot images for rpi4 and Mark 2 ??? 2022 - OpenVoiceOS starts releasing manjaro images for rpi4 and Mark 2 Apr 2022 - ovos-stt-http-server released Apr 2022 - ovos-stt-plugin-server released May 2022 - ovos-tts-plugin-beepspeak forked from chatterbox May 2022 - ovos-tts-plugin-marytts released May 2022 - ovos-tts-plugin-polly forked from chatterbox May 2022 - ovos-translate-server released May 2022 - ovos-core version 0.0.3 released May 2022 - MycroftAI founders resign Jun 2022 - ovos-config adopted in ovos-core Jun 2022 - skill-ovos-alarm forked from Mycroft Jun 2022 - skill-ovos-qml-camera forked from Mycroft Jun 2022 - Plasma Bigscreen drops mycroft-core in favor of ovos-core version 0.0.4 (alpha) Jul 2022 - MycroftAI starts work on mycroft-dinkum behind the scenes, a replacement/rewrite of mycroft-core Jul 2022 - ovos-tts-plugin-mimic3 forked from Mycroft Jul 2022 - skill-homescreen-lite released Aug 2022 - padacioso transfered from @JarbasAi to OVOS Aug 2022 - adopt padacioso as a optional padatious alternative to avoid libfann (LGPL) Aug 2022 - ovos-core version 0.0.4 released Aug 2022 - experimental support for \"continuous_listening\" and \"hybrid_listening\" added Sep 2022 - MycroftAI Mimic3 TTS released, AGPL licensed Sep 2022 . MycroftAI Mark 2 starts shipping Sep 2022 - skill-ovos-news transferred from @JarbasAl to OVOS Oct 2022 - ovos-backend-manager UI released Oct 2022 - ovos-stt-plugin-whispercpp released Oct 2022 - new ask_yesno parser added Oct 2022 - ovos-backend-client adopted by ovos-core , selectable backend support added (offline, personal, selene) Oct 2022 - ovos-tts-plugin-mimic3-server released Oct 2022 - mycroft.blue-systems.com mimic 3 public server added Oct 2022 - mimic3.ziggyai.online mimic 3 public server added Oct 2022 - @aix talks about OVOS in KDE Akademy Oct 2022 - skill-ovos-soundcloud transferred from @JarbasAl to OVOS Oct 2022 - skill-ovos-youtube transferred from @JarbasAl to OVOS ??? 2022 - mycroft-dinkum source code made public, Apache2 licensed ??? 2022 - Extremely negative feedback from developer community, several key members annouce no intention to support mycroft-dinkum ??? 2022 - NeonAI starts release NeonOS images for the Mark 2 Nov 2022 - ovos-core version 0.0.5 released Nov 2022 - MycroftAI staff lay off, only a skeleton crew remaining Nov 2022 - ovos-bus-client forked from Mycroft Nov 2022 - tts.smartgic.io/mimic3 public Mimic3 server added Dec 2022 - Home Assistant PHAL plugin initial release by @AIX , exposes HA devices to the messagebus Dec 2022 - skill-ovos-youtube-music transferred from @JarbasAl to OVOS Dec 2022 - skill-ovos-bandcamp transferred from @JarbasAl to OVOS Jan 2023 - ovos-core version 0.0.6 released Jan 2023 - negative user feedback from the community for Mark 2, lots of bugs and missing features Jan 2023 - \"fallback\" wake word support added to ovos-plugin-manager Jan 2023 - skill-ovos-local-media released Jan 2023 - ChatGPT skill released Feb 2023 - ovos-config cli tool by community member @sgee released Feb 2023 - ovos-solver-plugin-aiml released Feb 2023 - ovos-solver-plugin-rivescript released Feb 2023 - skill-ovos-somafm transferred from @JarbasAl to OVOS Feb 2023 - MycroftAI partners up with NeonAI to maintain mycroft-core(?) Mar 2023 - Mimic3 TTS public servers become default OVOS voice (alan pope) Mar 2023 - ovos-core version 0.0.7 released Mar 2023 - Fundraiser to form OpenVoiceOS V.z.w. (Dutch: \"Vereninging zonder winstoogmerk\") completed in a couple days Mar 2023 - First stretch goal of fundraiser reached and second stretch goal announced Mar 2023 - raspbian-ovos images start being released, maintained by community member @builderjer Mar 2023 - community docs start being maintained by community members Mar 2023 - ovos-ww-plugin-openWakeWord released, maintained by author @dscripka Mar 2023 - skill-ovos-icanhazdadjokes transferred from @JarbasAl to OVOS Mar 2023 - ovos-skill-alerts forked from Neon, maintained by community member @sgee Apr 2023 - ovos-core splits ovos-audio , ovos-listener , ovos-gui and ovos-bus into their own packages Apr 2023 - @Aix leaves OpenVoiceOS Apr 2023 - OpenVoiceOS stops releasing manjaro based images Apr 2023 - ovos-stt-plugin-fasterwhisper released Apr 2023 - ovos-tts-plugin-piper released Apr 2023 - precise-lite-trainer released Apr 2023 - ovos-vad-plugin-precise released Apr 2023 - ovos-dinkum-listener released Apr 2023 - ovos-translate-plugin-deepl released, maintained by community member @sgee Apr 2023 - mycroft-classic-listener released, to preserve original mark 1 listener Apr 2023 - skill-ovos-tunein transferred from @JarbasAl to OVOS, maintained by community member @sgee Apr 2023 - jurebes intent parser released May 2023 - mycroft import deprecated in favor of ovos_core module for skills service May 2023 - stt.openvoiceos.org moves to whisper (small, cpu only) May 2023 - ovos-docker released, maintained by community member @goldyfruit May 2023 - Open Voice OS TTS/STT status page released, maintained by community member @goldyfruit May 2023 - First successful run of OpenVoiceOS on Mac OS using containers with ovos-docker May 2023 - ovos-docker-stt STT containers released, maintained by community member @goldyfruit May 2023 - ovos-microphone-plugin-sounddevice released, which provides native Mac OS suuport, maintained by community member @goldyfruit May 2023 - ovos-persona alpha release May 2023 - ovos-audio-transformer-plugin-speechbrain-langdetect released May 2023 - ovos-skill-easter-eggs transferred from @JarbasAl to OVOS, maintained by community member @mikejgray May 2023 - skill-ovos-dismissal transferred from @ChanceNCounter to OVOS May 2023 - skill-ovos-dictation transferred from @JarbasAl to OVOS Jun 2023 - Home Assistant plugin starts being maintained by community member @mikejgray Jun 2023 - quebra_frases transferred from @JarbasAl to OVOS Jun 2023 - ovos-translate-plugin-nllb released Jun 2023 - fasterwhisper.ziggyai.online public STT server added (large, GPU) Jun 2023 - Home Assistant Notify integration released by community member @mikejgray Jun 2023 - First (and second!) successful run of OpenVoiceOS on Windows, using WSL2 and ovos-docker Jun 2023 - ovos-docker-tts TTS containers released, maintained by community member @goldyfruit Jun 2023 - ovos-tts-plugin-azure released Jun 2023 - ovos-utterance-corrections-plugin released Jul 2023 - mycroft-gui-qt6 forked from mycroft-gui at last commit supporting QT6 before license change to GPL (reverted shortly after) Jul 2023 - mycroft-gui-qt5 forked from mycroft-gui at last commit supporting QT5 Jul 2023 - pipertts.ziggyai.online public TTS server added Jul 2023 - tts.smartgic.io/piper public TTS server added Jul 2023 - piper TTS public servers become default OVOS voice (alan pope) Jul 2023 - skill-ovos-spotify port of the mycroft-spotify skill by community member and original author @forslund Aug 2023 - ovos-translate-server-plugin released Aug 2023 - ovos-docker-tx translation containers released, maintained by community member @goldyfruit Aug 2023 - nllb.openvoiceos.org public translation server added Aug 2023 - translator.smartgic.io/nllb public translation server added Aug 2023 - adopt NLLB public servers as default translation plugin Aug 2023 - skill-ovos-wolfie transferred from @JarbasAl to OVOS Aug 2023 - skill-ovos-ddg transferred from @JarbasAl to OVOS Aug 2023 - skill-ovos-wikipedia transferred from @JarbasAl to OVOS Aug 2023 - ovos-stt-azure-plugin released Sep 2023 - skill-ovos-parrot transferred from @JarbasAl to OVOS Sep 2023 - stt.smartgic.io/fasterwhisper public STT server (large, GPU) Sep 2023 - GUI fully functional with ovos-docker containers Sep 2023 - persona-server alpha version released Sep 2023 - ovos-audio-transformer-plugin-ggwave released Oct 2023 - ovosnllb.ziggyai.online public translation server added Oct 2023 - ovos-tts-plugin-mimic3-server deprecated Oct 2023 - ovos-PHAL-sensors released, exposing OVOS sensors in Home Assistant Oct 2023 - ovos-bidirectional-translation-plugin released Nov 2023 - Plasma Bigscreen moves to QT6 and explicitly drops support for OVOS Dec 2023 - ovos-installer first release! codename Duke Nukem , maintained by community member @goldyfruit Dec 2023 - ovos-logs cli tool by community member @sgee added to ovos-utils Dec 2023 - ovos-docs-viewer cli tool released Dec 2023 - skill-ovos-spelling forked from Mycroft Dec 2023 - skill-ovos-ip forked from Mycroft Dec 2023 - skill-ovos-wallpapers transferred to OVOS Dec 2023 - ovos-i2csound released by community member @builderjer ??? 202? - ovos-tts-plugin-mimic2 deprecated Jan 2024 - skill-ovos-boot-finished forked from Neon Jan 2024 - skill-ovos-audio-recording forked from Neon Jan 2024 - ovos-utterance-plugin-cancel forked from Neon, deprecates dismissal skill Jan 2024 - ovos-mark1-utils released Jan 2024 - Mycroft forums move to Open Conversational AI Jan 2024 - ovos-vad-plugin-noise released to support older platforms Feb 2024 - ovos-tts-plugin-edge-tts released Feb 2024 - Selene servers and Mycroft AI website go down Feb 2024 - skill-ovos-randomness released, maintained by community member @mikejgray Feb 2024 - @Chance leaves OpenVoiceOS Feb 2024 - skill-ovos-wordnet released Near Future - ovos-core version 0.0.8 released","title":"Timeline"},{"location":"timeline/#family-tree","text":"mycroft-core created neon-core forked from mycroft-core chatterbox forked from mycroft-core (closed source) mycroft-lib forked from mycroft-core to become a library, it is only a properly packaged mycroft-core chatterbox rewritten to use mycroft-lib internally, no longer a hard fork neon-core rewritten to use mycroft-lib internally, no longer a hard fork mycroft-lib renamed to HolmesIV to avoid trademark issues HolmesV created to use HolmesIV internally, HolmesV gets features and HolmesIV bug fixes only chatterbox updated to HolmesV neon-core updated to HolmesV HolmesIV+HolmesV abandoned, chatterbox focus on closed source product ovos-core forked from HolmesV neon-core updated to ovos-core mycroft-dinkum forked from mycroft-core","title":"Family Tree"},{"location":"timeline/#events-timeline","text":"Aug 2015 - MycroftAI kickstarter launch Feb 2016 - MycroftAI Mimic TTS released May 2016 - mycroft-core repositories made public under GPL Jun 2016 - @Aix releases MycroftAI gnome shell extension Jun 2016 - @Aix becomes a contributor on the desktop-integration team Aug 2016 - Steve Penrod becomes MycroftAI CTO ??? 2016 - NeonGecko begins working with Mycroft AI Jan 2017 - @Aix release MycrofAI plasmoid for KDE Plasma desktop Mar 2017 - @Aix incubates Mycroft plasmoid project under KDE Apr 2017 - @JarbasAI forks mycroft-core as jarbas-core May 2017 - @JarbasAI becomes a mycroft-core contributor Sep 2017 - MycroftAI Mark 1 released / mycroft kickstarter campaign fullfilled Sep 2017 - @Aix joins Blue Systems to work on MycroftAI and Plasma integration projects Sep 2017 - jarbas-core tacotron TTS integration added Aug 2017 - MycroftAI starts supporting KDE plasmoid project and installers Oct 2017 - mycroft-core relicensed as Apache 2.0 Nov 2017 - @Aix becomes a mycroft-core contributor Dec 2017 - jarbas-core abandoned ??? 2017 - Neon AI forks mycroft-core as neon-core to remove dependency on MycroftAI servers and work on other features Jan 2018 - initial release of personal-backend , reverse engineered MycroftAI backend by @JarbasAI, licensed as Apache 2.0 Jan 2018 - MycroftAI mark 2 kickstarter launch Jul 2018 - personal-backend added to MycroftAI Roadmap Aug 2018 - MycroftAI Mimic2 TTS based on tacotron released Sep 2018 - Mycroft-GUI was created by collaboration between MycroftAI, Blue Systems based on KDE frameworks, maintained by @Aix Oct 2018 - @j1nx creates \"MycroftOS\" , the first version of what will eventually become the OpenVoiceOS smart speaker Jan 2019 - @JarbasAI personal-backend implementation adopted by MycroftAI Mar 2019 - MycroftAI mark 2 completely redesigned and based on different hardware Apr 2019 - Steve Penrod and @JarbasAI create lingua-franca under MycroftAI with @JarbasAI as maintainer ??? 2019 - @JarbasAI partners with NeonAI to maintain neon-core Jul 2019 - steve penrod leaves MycroftAI Jul 2019 - Chatterbox kickstarter launched by @JarbasAI Sep 2019 - Mycroft on Plasma Automobile Demo at Akademy 2019 by @aix Oct 2019 - Official MycroftAI backend open sourced, licensed as AGPL Dec 2019 - @Chance joins lingua-franca as a maintainer Dec 2019 - Chatterbox released (closed source) / kickstarter campaign fullfilled Dec 2019 - MycroftAI starts to work on Kivy based UI ??? 2020 - MycroftAI starts neglecting community contributions Mar 2020 - MycroftAI abandons personal-backend Mar 2020 - Michael Lewis becomes MycroftAI CEO May 2020 - @JarbasAI and @AIX partner up to create a VOIP skill Sep 2020 - Community takes stand against reimplementation of GUI using Kivy Oct 2020 - Kivy has been dropped in support for QT Mycroft-GUI Oct 2020 - @JarbasAI, @AIX and @j1nx form OpenVoiceOS community project around mycroft-core Oct 2020 - ovos-utils transferred from @Jarbasai to OVOS and renamed from jarbas-utils ??? 2020 - Chatterbox forks mycroft-core as mycroft-lib (open source) ??? 2020 - mycroft-lib reorganizes mycroft imports cleanly separating each service ??? 2020 - Chatterbox (closed source) recreated on top of mycroft-lib ??? 2020 - neon-core recreated on top of mycroft-lib Oct 2020 - MycroftOS renamed to OpenVoiceOS - Mycroft Edition due to trademark issues ??? 2020 - @JarbasAI leaves lingua-franca ??? 2020 - @Chance joins OpenVoiceOS Dec 2020 - ovos-ww-plugin-pocketsphinx released Dec 2020 - ovos-ww-plugin-snowboy released Dec 2020 - ovos-ww-plugin-precise released Dec 2020 - ovos-stt-plugin-vosk released Dec 2020 - ovos-stt-plugin-chromium released Jan 2021 - ovos-plugin-manager released Jan 2021 - personal-backend adopted by OpenVoiceOS, original repo unarchived and ownership transferred Jan 2021 - Mycroft embedded shell adopted by OpenVoiceOS and renamed to ovos-shell Jan 2021 - skill-ovos-setup forked from Mycroft to replace pairing Jan 2021 - ovos-skill-manager released to support more skill Marketplaces Feb 2021 - skill-ovos-stop forked from Mycroft Mar 2021 - skill-ovos-common-play forked from Mycroft Mar 2021 - MycroftAI mark 2 dev kits start shipping Apr 2021 - OpenVoiceOS adopts mycroft-lib instead of mycroft-core ??? 202? - mycroft-lib renamed to HolmesIV to avoid trademark issues Apr 2021 - ovos-workshop released, bringing OVOS features to individual skills May 2021 - chatterbox forks lingua-franca as lingua-nostra May 2021 . OpenVoiceOS deprecates mycroft-lib and adopts HolmesIV Jun 2021 - ovos-tts-plugin-espeakNG released Jun 2021 - ovos-tts-plugin-mimic released Jun 2021 - ovos-tts-plugin-mimic2 released Jun 2021 - ovos-tts-plugin-pico released Aug 2021 - ovos-tts-plugin-google-tx released Aug 2021 - ovos-ww-plugin-vosk released Aug 2021 - precise-lite forked from Mycroft, adding tflite support Aug 2021 - ovos-ww-plugin-precise-lite released Aug 2021 - ovos-ww-plugin-nyumaya released Aug 2021 - precise-lite-models start being shared Aug 2021 - skill-ovos-volume released Sep 2021 - VocalFusionDriver released by OVOS to support the mk2 Sep 2021 - ovos-tts-plugin-SAM released Sep 2021 - backend made optional in HolmesIV Sep 2021 - msm made optional in HolmesIV Oct 2021 - \"instant_listen\" introduced in HolmesIV Oct 2021 - HolmesIV abandoned by chatterbox Oct 2021 - lingua-nostra abandoned by chatterbox Oct 2021 - OpenVoiceOS forks HolmesIV as ovos-core Oct 2021 - ovos-core becomes XDG compliant Oct 2021 - neon-core deprecates HolmesIV and adopts ovos-core Oct 2021 - skill-ovos-common-play deprecated in favor of OCP ??? 2021 - @Chance leaves lingua-franca Nov 2021 - ovos-plugin-manager released Nov 2021 - skill-ovos-timer forked from Mycroft Nov 2021 - skill-ovos-homescreen forked from Mycroft Nov 2021 - @JarbasAI leaves chatterbox Nov 2021 - ovos-core version 0.0.1 released Nov 2021 - ovos-utils adopted in ovos-core Nov 2021 - ovos-plugin-manager adopted in ovos-core Nov 2021 - multiple wake words support added Nov 2021 - installable skills (setup.py) support added Nov 2021 - ovos-PHAL released (mycroft compatible) Nov 2021 - skill-ovos-hello-world forked from Mycroft Nov 2021 - skill-ovos-naptime forked from Mycroft ??? - @NeonDaniel joins OpenVoiceOS ??? 2021 - NeonAI adopts ovos-shell Dec 2021 - ovos-PHAL-plugin-mk1 released Dec 2021 - skill-ovos-fallback-unknown forked from Mycroft Dec 2021 - skill-ovos-weather forked from Mycroft Dec 2021 - skill-ovos-common-query forked from Mycroft Dec 2021 - skill-ovos-application-launcher released Jan 2022 - OpenVoiceOS forks lingua-franca ??? 2022 - OpenVoiceOS starts development in the open via matrix chat Feb 2022 - OCP released as an audio plugin (mycroft compatible) Feb 2022 - PHAL replaces mycroft.client.enclosure in ovos-core Feb 2022 - skill-ovos-date-time forked from Mycroft Mar 2022 - Fallback STT support added Mar 2022 - VAD plugins support added Mar 2022 - ovos-vad-plugin-webrtcvad released Mar 2022 - ovos-vad-plugin-silero released Mar 2022 - OCP adopted in ovos-core as default media handler Mar 2022 - ovos-PHAL-plugin-mk2 released Mar 2022 - ovos-PHAL-plugin-respeaker-2mic released Mar 2022 - ovos-PHAL-plugin-respeaker-4mic released ??? 2022 - OpenVoiceOS starts releasing buildroot images for rpi4 and Mark 2 ??? 2022 - OpenVoiceOS starts releasing manjaro images for rpi4 and Mark 2 Apr 2022 - ovos-stt-http-server released Apr 2022 - ovos-stt-plugin-server released May 2022 - ovos-tts-plugin-beepspeak forked from chatterbox May 2022 - ovos-tts-plugin-marytts released May 2022 - ovos-tts-plugin-polly forked from chatterbox May 2022 - ovos-translate-server released May 2022 - ovos-core version 0.0.3 released May 2022 - MycroftAI founders resign Jun 2022 - ovos-config adopted in ovos-core Jun 2022 - skill-ovos-alarm forked from Mycroft Jun 2022 - skill-ovos-qml-camera forked from Mycroft Jun 2022 - Plasma Bigscreen drops mycroft-core in favor of ovos-core version 0.0.4 (alpha) Jul 2022 - MycroftAI starts work on mycroft-dinkum behind the scenes, a replacement/rewrite of mycroft-core Jul 2022 - ovos-tts-plugin-mimic3 forked from Mycroft Jul 2022 - skill-homescreen-lite released Aug 2022 - padacioso transfered from @JarbasAi to OVOS Aug 2022 - adopt padacioso as a optional padatious alternative to avoid libfann (LGPL) Aug 2022 - ovos-core version 0.0.4 released Aug 2022 - experimental support for \"continuous_listening\" and \"hybrid_listening\" added Sep 2022 - MycroftAI Mimic3 TTS released, AGPL licensed Sep 2022 . MycroftAI Mark 2 starts shipping Sep 2022 - skill-ovos-news transferred from @JarbasAl to OVOS Oct 2022 - ovos-backend-manager UI released Oct 2022 - ovos-stt-plugin-whispercpp released Oct 2022 - new ask_yesno parser added Oct 2022 - ovos-backend-client adopted by ovos-core , selectable backend support added (offline, personal, selene) Oct 2022 - ovos-tts-plugin-mimic3-server released Oct 2022 - mycroft.blue-systems.com mimic 3 public server added Oct 2022 - mimic3.ziggyai.online mimic 3 public server added Oct 2022 - @aix talks about OVOS in KDE Akademy Oct 2022 - skill-ovos-soundcloud transferred from @JarbasAl to OVOS Oct 2022 - skill-ovos-youtube transferred from @JarbasAl to OVOS ??? 2022 - mycroft-dinkum source code made public, Apache2 licensed ??? 2022 - Extremely negative feedback from developer community, several key members annouce no intention to support mycroft-dinkum ??? 2022 - NeonAI starts release NeonOS images for the Mark 2 Nov 2022 - ovos-core version 0.0.5 released Nov 2022 - MycroftAI staff lay off, only a skeleton crew remaining Nov 2022 - ovos-bus-client forked from Mycroft Nov 2022 - tts.smartgic.io/mimic3 public Mimic3 server added Dec 2022 - Home Assistant PHAL plugin initial release by @AIX , exposes HA devices to the messagebus Dec 2022 - skill-ovos-youtube-music transferred from @JarbasAl to OVOS Dec 2022 - skill-ovos-bandcamp transferred from @JarbasAl to OVOS Jan 2023 - ovos-core version 0.0.6 released Jan 2023 - negative user feedback from the community for Mark 2, lots of bugs and missing features Jan 2023 - \"fallback\" wake word support added to ovos-plugin-manager Jan 2023 - skill-ovos-local-media released Jan 2023 - ChatGPT skill released Feb 2023 - ovos-config cli tool by community member @sgee released Feb 2023 - ovos-solver-plugin-aiml released Feb 2023 - ovos-solver-plugin-rivescript released Feb 2023 - skill-ovos-somafm transferred from @JarbasAl to OVOS Feb 2023 - MycroftAI partners up with NeonAI to maintain mycroft-core(?) Mar 2023 - Mimic3 TTS public servers become default OVOS voice (alan pope) Mar 2023 - ovos-core version 0.0.7 released Mar 2023 - Fundraiser to form OpenVoiceOS V.z.w. (Dutch: \"Vereninging zonder winstoogmerk\") completed in a couple days Mar 2023 - First stretch goal of fundraiser reached and second stretch goal announced Mar 2023 - raspbian-ovos images start being released, maintained by community member @builderjer Mar 2023 - community docs start being maintained by community members Mar 2023 - ovos-ww-plugin-openWakeWord released, maintained by author @dscripka Mar 2023 - skill-ovos-icanhazdadjokes transferred from @JarbasAl to OVOS Mar 2023 - ovos-skill-alerts forked from Neon, maintained by community member @sgee Apr 2023 - ovos-core splits ovos-audio , ovos-listener , ovos-gui and ovos-bus into their own packages Apr 2023 - @Aix leaves OpenVoiceOS Apr 2023 - OpenVoiceOS stops releasing manjaro based images Apr 2023 - ovos-stt-plugin-fasterwhisper released Apr 2023 - ovos-tts-plugin-piper released Apr 2023 - precise-lite-trainer released Apr 2023 - ovos-vad-plugin-precise released Apr 2023 - ovos-dinkum-listener released Apr 2023 - ovos-translate-plugin-deepl released, maintained by community member @sgee Apr 2023 - mycroft-classic-listener released, to preserve original mark 1 listener Apr 2023 - skill-ovos-tunein transferred from @JarbasAl to OVOS, maintained by community member @sgee Apr 2023 - jurebes intent parser released May 2023 - mycroft import deprecated in favor of ovos_core module for skills service May 2023 - stt.openvoiceos.org moves to whisper (small, cpu only) May 2023 - ovos-docker released, maintained by community member @goldyfruit May 2023 - Open Voice OS TTS/STT status page released, maintained by community member @goldyfruit May 2023 - First successful run of OpenVoiceOS on Mac OS using containers with ovos-docker May 2023 - ovos-docker-stt STT containers released, maintained by community member @goldyfruit May 2023 - ovos-microphone-plugin-sounddevice released, which provides native Mac OS suuport, maintained by community member @goldyfruit May 2023 - ovos-persona alpha release May 2023 - ovos-audio-transformer-plugin-speechbrain-langdetect released May 2023 - ovos-skill-easter-eggs transferred from @JarbasAl to OVOS, maintained by community member @mikejgray May 2023 - skill-ovos-dismissal transferred from @ChanceNCounter to OVOS May 2023 - skill-ovos-dictation transferred from @JarbasAl to OVOS Jun 2023 - Home Assistant plugin starts being maintained by community member @mikejgray Jun 2023 - quebra_frases transferred from @JarbasAl to OVOS Jun 2023 - ovos-translate-plugin-nllb released Jun 2023 - fasterwhisper.ziggyai.online public STT server added (large, GPU) Jun 2023 - Home Assistant Notify integration released by community member @mikejgray Jun 2023 - First (and second!) successful run of OpenVoiceOS on Windows, using WSL2 and ovos-docker Jun 2023 - ovos-docker-tts TTS containers released, maintained by community member @goldyfruit Jun 2023 - ovos-tts-plugin-azure released Jun 2023 - ovos-utterance-corrections-plugin released Jul 2023 - mycroft-gui-qt6 forked from mycroft-gui at last commit supporting QT6 before license change to GPL (reverted shortly after) Jul 2023 - mycroft-gui-qt5 forked from mycroft-gui at last commit supporting QT5 Jul 2023 - pipertts.ziggyai.online public TTS server added Jul 2023 - tts.smartgic.io/piper public TTS server added Jul 2023 - piper TTS public servers become default OVOS voice (alan pope) Jul 2023 - skill-ovos-spotify port of the mycroft-spotify skill by community member and original author @forslund Aug 2023 - ovos-translate-server-plugin released Aug 2023 - ovos-docker-tx translation containers released, maintained by community member @goldyfruit Aug 2023 - nllb.openvoiceos.org public translation server added Aug 2023 - translator.smartgic.io/nllb public translation server added Aug 2023 - adopt NLLB public servers as default translation plugin Aug 2023 - skill-ovos-wolfie transferred from @JarbasAl to OVOS Aug 2023 - skill-ovos-ddg transferred from @JarbasAl to OVOS Aug 2023 - skill-ovos-wikipedia transferred from @JarbasAl to OVOS Aug 2023 - ovos-stt-azure-plugin released Sep 2023 - skill-ovos-parrot transferred from @JarbasAl to OVOS Sep 2023 - stt.smartgic.io/fasterwhisper public STT server (large, GPU) Sep 2023 - GUI fully functional with ovos-docker containers Sep 2023 - persona-server alpha version released Sep 2023 - ovos-audio-transformer-plugin-ggwave released Oct 2023 - ovosnllb.ziggyai.online public translation server added Oct 2023 - ovos-tts-plugin-mimic3-server deprecated Oct 2023 - ovos-PHAL-sensors released, exposing OVOS sensors in Home Assistant Oct 2023 - ovos-bidirectional-translation-plugin released Nov 2023 - Plasma Bigscreen moves to QT6 and explicitly drops support for OVOS Dec 2023 - ovos-installer first release! codename Duke Nukem , maintained by community member @goldyfruit Dec 2023 - ovos-logs cli tool by community member @sgee added to ovos-utils Dec 2023 - ovos-docs-viewer cli tool released Dec 2023 - skill-ovos-spelling forked from Mycroft Dec 2023 - skill-ovos-ip forked from Mycroft Dec 2023 - skill-ovos-wallpapers transferred to OVOS Dec 2023 - ovos-i2csound released by community member @builderjer ??? 202? - ovos-tts-plugin-mimic2 deprecated Jan 2024 - skill-ovos-boot-finished forked from Neon Jan 2024 - skill-ovos-audio-recording forked from Neon Jan 2024 - ovos-utterance-plugin-cancel forked from Neon, deprecates dismissal skill Jan 2024 - ovos-mark1-utils released Jan 2024 - Mycroft forums move to Open Conversational AI Jan 2024 - ovos-vad-plugin-noise released to support older platforms Feb 2024 - ovos-tts-plugin-edge-tts released Feb 2024 - Selene servers and Mycroft AI website go down Feb 2024 - skill-ovos-randomness released, maintained by community member @mikejgray Feb 2024 - @Chance leaves OpenVoiceOS Feb 2024 - skill-ovos-wordnet released Near Future - ovos-core version 0.0.8 released","title":"Events timeline"},{"location":"transformer_plugins/","text":"Transformer Plugins NEW in ovos-core version 0.0.8 Transformer plugins are able to transform data along the way, this can be audio data, text or metadata Audio Transformers NEW in ovos-core version 0.0.8 , originally developed for Neon plugin description ovos-audio-transformer-plugin-fasterwhisper detect language from audio to inform STT ovos-audio-transformer-plugin-speechbrain-langdetect detect language from audio to inform STT ovos-audio-transformer-plugin-ggwave decodes data over audio and emits bus events in response Utterance Transformers NEW in ovos-core version 0.0.8 , originally developed for Neon plugin description source ovos-utterance-normalizer normalizes text before pipeline stage OpenVoiceOS/ovos-classifiers ovos-utterance-plugin-cancel cancel an utterance mid transcription OpenVoiceOS/ovos-utterance-plugin-cancel ovos-utterance-corrections-plugin manually correct bad transcriptions OpenVoiceOS/ovos-utterance-corrections-plugin ovos-utterance-translation-plugin auto translate unsupported languages OpenVoiceOS/ovos-bidirectional-translation-plugin Metadata Transformers NEW in ovos-core version 0.0.8 Dialog Transformers NEW in ovos-core version 0.0.8 plugin description source ovos-dialog-transformer-openai-plugin rewrite speech with a LLM before executing TTS OpenVoiceOS/ovos-solver-plugin-openai-persona ovos-dialog-translation-plugin translate speech back into user language OpenVoiceOS/ovos-bidirectional-translation-plugin TTS Transformers NEW in ovos-core version 0.0.8 plugin description source ovos-tts-transformer-sox-plugin apply sound effects via sox OpenVoiceOS/ovos-tts-transformer-sox-plugin Standalone Usage TODO Plugin Templates TODO","title":"Transformers"},{"location":"transformer_plugins/#transformer-plugins","text":"NEW in ovos-core version 0.0.8 Transformer plugins are able to transform data along the way, this can be audio data, text or metadata","title":"Transformer Plugins"},{"location":"transformer_plugins/#audio-transformers","text":"NEW in ovos-core version 0.0.8 , originally developed for Neon plugin description ovos-audio-transformer-plugin-fasterwhisper detect language from audio to inform STT ovos-audio-transformer-plugin-speechbrain-langdetect detect language from audio to inform STT ovos-audio-transformer-plugin-ggwave decodes data over audio and emits bus events in response","title":"Audio Transformers"},{"location":"transformer_plugins/#utterance-transformers","text":"NEW in ovos-core version 0.0.8 , originally developed for Neon plugin description source ovos-utterance-normalizer normalizes text before pipeline stage OpenVoiceOS/ovos-classifiers ovos-utterance-plugin-cancel cancel an utterance mid transcription OpenVoiceOS/ovos-utterance-plugin-cancel ovos-utterance-corrections-plugin manually correct bad transcriptions OpenVoiceOS/ovos-utterance-corrections-plugin ovos-utterance-translation-plugin auto translate unsupported languages OpenVoiceOS/ovos-bidirectional-translation-plugin","title":"Utterance Transformers"},{"location":"transformer_plugins/#metadata-transformers","text":"NEW in ovos-core version 0.0.8","title":"Metadata Transformers"},{"location":"transformer_plugins/#dialog-transformers","text":"NEW in ovos-core version 0.0.8 plugin description source ovos-dialog-transformer-openai-plugin rewrite speech with a LLM before executing TTS OpenVoiceOS/ovos-solver-plugin-openai-persona ovos-dialog-translation-plugin translate speech back into user language OpenVoiceOS/ovos-bidirectional-translation-plugin","title":"Dialog Transformers"},{"location":"transformer_plugins/#tts-transformers","text":"NEW in ovos-core version 0.0.8 plugin description source ovos-tts-transformer-sox-plugin apply sound effects via sox OpenVoiceOS/ovos-tts-transformer-sox-plugin","title":"TTS Transformers"},{"location":"transformer_plugins/#standalone-usage","text":"TODO","title":"Standalone Usage"},{"location":"transformer_plugins/#plugin-templates","text":"TODO","title":"Plugin Templates"},{"location":"translate_server/","text":"OpenVoiceOS Translate Server Turn any OVOS Language plugin into a microservice! Install pip install ovos-translate-server Companion Plugin Use with OpenVoiceOS companion plugin Public Servers Member Service Plugin URL Translate ovos-translate-plugin-server NLLB - https://nllb.openvoiceos.org Translate ovos-translate-plugin-server NLLB - https://ovosnllb.ziggyai.online Translate ovos-translate-plugin-server NLLB - https://translator.smartgic.io/nllb Detect ovos-translate-plugin-server lingua-podre - https://nllb.openvoiceos.org/detect Detect ovos-translate-plugin-server lingua-podre - https://ovosnllb.ziggyai.online/detect Detect ovos-translate-plugin-server lingua-podre - https://translator.smartgic.io/nllb/detect Usage ovos-translate-server --help usage: ovos-translate-server [-h] [--tx-engine TX_ENGINE] [--detect-engine DETECT_ENGINE] [--port PORT] [--host HOST] optional arguments: -h, --help show this help message and exit --tx-engine TX_ENGINE translate plugin to be used --detect-engine DETECT_ENGINE lang detection plugin to be used --port PORT port number --host HOST host eg, to use the NLLB plugin for translation, and Lang Classifier Classics for detection ovos-translate-server --tx-engine ovos-translate-plugin-nllb --detect-engine ovos-lang-detector-classics-plugin then you can do get requests http://0.0.0.0:9686/translate/en/o meu nome \u00e9 Casimiro (auto detect source lang) http://0.0.0.0:9686/translate/pt/en/o meu nome \u00e9 Casimiro (specify source lang) http://0.0.0.0:9686/detect/o meu nome \u00e9 Casimiro Docker Pre built containers can be found in the ovos-docker-tx repository Template you can create easily crete a docker file to serve any plugin FROM python:3.7 RUN pip3 install ovos-utils==0.0.15 RUN pip3 install ovos-plugin-manager==0.0.4 RUN pip3 install ovos-translate-server==0.0.1 RUN pip3 install {PLUGIN_HERE} ENTRYPOINT ovos-translate-server --tx-engine {PLUGIN_HERE} --detect-engine {PLUGIN_HERE} build it docker build . -t my_ovos_translate_plugin run it docker run -p 8080:9686 my_ovos_translate_plugin Each plugin can provide its own Dockerfile in its repository using ovos-translate-server","title":"Translate Server"},{"location":"translate_server/#openvoiceos-translate-server","text":"Turn any OVOS Language plugin into a microservice!","title":"OpenVoiceOS Translate Server"},{"location":"translate_server/#install","text":"pip install ovos-translate-server","title":"Install"},{"location":"translate_server/#companion-plugin","text":"Use with OpenVoiceOS companion plugin","title":"Companion Plugin"},{"location":"translate_server/#public-servers","text":"Member Service Plugin URL Translate ovos-translate-plugin-server NLLB - https://nllb.openvoiceos.org Translate ovos-translate-plugin-server NLLB - https://ovosnllb.ziggyai.online Translate ovos-translate-plugin-server NLLB - https://translator.smartgic.io/nllb Detect ovos-translate-plugin-server lingua-podre - https://nllb.openvoiceos.org/detect Detect ovos-translate-plugin-server lingua-podre - https://ovosnllb.ziggyai.online/detect Detect ovos-translate-plugin-server lingua-podre - https://translator.smartgic.io/nllb/detect","title":"Public Servers"},{"location":"translate_server/#usage","text":"ovos-translate-server --help usage: ovos-translate-server [-h] [--tx-engine TX_ENGINE] [--detect-engine DETECT_ENGINE] [--port PORT] [--host HOST] optional arguments: -h, --help show this help message and exit --tx-engine TX_ENGINE translate plugin to be used --detect-engine DETECT_ENGINE lang detection plugin to be used --port PORT port number --host HOST host eg, to use the NLLB plugin for translation, and Lang Classifier Classics for detection ovos-translate-server --tx-engine ovos-translate-plugin-nllb --detect-engine ovos-lang-detector-classics-plugin then you can do get requests http://0.0.0.0:9686/translate/en/o meu nome \u00e9 Casimiro (auto detect source lang) http://0.0.0.0:9686/translate/pt/en/o meu nome \u00e9 Casimiro (specify source lang) http://0.0.0.0:9686/detect/o meu nome \u00e9 Casimiro","title":"Usage"},{"location":"translate_server/#docker","text":"Pre built containers can be found in the ovos-docker-tx repository","title":"Docker"},{"location":"translate_server/#template","text":"you can create easily crete a docker file to serve any plugin FROM python:3.7 RUN pip3 install ovos-utils==0.0.15 RUN pip3 install ovos-plugin-manager==0.0.4 RUN pip3 install ovos-translate-server==0.0.1 RUN pip3 install {PLUGIN_HERE} ENTRYPOINT ovos-translate-server --tx-engine {PLUGIN_HERE} --detect-engine {PLUGIN_HERE} build it docker build . -t my_ovos_translate_plugin run it docker run -p 8080:9686 my_ovos_translate_plugin Each plugin can provide its own Dockerfile in its repository using ovos-translate-server","title":"Template"},{"location":"tts_plugins/","text":"TTS Plugins TTS plugins are responsible for converting text into audio for playback TTS All Mycroft TTS plugins need to define a class based on the TTS base class from ovos_plugin_manager from ovos_plugin_manager.templates.tts import TTS class myTTS(TTS): def __init__(self, lang, config): super().__init__(lang, config, validator, audio_ext='wav', phonetic_spelling=False, ssml_tags=None) # Any specific init code goes here The super() call does some setup adding specific options to how Mycroft will preprocess the sentence. audio_ext : filetype of output, possible options 'wav' and 'mp3'. phonetec_spelling , True if Mycroft should preprocess some difficult to pronounce words (eg spotify) or provide the raw text to the TTS. ssml_tags : list of valid SSML tags for the TTS if any, otherwise None. validator : a special class that verifies that the TTS is working in the current configuration. The get_tts() method will be called by Mycroft to generate audio and (optionally) phonemes. This is the main method that the plugin creator needs to implement. It is called with: sentence (str): a piece of text to turn into audio. wav_file (str): where the plugin should store the generated audio data. This method should generate audio data and return a Tuple (wav_file, visemes) : wav_file (str): path to written data (generally the input argument) phonemes (list): phoneme list for synthesized audio TTS Validator To check if the TTS can be used, a validator class is needed. This should inherit from mycroft.tts.TTSValidaor . It will be called with the TTS class as argument and will store it in self.tts . The following is the bare minimum implementation: class MyValidator(TTSValidator): def get_tts_class(self): # Should return a reference to the TTS class it's inteded to validate. def validate_lang(self): # Raise exception if `self.tts.lang` is not supported. def validate_connection(self): # Check that the software needed for the TTS is reachable, # be it a local executable, python module or remote server and # if not available raise an exception. NOTE: TTSValidator is optional Entry point To make the class detectable as an TTS plugin, the package needs to provide an entry point under the mycroft.plugin.tts namespace. setup([...], entry_points = {'mycroft.plugin.tts': 'example_tts = my_tts:myTTS'} ) Where example_tts is is the TTS module name for the plugin, my_tts is the Python module and myTTS is the class in the module to return. List of TTS plugins Plugin Streaming Offline Type ovos-tts-plugin-mimic \u274c \u2714\ufe0f FOSS ovos-tts-plugin-mimic3 \u274c \u2714\ufe0f FOSS ovos-tts-plugin-piper \u274c \u2714\ufe0f FOSS ovos-tts-plugin-marytts \u274c \u274c API (self hosted) ovos-tts-server-plugin \u274c \u274c API (self hosted) ovos-tts-plugin-pico \u274c \u2714\ufe0f FOSS ovos-tts-plugin-edge-tts \u2714\ufe0f \u274c API (free) ovos-tts-plugin-polly \u274c \u274c API (key) ovos-tts-plugin-voicerss \u274c \u274c API (key) ovos-tts-plugin-google-TX \u274c \u274c API (free) ovos-tts-plugin-responsivevoice \u274c \u274c API (free) ovos-tts-plugin-espeakNG \u274c \u2714\ufe0f FOSS ovos-tts-plugin-cotovia \u274c \u2714\ufe0f FOSS ovos-tts-plugin-SAM \u274c \u2714\ufe0f Abandonware ovos-tts-plugin-beepspeak \u274c \u2714\ufe0f Fun neon-tts-plugin-larynx_server \u274c \u274c API (self hosted) neon-tts-plugin-coqui \u274c \u2714\ufe0f FOSS neon-tts-plugin-coqui-remote \u274c \u274c API (self hosted) neon-tts-plugin-glados \u274c \u2714\ufe0f FOSS Standalone Usage TODO Plugin Template from ovos_plugin_manager.templates.tts import TTS # base plugin class class MyTTSPlugin(TTS): def __init__(self, *args, **kwargs): # in here you should specify if your plugin return wav or mp3 files # you should also specify any valid ssml tags ssml_tags = [\"speak\", \"s\", \"w\", \"voice\", \"prosody\", \"say-as\", \"break\", \"sub\", \"phoneme\"] super().__init__(*args, **kwargs, audio_ext=\"wav\", ssml_tags=ssml_tags) # read config settings for your plugin if any self.pitch = self.config.get(\"pitch\", 0.5) def get_tts(self, sentence, wav_file): # TODO - create TTS audio @ wav_file (path) return wav_file, None @property def available_languages(self): \"\"\"Return languages supported by this TTS implementation in this state This property should be overridden by the derived class to advertise what languages that engine supports. Returns: set: supported languages \"\"\" # TODO - what langs can this TTS handle? return {\"en-us\", \"es-es\"} # sample valid configurations per language # \"display_name\" and \"offline\" provide metadata for UI # \"priority\" is used to calculate position in selection dropdown # 0 - top, 100-bottom # all other keys represent an example valid config for the plugin MyTTSConfig = { lang: [{\"lang\": lang, \"display_name\": f\"MyTTS ({lang}\", \"priority\": 70, \"offline\": True}] for lang in [\"en-us\", \"es-es\"] }","title":"Text To Speech"},{"location":"tts_plugins/#tts-plugins","text":"TTS plugins are responsible for converting text into audio for playback","title":"TTS Plugins"},{"location":"tts_plugins/#tts","text":"All Mycroft TTS plugins need to define a class based on the TTS base class from ovos_plugin_manager from ovos_plugin_manager.templates.tts import TTS class myTTS(TTS): def __init__(self, lang, config): super().__init__(lang, config, validator, audio_ext='wav', phonetic_spelling=False, ssml_tags=None) # Any specific init code goes here The super() call does some setup adding specific options to how Mycroft will preprocess the sentence. audio_ext : filetype of output, possible options 'wav' and 'mp3'. phonetec_spelling , True if Mycroft should preprocess some difficult to pronounce words (eg spotify) or provide the raw text to the TTS. ssml_tags : list of valid SSML tags for the TTS if any, otherwise None. validator : a special class that verifies that the TTS is working in the current configuration. The get_tts() method will be called by Mycroft to generate audio and (optionally) phonemes. This is the main method that the plugin creator needs to implement. It is called with: sentence (str): a piece of text to turn into audio. wav_file (str): where the plugin should store the generated audio data. This method should generate audio data and return a Tuple (wav_file, visemes) : wav_file (str): path to written data (generally the input argument) phonemes (list): phoneme list for synthesized audio","title":"TTS"},{"location":"tts_plugins/#tts-validator","text":"To check if the TTS can be used, a validator class is needed. This should inherit from mycroft.tts.TTSValidaor . It will be called with the TTS class as argument and will store it in self.tts . The following is the bare minimum implementation: class MyValidator(TTSValidator): def get_tts_class(self): # Should return a reference to the TTS class it's inteded to validate. def validate_lang(self): # Raise exception if `self.tts.lang` is not supported. def validate_connection(self): # Check that the software needed for the TTS is reachable, # be it a local executable, python module or remote server and # if not available raise an exception. NOTE: TTSValidator is optional","title":"TTS Validator"},{"location":"tts_plugins/#entry-point","text":"To make the class detectable as an TTS plugin, the package needs to provide an entry point under the mycroft.plugin.tts namespace. setup([...], entry_points = {'mycroft.plugin.tts': 'example_tts = my_tts:myTTS'} ) Where example_tts is is the TTS module name for the plugin, my_tts is the Python module and myTTS is the class in the module to return.","title":"Entry point"},{"location":"tts_plugins/#list-of-tts-plugins","text":"Plugin Streaming Offline Type ovos-tts-plugin-mimic \u274c \u2714\ufe0f FOSS ovos-tts-plugin-mimic3 \u274c \u2714\ufe0f FOSS ovos-tts-plugin-piper \u274c \u2714\ufe0f FOSS ovos-tts-plugin-marytts \u274c \u274c API (self hosted) ovos-tts-server-plugin \u274c \u274c API (self hosted) ovos-tts-plugin-pico \u274c \u2714\ufe0f FOSS ovos-tts-plugin-edge-tts \u2714\ufe0f \u274c API (free) ovos-tts-plugin-polly \u274c \u274c API (key) ovos-tts-plugin-voicerss \u274c \u274c API (key) ovos-tts-plugin-google-TX \u274c \u274c API (free) ovos-tts-plugin-responsivevoice \u274c \u274c API (free) ovos-tts-plugin-espeakNG \u274c \u2714\ufe0f FOSS ovos-tts-plugin-cotovia \u274c \u2714\ufe0f FOSS ovos-tts-plugin-SAM \u274c \u2714\ufe0f Abandonware ovos-tts-plugin-beepspeak \u274c \u2714\ufe0f Fun neon-tts-plugin-larynx_server \u274c \u274c API (self hosted) neon-tts-plugin-coqui \u274c \u2714\ufe0f FOSS neon-tts-plugin-coqui-remote \u274c \u274c API (self hosted) neon-tts-plugin-glados \u274c \u2714\ufe0f FOSS","title":"List of TTS plugins"},{"location":"tts_plugins/#standalone-usage","text":"TODO","title":"Standalone Usage"},{"location":"tts_plugins/#plugin-template","text":"from ovos_plugin_manager.templates.tts import TTS # base plugin class class MyTTSPlugin(TTS): def __init__(self, *args, **kwargs): # in here you should specify if your plugin return wav or mp3 files # you should also specify any valid ssml tags ssml_tags = [\"speak\", \"s\", \"w\", \"voice\", \"prosody\", \"say-as\", \"break\", \"sub\", \"phoneme\"] super().__init__(*args, **kwargs, audio_ext=\"wav\", ssml_tags=ssml_tags) # read config settings for your plugin if any self.pitch = self.config.get(\"pitch\", 0.5) def get_tts(self, sentence, wav_file): # TODO - create TTS audio @ wav_file (path) return wav_file, None @property def available_languages(self): \"\"\"Return languages supported by this TTS implementation in this state This property should be overridden by the derived class to advertise what languages that engine supports. Returns: set: supported languages \"\"\" # TODO - what langs can this TTS handle? return {\"en-us\", \"es-es\"} # sample valid configurations per language # \"display_name\" and \"offline\" provide metadata for UI # \"priority\" is used to calculate position in selection dropdown # 0 - top, 100-bottom # all other keys represent an example valid config for the plugin MyTTSConfig = { lang: [{\"lang\": lang, \"display_name\": f\"MyTTS ({lang}\", \"priority\": 70, \"offline\": True}] for lang in [\"en-us\", \"es-es\"] }","title":"Plugin Template"},{"location":"tts_server/","text":"OpenVoiceOS TTS Server Turn any OVOS TTS plugin into a microservice! Companion plugin Use with OpenVoiceOS companion plugin Public Servers Member Service Plugin URL TTS ovos-tts-server-plugin Mimic 1 - https://tts.smartgic.io/mimic Mimic 3 - https://tts.smartgic.io/mimic3 Piper - https://tts.smartgic.io/piper S.A.M. - https://tts.smartgic.io/sam TTS ovos-tts-server-plugin Mimic 3 - https://mimic3.ziggyai.online Piper - https://pipertts.ziggyai.online TTS ovos-tts-server-plugin Coqui - https://coqui.neonaibeta.com Coqui 2 - https://coqui.neonaialpha.com Coqui 3 - https://coqui.neonaiservices.com Install pip install ovos-tts-server Usage ovos-tts-server --help usage: ovos-tts-server [-h] [--engine ENGINE] [--port PORT] [--host HOST] [--cache] options: -h, --help show this help message and exit --engine ENGINE tts plugin to be used --port PORT port number --host HOST host --cache save every synth to disk eg, to use the GladosTTS plugin ovos-tts-server --engine neon-tts-plugin-glados --cache then do a get request http://192.168.1.112:9666/synthesize/hello Docker Pre built containers can be found in the ovos-docker-tts repository Template FROM python:3.7 RUN pip3 install ovos-utils==0.0.15 RUN pip3 install ovos-plugin-manager==0.0.4 RUN pip3 install ovos-tts-server==0.0.1 RUN pip3 install {PLUGIN_HERE} ENTRYPOINT ovos-tts-server --engine {PLUGIN_HERE} --cache build it docker build . -t my_ovos_tts_plugin run it docker run -p 8080:9666 my_ovos_tts_plugin use it http://localhost:8080/synthesize/hello Each plugin can provide its own Dockerfile in its repository using ovos-tts-server","title":"TTS Server"},{"location":"tts_server/#openvoiceos-tts-server","text":"Turn any OVOS TTS plugin into a microservice!","title":"OpenVoiceOS TTS Server"},{"location":"tts_server/#companion-plugin","text":"Use with OpenVoiceOS companion plugin","title":"Companion plugin"},{"location":"tts_server/#public-servers","text":"Member Service Plugin URL TTS ovos-tts-server-plugin Mimic 1 - https://tts.smartgic.io/mimic Mimic 3 - https://tts.smartgic.io/mimic3 Piper - https://tts.smartgic.io/piper S.A.M. - https://tts.smartgic.io/sam TTS ovos-tts-server-plugin Mimic 3 - https://mimic3.ziggyai.online Piper - https://pipertts.ziggyai.online TTS ovos-tts-server-plugin Coqui - https://coqui.neonaibeta.com Coqui 2 - https://coqui.neonaialpha.com Coqui 3 - https://coqui.neonaiservices.com","title":"Public Servers"},{"location":"tts_server/#install","text":"pip install ovos-tts-server","title":"Install"},{"location":"tts_server/#usage","text":"ovos-tts-server --help usage: ovos-tts-server [-h] [--engine ENGINE] [--port PORT] [--host HOST] [--cache] options: -h, --help show this help message and exit --engine ENGINE tts plugin to be used --port PORT port number --host HOST host --cache save every synth to disk eg, to use the GladosTTS plugin ovos-tts-server --engine neon-tts-plugin-glados --cache then do a get request http://192.168.1.112:9666/synthesize/hello","title":"Usage"},{"location":"tts_server/#docker","text":"Pre built containers can be found in the ovos-docker-tts repository","title":"Docker"},{"location":"tts_server/#template","text":"FROM python:3.7 RUN pip3 install ovos-utils==0.0.15 RUN pip3 install ovos-plugin-manager==0.0.4 RUN pip3 install ovos-tts-server==0.0.1 RUN pip3 install {PLUGIN_HERE} ENTRYPOINT ovos-tts-server --engine {PLUGIN_HERE} --cache build it docker build . -t my_ovos_tts_plugin run it docker run -p 8080:9666 my_ovos_tts_plugin use it http://localhost:8080/synthesize/hello Each plugin can provide its own Dockerfile in its repository using ovos-tts-server","title":"Template"},{"location":"universal_skills/","text":"UniversalSkill The UniversalSkill class is designed to facilitate automatic translation of input and output messages between different languages. This skill is particularly useful when native language support is not feasible, providing a convenient way to handle multilingual interactions. NEW - ovos-core version 0.0.8 Overview This skill ensures that intent handlers receive utterances in the skill's internal language and are expected to produce responses in the same internal language. The speak method, used for generating spoken responses, automatically translates utterances from the internal language to the original query language. NOTE: The self.lang attribute reflects the original query language, while received utterances are always in self.internal_language . Language Plugins To run UniversalSkills you need to configure Translation plugins in mycroft.conf // Translation plugins \"language\": { // by default uses public servers // https://github.com/OpenVoiceOS/ovos-translate-server \"detection_module\": \"ovos-lang-detector-plugin-server\", \"translation_module\": \"ovos-translate-plugin-server\" }, Usage Initialization # Example initialization from ovos_workshop.skills.auto_translatable import UniversalSkill class MyMultilingualSkill(UniversalSkill): \"\"\" Skill that auto translates input/output from any language This skill is designed to automatically translate input and output messages between different languages. The intent handlers are ensured to receive utterances in the skill's internal language, and they are expected to produce utterances in the same internal language. The `speak` method will always translate utterances from the internal language to the original query language (`self.lang`). NOTE: `self.lang` reflects the original query language, but received utterances are always in `self.internal_language`. \"\"\" def __init__(self, *args, **kwargs): \"\"\" Initialize the UniversalSkill. Parameters for super(): - internal_language (str): The language in which the skill internally operates. - translate_tags (bool): Whether to translate the private __tags__ value (adapt entities). - autodetect (bool): If True, the skill will detect the language of the utterance and ignore self.lang / Session.lang. - translate_keys (list): default [\"utterance\", \"utterances\"] Keys added here will have values translated in message.data. \"\"\" # skill hardcoded in portuguese super().__init__(internal_language=\"pt-pt\", translate_tags=translate_tags, autodetect=autodetect, translate_keys=translate_keys, *args, **kwargs) Intents and Utterances Use the register_intent and register_intent_file methods to register intents with universal intent handlers. The usual decorators also work The speak method is used to generate spoken responses. It automatically translates utterances if the output language is different from the skill's internal language or autodetection is enabled. # Example speaking utterance, hardcoded to self.internal_language self.speak(\"Hello, how are you?\") Universal Intent Handler NOTE Users should NOT use the create_universal_handler method manually in skill intents; it is automatically utilized by self.register_intent . The following example demonstrates its usage with self.add_event . # Example universal handler creation def my_event_handler(message): # Your event handling logic here pass # Manual usage with self.add_event my_handler = self.create_universal_handler(my_event_handler) self.add_event(\"my_event\", my_handler) EnglishCatFacts Skill Example Let's create a simple tutorial skill that interacts with an API to fetch cat facts in English. We'll use the UniversalSkill class to support translations for other languages. from ovos_workshop.skills.auto_translatable import UniversalSkill class EnglishCatFactsSkill(UniversalSkill): def __init__(self, *args, **kwargs): \"\"\" This skill is hardcoded in english, indicated by internal_language \"\"\" super().__init__(internal_language=\"en-us\", *args, **kwargs) def fetch_cat_fact(self): # Your logic to fetch a cat fact from an API cat_fact = \"Cats have five toes on their front paws but only four on their back paws.\" return cat_fact @intent_handler(\"cat_fact.intent\") def handle_cat_fact_request(self, message): # Fetch a cat fact in self.internal_language cat_fact = self.fetch_cat_fact() # Speak the cat fact, it will be translated to self.lang if needed self.speak(cat_fact) In this example, the CatFactsSkill class extends UniversalSkill , allowing it to seamlessly translate cat facts into the user's preferred language. SpanishDatabase Skill Example A more advanced example, let's consider a skill that listens to bus messages. Our skill listens for messages containing a \"phrase\" payload in message.data that can be in any language, and it saves this phrase in spanish to a database. Then it speaks a hardcoded spanish utterance, and it gets translated into the language of the bus message Session from ovos_workshop.skills.auto_translatable import UniversalSkill class SpanishDatabaseSkill(UniversalSkill): def __init__(self, *args, **kwargs): \"\"\" This skill is hardcoded in spanish, indicated by internal_language \"\"\" translate_keys=[\"phrase\"] # translate \"phrase\" in message.data super().__init__(internal_language=\"es-es\", translate_keys=translate_keys, *args, **kwargs) def initialize(self): # wrap the event into a auto translation layer handler = self.create_universal_handler(self.handle_entry) self.add_event(\"skill.database.add\", handler) def handle_entry(self, message: Message): phrase = message.data[\"phrase\"] # assured to be in self.internal_language # Your logic to save phrase to a database self.speak(\"agregado a la base de datos\") # will be spoken in self.lang","title":"Universal Skills"},{"location":"universal_skills/#universalskill","text":"The UniversalSkill class is designed to facilitate automatic translation of input and output messages between different languages. This skill is particularly useful when native language support is not feasible, providing a convenient way to handle multilingual interactions. NEW - ovos-core version 0.0.8","title":"UniversalSkill"},{"location":"universal_skills/#overview","text":"This skill ensures that intent handlers receive utterances in the skill's internal language and are expected to produce responses in the same internal language. The speak method, used for generating spoken responses, automatically translates utterances from the internal language to the original query language. NOTE: The self.lang attribute reflects the original query language, while received utterances are always in self.internal_language .","title":"Overview"},{"location":"universal_skills/#language-plugins","text":"To run UniversalSkills you need to configure Translation plugins in mycroft.conf // Translation plugins \"language\": { // by default uses public servers // https://github.com/OpenVoiceOS/ovos-translate-server \"detection_module\": \"ovos-lang-detector-plugin-server\", \"translation_module\": \"ovos-translate-plugin-server\" },","title":"Language Plugins"},{"location":"universal_skills/#usage","text":"","title":"Usage"},{"location":"universal_skills/#initialization","text":"# Example initialization from ovos_workshop.skills.auto_translatable import UniversalSkill class MyMultilingualSkill(UniversalSkill): \"\"\" Skill that auto translates input/output from any language This skill is designed to automatically translate input and output messages between different languages. The intent handlers are ensured to receive utterances in the skill's internal language, and they are expected to produce utterances in the same internal language. The `speak` method will always translate utterances from the internal language to the original query language (`self.lang`). NOTE: `self.lang` reflects the original query language, but received utterances are always in `self.internal_language`. \"\"\" def __init__(self, *args, **kwargs): \"\"\" Initialize the UniversalSkill. Parameters for super(): - internal_language (str): The language in which the skill internally operates. - translate_tags (bool): Whether to translate the private __tags__ value (adapt entities). - autodetect (bool): If True, the skill will detect the language of the utterance and ignore self.lang / Session.lang. - translate_keys (list): default [\"utterance\", \"utterances\"] Keys added here will have values translated in message.data. \"\"\" # skill hardcoded in portuguese super().__init__(internal_language=\"pt-pt\", translate_tags=translate_tags, autodetect=autodetect, translate_keys=translate_keys, *args, **kwargs)","title":"Initialization"},{"location":"universal_skills/#intents-and-utterances","text":"Use the register_intent and register_intent_file methods to register intents with universal intent handlers. The usual decorators also work The speak method is used to generate spoken responses. It automatically translates utterances if the output language is different from the skill's internal language or autodetection is enabled. # Example speaking utterance, hardcoded to self.internal_language self.speak(\"Hello, how are you?\")","title":"Intents and Utterances"},{"location":"universal_skills/#universal-intent-handler","text":"NOTE Users should NOT use the create_universal_handler method manually in skill intents; it is automatically utilized by self.register_intent . The following example demonstrates its usage with self.add_event . # Example universal handler creation def my_event_handler(message): # Your event handling logic here pass # Manual usage with self.add_event my_handler = self.create_universal_handler(my_event_handler) self.add_event(\"my_event\", my_handler)","title":"Universal Intent Handler"},{"location":"universal_skills/#englishcatfacts-skill-example","text":"Let's create a simple tutorial skill that interacts with an API to fetch cat facts in English. We'll use the UniversalSkill class to support translations for other languages. from ovos_workshop.skills.auto_translatable import UniversalSkill class EnglishCatFactsSkill(UniversalSkill): def __init__(self, *args, **kwargs): \"\"\" This skill is hardcoded in english, indicated by internal_language \"\"\" super().__init__(internal_language=\"en-us\", *args, **kwargs) def fetch_cat_fact(self): # Your logic to fetch a cat fact from an API cat_fact = \"Cats have five toes on their front paws but only four on their back paws.\" return cat_fact @intent_handler(\"cat_fact.intent\") def handle_cat_fact_request(self, message): # Fetch a cat fact in self.internal_language cat_fact = self.fetch_cat_fact() # Speak the cat fact, it will be translated to self.lang if needed self.speak(cat_fact) In this example, the CatFactsSkill class extends UniversalSkill , allowing it to seamlessly translate cat facts into the user's preferred language.","title":"EnglishCatFacts Skill Example"},{"location":"universal_skills/#spanishdatabase-skill-example","text":"A more advanced example, let's consider a skill that listens to bus messages. Our skill listens for messages containing a \"phrase\" payload in message.data that can be in any language, and it saves this phrase in spanish to a database. Then it speaks a hardcoded spanish utterance, and it gets translated into the language of the bus message Session from ovos_workshop.skills.auto_translatable import UniversalSkill class SpanishDatabaseSkill(UniversalSkill): def __init__(self, *args, **kwargs): \"\"\" This skill is hardcoded in spanish, indicated by internal_language \"\"\" translate_keys=[\"phrase\"] # translate \"phrase\" in message.data super().__init__(internal_language=\"es-es\", translate_keys=translate_keys, *args, **kwargs) def initialize(self): # wrap the event into a auto translation layer handler = self.create_universal_handler(self.handle_entry) self.add_event(\"skill.database.add\", handler) def handle_entry(self, message: Message): phrase = message.data[\"phrase\"] # assured to be in self.internal_language # Your logic to save phrase to a database self.speak(\"agregado a la base de datos\") # will be spoken in self.lang","title":"SpanishDatabase Skill Example"},{"location":"user_res_files/","text":"Customization Resource Files Resource files are essential components of OVOS skills, containing data such as dialogs, intents, vocabularies, regular expressions, templates, and more. These files define how a skill interacts with the user and responds to queries. RECAP : the skill contains a locale folder with subfolders for each lang, eg en-us , learn more in skill structure docs Customizing Dialogs Users can personalize the behavior of skills by customizing dialogues to better suit their preferences. To give a unique twist and personality to your assistant you don't need to fork existing skills only to change dialogs Here's a step-by-step guide on how to replace the dialog of an existing skill: Identify the Skill and Resource to Replace : Determine the ID of the skill whose dialog you want to replace. In this example, let's assume the skill ID is skill-ovos-date-time.openvoiceos . Identify the specific dialog file you want to replace. For this example, let's say you want to replace the time.current.dialog file located in the locale/en-us/dialog directory of the skill. Create the Replacement Dialog File : Create a new dialog file with the same name ( time.current.dialog ) as the original file. Customize the content of the dialog file according to your preferences. You can modify the existing dialogues, add new ones, or remove any that you don't want to use. Locate the User-Specific Resource Directory : Use the provided skill ID ( skill-ovos-date-time.openvoiceos ) to locate the user-specific resource directory. The user-specific resource directory is located within the XDG data directory. It follows the path XDG_DATA_HOME/mycroft/resources/skill-ovos-date-time.openvoiceos (where XDG_DATA_HOME is the user's data directory, usually ~/.local/share on Linux). If it does not exist, create it, This can be done using file manager tools or command-line utilities such as mkdir on Unix-like systems. Copy the Replacement Dialog File to the User-Specific Directory : Copy or move the replacement dialog file ( time.current.dialog ) to the appropriate directory within the user-specific resource directory. Place the file in the locale/en-us/dialog directory within the user-specific resource directory. This mirrors the directory structure of the original skill. In this example the final path of the file would be ~/.local/share/mycroft/resources/skill-ovos-date-time.openvoiceos/locale/en-us/dialog/time.current.dialog Verify the Replacement : Test the skill to ensure that the modified dialogues are being used instead of the original ones. Customizing dialogues offers users flexibility in tailoring the behavior of skills to their specific needs and preferences. Local Language support Adding support for additional languages to existing skills enables users to interact with OVOS in their preferred language. While developing or waiting for skills to support your language you might want to add it locally Users can add language support for a skill by creating a new language folder in the user resources directory and copying the necessary files over: Identify the Skill and Language to Add : Determine the ID of the skill for which you want to add language support. Let's continue using the skill ID skill-ovos-date-time.openvoiceos . Identify the language you want to add support for. For this example, let's say you want to add support for Spanish (language code: es-es ). Create the New Language Folder : Create a new directory with the name of the language code ( es-es for Spanish) within the locale directory of the skill. This can be done using file manager tools or command-line utilities such as mkdir on Unix-like systems. Using the previous example, we would create ~/.local/share/mycroft/resources/skill-ovos-date-time.openvoiceos/locale/es-es/ Copy the Required Files to the New Language Folder : Copy all the necessary resource files from an existing language folder (e.g., en-us ) to the newly created language folder ( es-es ). This includes files such as dialogues, vocabularies, regex patterns, etc., depending on the resources used by the skill. Ensure that all files are placed in the corresponding directories within the new language folder to maintain the directory structure of the original skill. Verify the Language Addition : Once the files are copied over, verify that the new language is supported by the skill. Restart OpenVoiceOS to allow the skill to recognize the newly added language resources. Test the skill using the newly added language to ensure that it functions correctly and uses the appropriate language-specific resources. By following these steps, users can add support for additional languages to existing skills by creating new language folders and copying the required resource files. NEXT STEPS : consider sending a Pull Request to the skill to directly add language support! This allows users to extend the language capabilities of skills beyond the languages provided by default.","title":"Customization"},{"location":"user_res_files/#customization","text":"","title":"Customization"},{"location":"user_res_files/#resource-files","text":"Resource files are essential components of OVOS skills, containing data such as dialogs, intents, vocabularies, regular expressions, templates, and more. These files define how a skill interacts with the user and responds to queries. RECAP : the skill contains a locale folder with subfolders for each lang, eg en-us , learn more in skill structure docs","title":"Resource Files"},{"location":"user_res_files/#customizing-dialogs","text":"Users can personalize the behavior of skills by customizing dialogues to better suit their preferences. To give a unique twist and personality to your assistant you don't need to fork existing skills only to change dialogs Here's a step-by-step guide on how to replace the dialog of an existing skill: Identify the Skill and Resource to Replace : Determine the ID of the skill whose dialog you want to replace. In this example, let's assume the skill ID is skill-ovos-date-time.openvoiceos . Identify the specific dialog file you want to replace. For this example, let's say you want to replace the time.current.dialog file located in the locale/en-us/dialog directory of the skill. Create the Replacement Dialog File : Create a new dialog file with the same name ( time.current.dialog ) as the original file. Customize the content of the dialog file according to your preferences. You can modify the existing dialogues, add new ones, or remove any that you don't want to use. Locate the User-Specific Resource Directory : Use the provided skill ID ( skill-ovos-date-time.openvoiceos ) to locate the user-specific resource directory. The user-specific resource directory is located within the XDG data directory. It follows the path XDG_DATA_HOME/mycroft/resources/skill-ovos-date-time.openvoiceos (where XDG_DATA_HOME is the user's data directory, usually ~/.local/share on Linux). If it does not exist, create it, This can be done using file manager tools or command-line utilities such as mkdir on Unix-like systems. Copy the Replacement Dialog File to the User-Specific Directory : Copy or move the replacement dialog file ( time.current.dialog ) to the appropriate directory within the user-specific resource directory. Place the file in the locale/en-us/dialog directory within the user-specific resource directory. This mirrors the directory structure of the original skill. In this example the final path of the file would be ~/.local/share/mycroft/resources/skill-ovos-date-time.openvoiceos/locale/en-us/dialog/time.current.dialog Verify the Replacement : Test the skill to ensure that the modified dialogues are being used instead of the original ones. Customizing dialogues offers users flexibility in tailoring the behavior of skills to their specific needs and preferences.","title":"Customizing Dialogs"},{"location":"user_res_files/#local-language-support","text":"Adding support for additional languages to existing skills enables users to interact with OVOS in their preferred language. While developing or waiting for skills to support your language you might want to add it locally Users can add language support for a skill by creating a new language folder in the user resources directory and copying the necessary files over: Identify the Skill and Language to Add : Determine the ID of the skill for which you want to add language support. Let's continue using the skill ID skill-ovos-date-time.openvoiceos . Identify the language you want to add support for. For this example, let's say you want to add support for Spanish (language code: es-es ). Create the New Language Folder : Create a new directory with the name of the language code ( es-es for Spanish) within the locale directory of the skill. This can be done using file manager tools or command-line utilities such as mkdir on Unix-like systems. Using the previous example, we would create ~/.local/share/mycroft/resources/skill-ovos-date-time.openvoiceos/locale/es-es/ Copy the Required Files to the New Language Folder : Copy all the necessary resource files from an existing language folder (e.g., en-us ) to the newly created language folder ( es-es ). This includes files such as dialogues, vocabularies, regex patterns, etc., depending on the resources used by the skill. Ensure that all files are placed in the corresponding directories within the new language folder to maintain the directory structure of the original skill. Verify the Language Addition : Once the files are copied over, verify that the new language is supported by the skill. Restart OpenVoiceOS to allow the skill to recognize the newly added language resources. Test the skill using the newly added language to ensure that it functions correctly and uses the appropriate language-specific resources. By following these steps, users can add support for additional languages to existing skills by creating new language folders and copying the required resource files. NEXT STEPS : consider sending a Pull Request to the skill to directly add language support! This allows users to extend the language capabilities of skills beyond the languages provided by default.","title":"Local Language support"},{"location":"vad_plugins/","text":"VAD Plugins Voice Activity Detection is the process of determining when speech starts and ends in a piece of audio VAD plugins classify audio and report if it contains speech or not. List of VAD plugins plugin description ovos-vad-plugin-silero silero VAD, recommended ovos-vad-plugin-webrtcvad webrtcvad plugin ovos-vad-plugin-noise noise threshold based VAD ovos-vad-plugin-precise VAD model trained with precise Standalone Usage TODO Plugin Template TODO","title":"Voice Activity Detection"},{"location":"vad_plugins/#vad-plugins","text":"Voice Activity Detection is the process of determining when speech starts and ends in a piece of audio VAD plugins classify audio and report if it contains speech or not.","title":"VAD Plugins"},{"location":"vad_plugins/#list-of-vad-plugins","text":"plugin description ovos-vad-plugin-silero silero VAD, recommended ovos-vad-plugin-webrtcvad webrtcvad plugin ovos-vad-plugin-noise noise threshold based VAD ovos-vad-plugin-precise VAD model trained with precise","title":"List of VAD plugins"},{"location":"vad_plugins/#standalone-usage","text":"TODO","title":"Standalone Usage"},{"location":"vad_plugins/#plugin-template","text":"TODO","title":"Plugin Template"},{"location":"versioning/","text":"Versioning OVOS consists of hundreds of packages with their own release cycle, during ovos-core version 0.0.7 or earlier OVOS was a single repository forked from mycroft-core , since version 0.0.8 services have been split into their own repos In this documentation we always refer to the ovos-core version that introduced a feature NOTE ovos-core version 0.0.8 is in alpha ! there is no stable release yet Packages have automated alpha releases on every Pull Request, you can install these from pypi with pip install XXX --pre Automations Please see github shared actions Release Process","title":"Versioning"},{"location":"versioning/#versioning","text":"OVOS consists of hundreds of packages with their own release cycle, during ovos-core version 0.0.7 or earlier OVOS was a single repository forked from mycroft-core , since version 0.0.8 services have been split into their own repos In this documentation we always refer to the ovos-core version that introduced a feature NOTE ovos-core version 0.0.8 is in alpha ! there is no stable release yet Packages have automated alpha releases on every Pull Request, you can install these from pypi with pip install XXX --pre","title":"Versioning"},{"location":"versioning/#automations","text":"Please see github shared actions","title":"Automations"},{"location":"versioning/#release-process","text":"","title":"Release Process"},{"location":"voice_apps/","text":"Standalone Apps Standalone applications can be made for OVOS, these applications are not skills, instead they are launched by the user The main use case for these applications is in desktops, when you want to add a voice interface to a regular application OVOSAbstractApplication ovos-workshop provides the OVOSAbstractApplication class, you can use all methods and decorators from regular Skills in applications built from this from ovos_workshop.app import OVOSAbstractApplication class MyApplication(OVOSAbstractApplication): def __init__(self, skill_id: str = \"my_app\", bus: Optional[MessageBusClient] = None, resources_dir: Optional[str] = None, gui: Optional[GUIInterface] = None, **kwargs): \"\"\" Create an Application. An application is essentially a skill, but designed such that it may be run without an intent service. @param skill_id: Unique ID for this application @param bus: MessageBusClient to bind to application @param resources_dir: optional root resource directory (else defaults to application `root_dir` @param gui: GUIInterface to bind (if `None`, one is created) \"\"\" super().__init__(skill_id, bus, resources_dir, gui=gui, **kwargs) def do_app_stuff(self): pass @intent_handler(\"app_action.intent\") def do_intent_stuff_in_app(self, message): pass if __name__ == \"__main__\": # launch your application from ovos_utils import wait_for_exit_signal app = MyApplication() # wait for user to exit wait_for_exit_signal() NOTE : from OVOS perspective Voice Apps are just like skills and need to have a unique skill_id","title":"Standalone Apps"},{"location":"voice_apps/#standalone-apps","text":"Standalone applications can be made for OVOS, these applications are not skills, instead they are launched by the user The main use case for these applications is in desktops, when you want to add a voice interface to a regular application","title":"Standalone Apps"},{"location":"voice_apps/#ovosabstractapplication","text":"ovos-workshop provides the OVOSAbstractApplication class, you can use all methods and decorators from regular Skills in applications built from this from ovos_workshop.app import OVOSAbstractApplication class MyApplication(OVOSAbstractApplication): def __init__(self, skill_id: str = \"my_app\", bus: Optional[MessageBusClient] = None, resources_dir: Optional[str] = None, gui: Optional[GUIInterface] = None, **kwargs): \"\"\" Create an Application. An application is essentially a skill, but designed such that it may be run without an intent service. @param skill_id: Unique ID for this application @param bus: MessageBusClient to bind to application @param resources_dir: optional root resource directory (else defaults to application `root_dir` @param gui: GUIInterface to bind (if `None`, one is created) \"\"\" super().__init__(skill_id, bus, resources_dir, gui=gui, **kwargs) def do_app_stuff(self): pass @intent_handler(\"app_action.intent\") def do_intent_stuff_in_app(self, message): pass if __name__ == \"__main__\": # launch your application from ovos_utils import wait_for_exit_signal app = MyApplication() # wait for user to exit wait_for_exit_signal() NOTE : from OVOS perspective Voice Apps are just like skills and need to have a unique skill_id","title":"OVOSAbstractApplication"},{"location":"why/","text":"About OpenVoiceOS Introducing OpenVoiceOS - The Free and Open-Source Personal Assistant and Smart Speaker. OpenVoiceOS is a new player in the smart speaker market, offering a powerful and flexible alternative to proprietary solutions like Amazon Echo and Google Home. With OpenVoiceOS, you have complete control over your personal data and the ability to customize and extend the functionality of your smart speaker. Built on open-source software, OpenVoiceOS is designed to provide users with a seamless and intuitive voice interface for controlling their smart home devices, playing music, setting reminders, and much more. The platform leverages cutting-edge technology, including machine learning and natural language processing, to deliver a highly responsive and accurate experience. In addition to its voice capabilities, OpenVoiceOS features a touch-screen GUI made using QT5 and the KF5 framework. The GUI provides an intuitive, user-friendly interface that allows you to access the full range of OpenVoiceOS features and functionality. Whether you prefer voice commands or a more traditional touch interface, OpenVoiceOS has you covered. One of the key advantages of OpenVoiceOS is its open-source nature, which means that anyone with the technical skills can contribute to the platform and help shape its future. Whether you're a software developer, data scientist, or just someone with a passion for technology, you can get involved and help build the next generation of personal assistants and smart speakers. With OpenVoiceOS, you have the option to run the platform fully offline, giving you complete control over your data and ensuring that your information is never shared with third parties. This makes OpenVoiceOS the perfect choice for anyone who values privacy and security. So if you're looking for a personal assistant and smart speaker that gives you the freedom and control you deserve, be sure to check out OpenVoiceOS today! Disclaimer : This post was written in collaboration with ChatGPT","title":"About OpenVoiceOS"},{"location":"why/#about-openvoiceos","text":"Introducing OpenVoiceOS - The Free and Open-Source Personal Assistant and Smart Speaker. OpenVoiceOS is a new player in the smart speaker market, offering a powerful and flexible alternative to proprietary solutions like Amazon Echo and Google Home. With OpenVoiceOS, you have complete control over your personal data and the ability to customize and extend the functionality of your smart speaker. Built on open-source software, OpenVoiceOS is designed to provide users with a seamless and intuitive voice interface for controlling their smart home devices, playing music, setting reminders, and much more. The platform leverages cutting-edge technology, including machine learning and natural language processing, to deliver a highly responsive and accurate experience. In addition to its voice capabilities, OpenVoiceOS features a touch-screen GUI made using QT5 and the KF5 framework. The GUI provides an intuitive, user-friendly interface that allows you to access the full range of OpenVoiceOS features and functionality. Whether you prefer voice commands or a more traditional touch interface, OpenVoiceOS has you covered. One of the key advantages of OpenVoiceOS is its open-source nature, which means that anyone with the technical skills can contribute to the platform and help shape its future. Whether you're a software developer, data scientist, or just someone with a passion for technology, you can get involved and help build the next generation of personal assistants and smart speakers. With OpenVoiceOS, you have the option to run the platform fully offline, giving you complete control over your data and ensuring that your information is never shared with third parties. This makes OpenVoiceOS the perfect choice for anyone who values privacy and security. So if you're looking for a personal assistant and smart speaker that gives you the freedom and control you deserve, be sure to check out OpenVoiceOS today! Disclaimer : This post was written in collaboration with ChatGPT","title":"About OpenVoiceOS"},{"location":"ww_plugins/","text":"Wake Word Plugins WakeWord plugins classify audio and report if a certain word or sound is present or not These plugins usually correspond to the name of the voice assistant, \"hey mycroft\", but can also be used for other purposes All Mycroft Wake Word Plugins need to provide a class derived from the HotWordEngine base class in ovos-plugin-manager When the __init__() method of the base class is run the config for that module will be loaded and available through self.config . OVOS's selected language will also be available through self.lang . HotwordPlugin found_wake_word() Each Wake Word plugin must define the found_wake_word() method taking one argument: frame_data - this is the audio data that needs to be checked for a wake word. You can process audio here or return a result previously handled in the update() method. update() The update() method is optional and takes one argument: chunk - live audio chunks allowing for streaming predictions. Results must be returned in the found_wake_word() method. stop() The stop() method is optional and takes no arguments. It should be used to perform any actions needed to shut down the hot word engine. This may include things such as unloading data or to shutdown external processes. Entry point To make the class detectable as a Wake Word plugin, the package needs to provide an entry point under the mycroft.plugin.wake_word namespace. setup([...], entry_points = {'mycroft.plugin.wake_word': 'example_wake_word_plugin = my_example_ww:myWakeWordEngine'} ) Where: example_wake_word_plugin is the Wake Word module name for the plugin my_example_ww is the Python module; and myWakeWordEngine is the class in the module to return List of Wake Word plugins Plugin Type ovos-ww-plugin-openWakeWord model ovos-ww-plugin-precise-lite model ovos-ww-plugin-vosk text samples ovos-ww-plugin-pocketsphinx phonemes ovos-ww-plugin-precise model ovos-ww-plugin-snowboy model ovos-ww-plugin-nyumaya model ovos-ww-plugin-hotkeys keyboard Standalone Usage first lets get some boilerplate ouf of the way for the microphone handling logic import pyaudio # helper class class CyclicAudioBuffer: def __init__(self, duration=0.98, initial_data=None, sample_rate=16000, sample_width=2): self.size = self.duration_to_bytes(duration, sample_rate, sample_width) initial_data = initial_data or self.get_silence(self.size) # Get at most size bytes from the end of the initial data self._buffer = initial_data[-self.size:] @staticmethod def duration_to_bytes(duration, sample_rate=16000, sample_width=2): return int(duration * sample_rate) * sample_width @staticmethod def get_silence(num_bytes): return b'\\0' * num_bytes def append(self, data): \"\"\"Add new data to the buffer, and slide out data if the buffer is full Arguments: data (bytes): binary data to append to the buffer. If buffer size is exceeded the oldest data will be dropped. \"\"\" buff = self._buffer + data if len(buff) > self.size: buff = buff[-self.size:] self._buffer = buff def get(self): \"\"\"Get the binary data.\"\"\" return self._buffer # pyaudio params FORMAT = pyaudio.paInt16 CHANNELS = 1 RATE = 16000 CHUNK = 1024 MAX_RECORD_SECONDS = 20 SAMPLE_WIDTH = pyaudio.get_sample_size(FORMAT) audio = pyaudio.PyAudio() # start Recording stream = audio.open(channels=CHANNELS, format=FORMAT, rate=RATE, frames_per_buffer=CHUNK, input=True) def load_plugin(): # Wake word initialization config = {\"model\": \"path/to/hey_computer.model\"} return MyHotWord(\"hey computer\", config=config) def listen_for_ww(plug): # TODO - see examples below return False plug = load_plugin() print(f\"Waiting for wake word {MAX_RECORD_SECONDS} seconds\") found = listen_for_ww(plug) if found: print(\"Found wake word!\") else: print(\"No wake word found\") # stop everything plug.stop() stream.stop_stream() stream.close() audio.terminate() new style plugins New style plugins expect to receive live audio, they may keep their own cyclic buffers internally def listen_for_ww(plug): for i in range(0, int(RATE / CHUNK * MAX_RECORD_SECONDS)): data = stream.read(CHUNK) # feed data directly to streaming prediction engines plug.update(data) # streaming engines return result here found = plug.found_wake_word(data) if found: return True old style plugins (DEPRECATED) Old style plugins expect to receive ~3 seconds of audio data at once def listen_for_ww(plug): # used for old style non-streaming wakeword (deprecated) audio_buffer = CyclicAudioBuffer(plug.expected_duration, sample_rate=RATE, sample_width=SAMPLE_WIDTH) for i in range(0, int(RATE / CHUNK * MAX_RECORD_SECONDS)): data = stream.read(CHUNK) # add data to rolling buffer, used by non-streaming engines audio_buffer.append(data) # non-streaming engines check the byte_data in audio_buffer audio_data = audio_buffer.get() found = plug.found_wake_word(audio_data) if found: return True new + old style plugins (backwards compatibility) if you are unsure what kind of plugin you will be using you can be compatible with both approaches like ovos-core def listen_for_ww(plug): # used for old style non-streaming wakeword (deprecated) audio_buffer = CyclicAudioBuffer(plug.expected_duration, sample_rate=RATE, sample_width=SAMPLE_WIDTH) for i in range(0, int(RATE / CHUNK * MAX_RECORD_SECONDS)): data = stream.read(CHUNK) # old style engines will ignore the update plug.update(data) # streaming engines will ignore the byte_data audio_buffer.append(data) audio_data = audio_buffer.get() found = plug.found_wake_word(audio_data) if found: return True Plugin Template from ovos_plugin_manager.templates.hotwords import HotWordEngine from threading import Event class MyWWPlugin(HotWordEngine): def __init__(self, key_phrase=\"hey mycroft\", config=None, lang=\"en-us\"): super().__init__(key_phrase, config, lang) self.detection = Event() # read config settings for your plugin self.sensitivity = self.config.get(\"sensitivity\", 0.5) # TODO - plugin stuff # how does your plugin work? phonemes? text? models? self.engine = MyWW(key_phrase) def found_wake_word(self, frame_data): \"\"\"Check if wake word has been found. Checks if the wake word has been found. Should reset any internal tracking of the wake word state. Arguments: frame_data (binary data): Deprecated. Audio data for large chunk of audio to be processed. This should not be used to detect audio data instead use update() to incrementally update audio Returns: bool: True if a wake word was detected, else False \"\"\" detected = self.detection.is_set() if detected: self.detection.clear() return detected def update(self, chunk): \"\"\"Updates the hotword engine with new audio data. The engine should process the data and update internal trigger state. Arguments: chunk (bytes): Chunk of audio data to process \"\"\" if self.engine.found_it(chunk): # TODO - check for wake word self.detection.set() def stop(self): \"\"\"Perform any actions needed to shut down the wake word engine. This may include things such as unloading data or shutdown external processess. \"\"\" self.engine.bye() # TODO - plugin specific shutdown","title":"Wake Word"},{"location":"ww_plugins/#wake-word-plugins","text":"WakeWord plugins classify audio and report if a certain word or sound is present or not These plugins usually correspond to the name of the voice assistant, \"hey mycroft\", but can also be used for other purposes All Mycroft Wake Word Plugins need to provide a class derived from the HotWordEngine base class in ovos-plugin-manager When the __init__() method of the base class is run the config for that module will be loaded and available through self.config . OVOS's selected language will also be available through self.lang .","title":"Wake Word Plugins"},{"location":"ww_plugins/#hotwordplugin","text":"","title":"HotwordPlugin"},{"location":"ww_plugins/#found_wake_word","text":"Each Wake Word plugin must define the found_wake_word() method taking one argument: frame_data - this is the audio data that needs to be checked for a wake word. You can process audio here or return a result previously handled in the update() method.","title":"found_wake_word()"},{"location":"ww_plugins/#update","text":"The update() method is optional and takes one argument: chunk - live audio chunks allowing for streaming predictions. Results must be returned in the found_wake_word() method.","title":"update()"},{"location":"ww_plugins/#stop","text":"The stop() method is optional and takes no arguments. It should be used to perform any actions needed to shut down the hot word engine. This may include things such as unloading data or to shutdown external processes.","title":"stop()"},{"location":"ww_plugins/#entry-point","text":"To make the class detectable as a Wake Word plugin, the package needs to provide an entry point under the mycroft.plugin.wake_word namespace. setup([...], entry_points = {'mycroft.plugin.wake_word': 'example_wake_word_plugin = my_example_ww:myWakeWordEngine'} ) Where: example_wake_word_plugin is the Wake Word module name for the plugin my_example_ww is the Python module; and myWakeWordEngine is the class in the module to return","title":"Entry point"},{"location":"ww_plugins/#list-of-wake-word-plugins","text":"Plugin Type ovos-ww-plugin-openWakeWord model ovos-ww-plugin-precise-lite model ovos-ww-plugin-vosk text samples ovos-ww-plugin-pocketsphinx phonemes ovos-ww-plugin-precise model ovos-ww-plugin-snowboy model ovos-ww-plugin-nyumaya model ovos-ww-plugin-hotkeys keyboard","title":"List of Wake Word plugins"},{"location":"ww_plugins/#standalone-usage","text":"first lets get some boilerplate ouf of the way for the microphone handling logic import pyaudio # helper class class CyclicAudioBuffer: def __init__(self, duration=0.98, initial_data=None, sample_rate=16000, sample_width=2): self.size = self.duration_to_bytes(duration, sample_rate, sample_width) initial_data = initial_data or self.get_silence(self.size) # Get at most size bytes from the end of the initial data self._buffer = initial_data[-self.size:] @staticmethod def duration_to_bytes(duration, sample_rate=16000, sample_width=2): return int(duration * sample_rate) * sample_width @staticmethod def get_silence(num_bytes): return b'\\0' * num_bytes def append(self, data): \"\"\"Add new data to the buffer, and slide out data if the buffer is full Arguments: data (bytes): binary data to append to the buffer. If buffer size is exceeded the oldest data will be dropped. \"\"\" buff = self._buffer + data if len(buff) > self.size: buff = buff[-self.size:] self._buffer = buff def get(self): \"\"\"Get the binary data.\"\"\" return self._buffer # pyaudio params FORMAT = pyaudio.paInt16 CHANNELS = 1 RATE = 16000 CHUNK = 1024 MAX_RECORD_SECONDS = 20 SAMPLE_WIDTH = pyaudio.get_sample_size(FORMAT) audio = pyaudio.PyAudio() # start Recording stream = audio.open(channels=CHANNELS, format=FORMAT, rate=RATE, frames_per_buffer=CHUNK, input=True) def load_plugin(): # Wake word initialization config = {\"model\": \"path/to/hey_computer.model\"} return MyHotWord(\"hey computer\", config=config) def listen_for_ww(plug): # TODO - see examples below return False plug = load_plugin() print(f\"Waiting for wake word {MAX_RECORD_SECONDS} seconds\") found = listen_for_ww(plug) if found: print(\"Found wake word!\") else: print(\"No wake word found\") # stop everything plug.stop() stream.stop_stream() stream.close() audio.terminate() new style plugins New style plugins expect to receive live audio, they may keep their own cyclic buffers internally def listen_for_ww(plug): for i in range(0, int(RATE / CHUNK * MAX_RECORD_SECONDS)): data = stream.read(CHUNK) # feed data directly to streaming prediction engines plug.update(data) # streaming engines return result here found = plug.found_wake_word(data) if found: return True old style plugins (DEPRECATED) Old style plugins expect to receive ~3 seconds of audio data at once def listen_for_ww(plug): # used for old style non-streaming wakeword (deprecated) audio_buffer = CyclicAudioBuffer(plug.expected_duration, sample_rate=RATE, sample_width=SAMPLE_WIDTH) for i in range(0, int(RATE / CHUNK * MAX_RECORD_SECONDS)): data = stream.read(CHUNK) # add data to rolling buffer, used by non-streaming engines audio_buffer.append(data) # non-streaming engines check the byte_data in audio_buffer audio_data = audio_buffer.get() found = plug.found_wake_word(audio_data) if found: return True new + old style plugins (backwards compatibility) if you are unsure what kind of plugin you will be using you can be compatible with both approaches like ovos-core def listen_for_ww(plug): # used for old style non-streaming wakeword (deprecated) audio_buffer = CyclicAudioBuffer(plug.expected_duration, sample_rate=RATE, sample_width=SAMPLE_WIDTH) for i in range(0, int(RATE / CHUNK * MAX_RECORD_SECONDS)): data = stream.read(CHUNK) # old style engines will ignore the update plug.update(data) # streaming engines will ignore the byte_data audio_buffer.append(data) audio_data = audio_buffer.get() found = plug.found_wake_word(audio_data) if found: return True","title":"Standalone Usage"},{"location":"ww_plugins/#plugin-template","text":"from ovos_plugin_manager.templates.hotwords import HotWordEngine from threading import Event class MyWWPlugin(HotWordEngine): def __init__(self, key_phrase=\"hey mycroft\", config=None, lang=\"en-us\"): super().__init__(key_phrase, config, lang) self.detection = Event() # read config settings for your plugin self.sensitivity = self.config.get(\"sensitivity\", 0.5) # TODO - plugin stuff # how does your plugin work? phonemes? text? models? self.engine = MyWW(key_phrase) def found_wake_word(self, frame_data): \"\"\"Check if wake word has been found. Checks if the wake word has been found. Should reset any internal tracking of the wake word state. Arguments: frame_data (binary data): Deprecated. Audio data for large chunk of audio to be processed. This should not be used to detect audio data instead use update() to incrementally update audio Returns: bool: True if a wake word was detected, else False \"\"\" detected = self.detection.is_set() if detected: self.detection.clear() return detected def update(self, chunk): \"\"\"Updates the hotword engine with new audio data. The engine should process the data and update internal trigger state. Arguments: chunk (bytes): Chunk of audio data to process \"\"\" if self.engine.found_it(chunk): # TODO - check for wake word self.detection.set() def stop(self): \"\"\"Perform any actions needed to shut down the wake word engine. This may include things such as unloading data or shutdown external processess. \"\"\" self.engine.bye() # TODO - plugin specific shutdown","title":"Plugin Template"}]}